{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c69a7f71-be4a-463e-a8d1-b2b76b49ebb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: umap 0.1.1\n",
      "Uninstalling umap-0.1.1:\n",
      "  Successfully uninstalled umap-0.1.1\n",
      "Requirement already satisfied: faiss-cpu in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy==1.26.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (1.26.3)\n",
      "Requirement already satisfied: openai==1.3.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (1.3.3)\n",
      "Requirement already satisfied: scikit-learn in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (1.5.0)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from openai==1.3.3) (3.7.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from openai==1.3.3) (4.66.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from openai==1.3.3) (0.27.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from openai==1.3.3) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from openai==1.3.3) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from openai==1.3.3) (2.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from anyio<4,>=3.5.0->openai==1.3.3) (1.1.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from anyio<4,>=3.5.0->openai==1.3.3) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from anyio<4,>=3.5.0->openai==1.3.3) (1.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai==1.3.3) (1.0.5)\n",
      "Requirement already satisfied: certifi in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai==1.3.3) (2023.7.22)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.3.3) (0.14.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai==1.3.3) (2.18.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai==1.3.3) (0.7.0)\n",
      "Requirement already satisfied: sentence-transformers==2.2.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (1.5.0)\n",
      "Requirement already satisfied: sentencepiece in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (4.66.2)\n",
      "Requirement already satisfied: nltk in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (3.8.1)\n",
      "Requirement already satisfied: torchvision in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (0.17.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (4.38.1)\n",
      "Requirement already satisfied: scipy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (0.21.4)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (2.2.2)\n",
      "Requirement already satisfied: numpy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (23.1)\n",
      "Requirement already satisfied: filelock in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.13.1)\n",
      "Requirement already satisfied: requests in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.10.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.3.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
      "Requirement already satisfied: sympy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.12)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.19.3)\n",
      "Requirement already satisfied: networkx in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.2.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.2)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence-transformers==2.2.2) (12.4.99)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.4.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2023.12.25)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.3)\n",
      "Requirement already satisfied: click in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nltk->sentence-transformers==2.2.2) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nltk->sentence-transformers==2.2.2) (1.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.7.22)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-learn->sentence-transformers==2.2.2) (3.5.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torchvision->sentence-transformers==2.2.2) (10.3.0)\n",
      "Requirement already satisfied: tenacity==8.2.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (8.2.3)\n",
      "Requirement already satisfied: tiktoken==0.5.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (0.5.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tiktoken==0.5.1) (2.31.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tiktoken==0.5.1) (2023.12.25)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken==0.5.1) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken==0.5.1) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken==0.5.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken==0.5.1) (1.26.6)\n",
      "Requirement already satisfied: torch in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers==4.38.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (4.38.1)\n",
      "Requirement already satisfied: umap-learn==0.5.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (0.5.5)\n",
      "Requirement already satisfied: urllib3==1.26.6 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (1.26.6)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers==4.38.1) (4.66.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers==4.38.1) (1.26.3)\n",
      "Requirement already satisfied: requests in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers==4.38.1) (2.31.0)\n",
      "Requirement already satisfied: filelock in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers==4.38.1) (3.13.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers==4.38.1) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers==4.38.1) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers==4.38.1) (0.21.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers==4.38.1) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers==4.38.1) (23.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers==4.38.1) (2023.12.25)\n",
      "Requirement already satisfied: numba>=0.51.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from umap-learn==0.5.5) (0.59.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from umap-learn==0.5.5) (0.5.12)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from umap-learn==0.5.5) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from umap-learn==0.5.5) (1.5.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: fsspec in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: jinja2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: sympy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.99)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from numba>=0.51.2->umap-learn==0.5.5) (0.42.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pynndescent>=0.5->umap-learn==0.5.5) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-learn>=0.22->umap-learn==0.5.5) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers==4.38.1) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers==4.38.1) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers==4.38.1) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu numpy==1.26.3 \n",
    "!pip install openai==1.3.3 scikit-learn \n",
    "!pip install sentence-transformers==2.2.2 \n",
    "!pip install tenacity==8.2.3 tiktoken==0.5.1 \n",
    "!pip install torch transformers==4.38.1 umap-learn==0.5.5 urllib3==1.26.6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7637d3e5-648a-40a2-840d-6d54e840006c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (4.2.0)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pypdf) (4.10.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f947157-faf6-4c59-883e-c7ec741ac880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf ~/.cache/*\n",
    "!rm -rf /tmp/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a458516-ea8c-41b8-ab29-1fcfbb522bae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from raptor import BaseSummarizationModel, BaseQAModel, BaseEmbeddingModel, RetrievalAugmentationConfig\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4278b904-ae43-4495-93fc-1299fb7534d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245dbd0ed5c7479890dc9eb313042919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# if you want to use the Gemma, you will need to authenticate with HuggingFace, Skip this step, if you have the model already downloaded\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b4939a4-23db-41cf-acb6-24c1eb9873a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "# You can define your own Summarization model by extending the base Summarization Class. \n",
    "class GEMMASummarizationModel(BaseSummarizationModel):\n",
    "    def __init__(self, model_name=\"google/gemma-2b-it\"):\n",
    "        # Initialize the tokenizer and the pipeline for the GEMMA model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.summarization_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_name,\n",
    "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "            device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),  # Use \"cpu\" if CUDA is not available\n",
    "        )\n",
    "\n",
    "    def summarize(self, context, max_tokens=150):\n",
    "        # Format the prompt for summarization\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"Write a summary of the following, including as many key details as possible: {context}:\"}\n",
    "        ]\n",
    "        \n",
    "        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        # Generate the summary using the pipeline\n",
    "        outputs = self.summarization_pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "        \n",
    "        # Extracting and returning the generated summary\n",
    "        summary = outputs[0][\"generated_text\"].strip()\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f28c60c-a8b1-4acc-991f-e44cbb3637bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GEMMAQAModel(BaseQAModel):\n",
    "    def __init__(self, model_name= \"google/gemma-2b-it\"):\n",
    "        # Initialize the tokenizer and the pipeline for the model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.qa_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_name,\n",
    "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "            device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "        )\n",
    "\n",
    "    def answer_question(self, context, question):\n",
    "        # Apply the chat template for the context and question\n",
    "        messages=[\n",
    "              {\"role\": \"user\", \"content\": f\"Given Context: {context} Give the best full answer amongst the option to question {question}\"}\n",
    "        ]\n",
    "        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        # Generate the answer using the pipeline\n",
    "        outputs = self.qa_pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "        \n",
    "        # Extracting and returning the generated answer\n",
    "        answer = outputs[0][\"generated_text\"][len(prompt):]\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b18b0c2-ddce-48d5-b3a8-e2b76757d2c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "class SBertEmbeddingModel(BaseEmbeddingModel):\n",
    "    def __init__(self, model_name=\"sentence-transformers/multi-qa-mpnet-base-cos-v1\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def create_embedding(self, text):\n",
    "        return self.model.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39b263a9-f265-4ff4-a256-61757c7a74a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94afdacc2513435da51f67d9d8389d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecae51a7f7484e02a63a13cd978b050d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 13:00:25,455 - Use pytorch device_name: cuda\n",
      "2024-06-05 13:00:25,455 - Load pretrained SentenceTransformer: sentence-transformers/multi-qa-mpnet-base-cos-v1\n"
     ]
    }
   ],
   "source": [
    "RAC = RetrievalAugmentationConfig(summarization_model=GEMMASummarizationModel(), qa_model=GEMMAQAModel(), embedding_model=SBertEmbeddingModel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a2f6afe-23cc-423a-9f59-c9c4e190915c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from raptor import RetrievalAugmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c7f9a4a-164f-4506-aa2a-ae1fc1912e13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:06:49,515 - Successfully initialized TreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <__main__.GEMMASummarizationModel object at 0x7f9677f31b20>\n",
      "            Embedding Models: {'EMB': <__main__.SBertEmbeddingModel object at 0x7f9677eeee20>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2024-06-05 12:06:49,516 - Successfully initialized ClusterTreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <__main__.GEMMASummarizationModel object at 0x7f9677f31b20>\n",
      "            Embedding Models: {'EMB': <__main__.SBertEmbeddingModel object at 0x7f9677eeee20>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2024-06-05 12:06:49,516 - Successfully initialized RetrievalAugmentation with Config \n",
      "        RetrievalAugmentationConfig:\n",
      "            \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <__main__.GEMMASummarizationModel object at 0x7f9677f31b20>\n",
      "            Embedding Models: {'EMB': <__main__.SBertEmbeddingModel object at 0x7f9677eeee20>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "            \n",
      "            \n",
      "        TreeRetrieverConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Context Embedding Model: EMB\n",
      "            Embedding Model: <__main__.SBertEmbeddingModel object at 0x7f9677eeee20>\n",
      "            Num Layers: None\n",
      "            Start Layer: None\n",
      "        \n",
      "            \n",
      "            QA Model: <__main__.GEMMAQAModel object at 0x7f97746216d0>\n",
      "            Tree Builder Type: cluster\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "RA = RetrievalAugmentation(config=RAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3607efc7-31b5-45f8-b855-79594342286c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Pages: 23\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "FILE_PATH = './RaptorResearchPaper.pdf'\n",
    "reader = PdfReader(FILE_PATH)\n",
    "num_of_pages = len(reader.pages)\n",
    "print(f\"Number of Pages: {num_of_pages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c607cfb3-fd1a-42c6-9c21-42bd36d6e08c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for page_num in range(3, 13):\n",
    "  page = reader.pages[page_num]\n",
    "  text += page.extract_text()+\" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5e76fe5-971e-4900-89e9-821e81e05fd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published as a conference paper at ICLR 2024\n",
      "Given a set of Ntext segments, each represented as a d-dimensional dense vector embedding, the\n",
      "likelihood of a text vector, x, given its membership in the kthGaussian distribution, is denoted by\n",
      "P(x|k) =N(x;µk,Σk). The overall probability distribution is a weighted combination P(x) =PK\n",
      "k=1πkN(x;µk,Σk), where πksignifies the mixture weight for the kthGaussian distribution.\n",
      "The high dimensionality of vector embeddings presents a challenge for traditional GMMs, as dis-\n",
      "tance metrics may behave poorly when used to measure similarity in high-dimensional spaces (Ag-\n",
      "garwal et al., 2001). To mitigate this, we employ Uniform Manifold Approximation and Projection\n",
      "(UMAP), a manifold learning technique for dimensionality reduction (McInnes et al., 2018). The\n",
      "number of nearest neighbors parameter, nneighbors , in UMAP determines the balance between\n",
      "the preservation of local and global structures. Our algorithm varies nneighbors to create a hierar-\n",
      "chical clustering structure: it first identifies global clusters and then performs local clustering within\n",
      "these global clusters. This two-step clustering process captures a broad spectrum of relationships\n",
      "among the text data, from broad themes to specific details.\n",
      "Should a local cluster’s combined context ever exceed the summarization model’s token threshold,\n",
      "our algorithm recursively applies clustering within the cluster, ensuring that the context remains\n",
      "within the token threshold.\n",
      "To determine the optimal number of clusters, we employ the Bayesian Information Criterion (BIC)\n",
      "for model selection. BIC not only penalizes model complexity but also rewards goodness of fit\n",
      "(Schwarz, 1978). The BIC for a given GMM is BIC = ln( N)k−2 ln(ˆL), where Nis the number\n",
      "of text segments (or data points), kis the number of model parameters, and ˆLis the maximized\n",
      "value of the likelihood function of the model. In the context of GMM, the number of parameters k\n",
      "is a function of the dimensionality of the input vectors and the number of clusters.\n",
      "With the optimal number of clusters determined by BIC, the Expectation-Maximization algorithm\n",
      "is then used to estimate the GMM parameters, namely the means, covariances, and mixture weights.\n",
      "While the Gaussian assumption in GMMs may not perfectly align with the nature of text data, which\n",
      "often exhibits a sparse and skewed distribution, our empirical observations suggest that it offers an\n",
      "effective model for our purpose. We run an ablation comparing GMM Clustering with summarizing\n",
      "contiguous chunks and provide details in Appendix B.\n",
      "Model-Based Summarization After clustering the nodes using Gaussian Mixture Models, the\n",
      "nodes in each cluster are sent to a language model for summarization. This step allows the model\n",
      "to transform large chunks of text into concise, coherent summaries of the selected nodes. For our\n",
      "experiments, we use gpt-3.5-turbo to generate the summaries. The summarization step con-\n",
      "denses the potentially large volume of retrieved information into a manageable size. We provide\n",
      "statistics on the compression due to the summarization in Appendix C and the prompt used for\n",
      "summarization in Appendix D.\n",
      "While the summarization model generally produces reliable summaries, a focused annotation study\n",
      "revealed that about 4% of the summaries contained minor hallucinations. These did not propagate\n",
      "to parent nodes and had no discernible impact on question-answering tasks. For an in-depth analysis\n",
      "of hallucinations, refer to the appendix E.\n",
      "Querying In this section, we elaborate on the two querying mechanisms employed by RAPTOR:\n",
      "tree traversal and collapsed tree. These methods offer unique ways of traversing the multi-layered\n",
      "RAPTOR tree to retrieve relevant information, each with its own advantages and trade-offs. We\n",
      "provide the pseudocode of both methods in Appendix F. Note that we embed all nodes using SBERT.\n",
      "The tree traversal method first selects the top-k most relevant root nodes based on their cosine\n",
      "similarity to the query embedding. The children of these selected nodes are considered at the next\n",
      "layer and the top-k nodes are selected from this pool again based on their cosine similarity to the\n",
      "query vector. This process is repeated until we reach the leaf nodes. Finally, the text from all selected\n",
      "nodes is concatenated to form the retrieved context. The algorithm’s steps are outlined below:\n",
      "1. Start at the root layer of the RAPTOR tree. Compute the cosine similarity between the\n",
      "query embedding and the embeddings of all nodes present at this initial layer.\n",
      "2. Choose the top- knodes based on the highest cosine similarity scores, forming the set S1.\n",
      "4 Published as a conference paper at ICLR 2024\n",
      "Figure 2: Illustration of the tree traversal and collapsed tree retrieval mechanisms. Tree traver-\n",
      "sal starts at the root level of the tree and retrieves the top- k(here, top- 1) node(s) based on cosine\n",
      "similarity to the query vector. At each level, it retrieves the top- knode(s) from the child nodes of\n",
      "the previous layer’s top- k. Collapsed tree collapses the tree into a single layer and retrieves nodes\n",
      "until a threshold number of tokens is reached, based on cosine similarity to the query vector. The\n",
      "nodes on which cosine similarity search is performed are highlighted in both illustrations.\n",
      "3. Proceed to the child nodes of the elements in set S1. Compute the cosine similarity between\n",
      "the query vector and the vector embeddings of these child nodes.\n",
      "4. Select the top kchild nodes with the highest cosine similarity scores to the query, forming\n",
      "the set S2.\n",
      "5. Continue this process recursively for dlayers, producing sets S1, S2, . . . , S d.\n",
      "6. Concatenate sets S1through Sdto assemble the relevant context to the query.\n",
      "By adjusting the depth dand the number of nodes kselected at each layer, the tree traversal method\n",
      "offers control over the specificity and breadth of the information retrieved. The algorithm starts with\n",
      "a broad outlook by considering the top layers of the tree and progressively focuses on finer details\n",
      "as it descends through the lower layers.\n",
      "Thecollapsed tree approach offers a simpler way to search for relevant information by considering\n",
      "all nodes in the tree simultaneously, as depicted in Figure 2. Instead of going layer-by-layer, this\n",
      "method flattens the multi-layered tree into a single layer, essentially bringing all the nodes onto the\n",
      "same level for comparison. The steps for this method are outlined below:\n",
      "1. First, collapse the entire RAPTOR tree into a single layer. This new set of nodes, denoted\n",
      "asC, contains nodes from every layer of the original tree.\n",
      "2. Next, calculate the cosine similarity between the query embedding and the embeddings of\n",
      "all nodes present in the collapsed set C.\n",
      "3. Finally, pick the top- knodes that have the highest cosine similarity scores with the query.\n",
      "Keep adding nodes to the result set until you reach a predefined maximum number of\n",
      "tokens, ensuring you don’t exceed the model’s input limitations.\n",
      "We tested both approaches on 20 stories from the QASPER dataset. Figure 3 shows the performance\n",
      "of tree traversal with different top- sizes and collapsed tree with different maximum token numbers.\n",
      "The collapsed tree approach consistently performs better. We believe collapsed tree retrieval is\n",
      "better due to offering greater flexibility than tree traversal; i.e., by searching through all the nodes\n",
      "simultaneously, it retrieves information that is at the correct level of granularity for a given question.\n",
      "In comparison, while using tree traversal with the same values of dandk, the ratio of nodes from\n",
      "each level of the tree will be constant. So, the ratio of higher-order thematic information to granular\n",
      "details will remain the same regardless of the question.\n",
      "5 Published as a conference paper at ICLR 2024\n",
      "One drawback, however, of the collapsed tree approach is that it requires cosine similarity search to\n",
      "be performed on all nodes in the tree. However, this can be made more efficient with fast k-nearest\n",
      "neighbor libraries such as FAISS (Johnson et al., 2019).\n",
      "Figure 3: Comparison of querying methods.\n",
      "Results on 20 stories from the QASPER dataset\n",
      "using tree traversal with different top-k values,\n",
      "and collapsed tree with different context lengths.\n",
      "Collapsed tree with 2000 tokens produces the best\n",
      "results, so we use this querying strategy for our\n",
      "main results.Overall, given the collapsed tree approach’s\n",
      "greater flexibility and its superior performance\n",
      "on the subset of the QASPER dataset, this is\n",
      "the querying approach with which we proceed.\n",
      "Specifically, we use the collapsed tree with\n",
      "2000 maximum tokens, which approximately\n",
      "equates to retrieving the top-20 nodes. Using a\n",
      "token-based approach ensures the context does\n",
      "not exceed model context constraints as token\n",
      "counts can vary across nodes. For experiments\n",
      "with the UnifiedQA model, we provide 400 to-\n",
      "kens of context, as UnifiedQA has a max con-\n",
      "text length of 512 tokens. We provide the same\n",
      "amount of tokens of context to RAPTOR and to\n",
      "the baselines.\n",
      "Qualitative Study We conduct a qualitative\n",
      "analysis to understand the benefits of RAP-\n",
      "TOR’s retrieval process compared to Dense\n",
      "Passage Retrieval (DPR) methods. Our study\n",
      "focuses on thematic, multi-hop questions using\n",
      "a 1500-word Cinderella fairytale. As illustrated\n",
      "in Figure 4, RAPTOR’s tree-based retrieval allows it to choose nodes from different tree layers,\n",
      "matching the question’s detail level. This approach often yields more relevant and comprehensive\n",
      "information for downstream tasks than DPR. For a detailed discussion and examples, including the\n",
      "text retrieved by both RAPTOR and DPR for specific questions, please refer to the appendix G.\n",
      "4 E XPERIMENTS\n",
      "Datasets We measure RAPTOR’s performance across three question-answering datasets: Narra-\n",
      "tiveQA, QASPER, and QuALITY .\n",
      "NarrativeQA is a dataset that comprises question-answer pairs based on the full texts of books\n",
      "and movie transcripts, totaling 1,572 documents (Ko ˇcisk`y et al., 2018; Wu et al., 2021). The\n",
      "NarrativeQA-Story task requires a comprehensive understanding of the entire narrative in order\n",
      "to accurately answer its questions, thus testing the model’s ability to comprehend longer texts in\n",
      "the literary domain. We measure performance on this dataset using the standard BLEU (B-1, B-4),\n",
      "ROUGE (R-L), and METEOR (M) metrics. Please see appendix H for more details on the Narra-\n",
      "tiveQA evaluation script used in our experiments.\n",
      "The QASPER dataset includes 5,049 questions across 1,585 NLP papers, with each question probing\n",
      "for information embedded within the full text (Dasigi et al., 2021). The answer types in QASPER\n",
      "are categorized as Answerable/Unanswerable, Yes/No, Abstractive, and Extractive. Accuracy is\n",
      "measured using standard F1.\n",
      "Lastly, the QuALITY dataset consists of multiple-choice questions, each accompanied by context\n",
      "passages averaging approximately 5,000 tokens in length (Pang et al., 2022). This dataset calls for\n",
      "reasoning over the entire document for QA tasks, enabling us to measure the performance of our re-\n",
      "trieval system on medium-length documents. The dataset includes a challenging subset, QuALITY-\n",
      "HARD, which contains questions that a majority of human annotators answered incorrectly in a\n",
      "speed-setting. We report accuracies for both the entire test set and the HARD subset.\n",
      "Controlled Baseline Comparisons We first present controlled comparisons using the UnifiedQA\n",
      "3B as the reader, with SBERT (Reimers & Gurevych, 2019), BM25 (Robertson et al., 1995; 2009),\n",
      "and DPR (Karpukhin et al., 2020) as the embedding models with and without the RAPTOR tree\n",
      "structure, on three datasets: QASPER, NarrativeQA, and QuALITY . As shown in Tables 1 and 2,\n",
      "6 Published as a conference paper at ICLR 2024\n",
      "Figure 4: Querying Process: Illustration of how RAPTOR retrieves information for two questions\n",
      "about the Cinderella story: “What is the central theme of the story?” and “How did Cinderella find\n",
      "a happy ending?”. Highlighted nodes indicate RAPTOR’s selections, while arrows point to DPR’s\n",
      "leaf nodes. Notably, RAPTOR’s context often encompasses the information retrieved by DPR, either\n",
      "directly or within higher-layer summaries.\n",
      "our results demonstrate that RAPTOR, when combined with any retriever, consistently outperforms\n",
      "the respective retriever across all datasets.2\n",
      "Since RAPTOR with SBERT has the best performance, we use it in all subsequent experiments.\n",
      "We now compare RAPTOR with BM25 and DPR, using three different LLMs: GPT-3, GPT-4, and\n",
      "UnifiedQA. As shown in Table 3, RAPTOR consistently outperforms BM25 and DPR across all\n",
      "three Language Models on the QASPER dataset. RAPTOR’s F-1 Match scores are 53.1%, 55.7%,\n",
      "and 36.6% when using GPT-3, GPT-4, and UnifiedQA, respectively. These scores surpass DPR by\n",
      "margins of 1.8, 2.7, and 4.5 points, and outdo BM25 by 6.5, 5.5, and 10.2 points across the respective\n",
      "LLMs. QASPER requires synthesizing information within NLP papers, so it is unsurprising that\n",
      "RAPTOR’s higher-level summary nodes would allow it to outperform methods that can only extract\n",
      "the top- kmost similar raw chunks of text, which may not contain the correct response in isolation.\n",
      "Table 1: NarrativeQA Performance With + Without RAPTOR: Performance comparison of\n",
      "various retrieval methods (SBERT, BM25, DPR) with and without RAPTOR on the NarrativeQA\n",
      "dataset, using UnifiedQA-3B as the language model. RAPTOR outperforms baselines of each re-\n",
      "spective retrieval method.\n",
      "Model ROUGE BLEU-1 BLEU-4 METEOR\n",
      "SBERT with RAPTOR 30.87% 23.50% 6.42% 19.20%\n",
      "SBERT without RAPTOR 29.26% 22.56% 5.95% 18.15%\n",
      "BM25 with RAPTOR 27.93% 21.17% 5.70% 17.03%\n",
      "BM25 without RAPTOR 23.52% 17.73% 4.65% 13.98%\n",
      "DPR with RAPTOR 30.94% 23.51% 6.45% 19.05%\n",
      "DPR without RAPTOR 29.56% 22.84% 6.12% 18.44%\n",
      "Likewise, in the QuALITY dataset as shown in Table 4, RAPTOR achieves an accuracy of 62.4%,\n",
      "which is a 2% and 5.1% improvement over DPR and BM25. Similar trends are observed when Uni-\n",
      "fiedQA is employed, with RAPTOR outperforming DPR and BM25 by 2.7% and 6.7%, respectively.\n",
      "Finally, in the NarrativeQA dataset, as presented in Table 6, RAPTOR excels across multiple met-\n",
      "rics. For ROUGE-L, it surpasses BM25 and DPR by 7.3 and 2.7 points, respectively. In other\n",
      "metrics like BLEU-1, BLEU-4, and METEOR, RAPTOR outperforms BM25 and DPR by margins\n",
      "ranging from 1.7 to 5.8 and 0.7 to 2.1 points, respectively.\n",
      "2For the DPR experiments in Tables 1 and 2, we used the dpr-multiset-base model as opposed to\n",
      "dpr-single-nq-base which was used in rest of the experiments done earlier. This decision was based on\n",
      "the performance observed in Karpukhin et al. (2020), where dpr-multiset-base showed superior results.\n",
      "7 Published as a conference paper at ICLR 2024\n",
      "Table 2: QuALITY and QASPER Performance With + Without RAPTOR: Performance com-\n",
      "parison across the QuALITY and QASPER datasets of various retrieval methods (SBERT, BM25,\n",
      "DPR) with and without RAPTOR. UnifiedQA-3B is used as the language model. RAPTOR outper-\n",
      "forms baselines of each respective retrieval method for both datasets.\n",
      "Model Accuracy (QuALITY) Answer F1 (QASPER)\n",
      "SBERT with RAPTOR 56.6% 36.70%\n",
      "SBERT without RAPTOR 54.9% 36.23%\n",
      "BM25 with RAPTOR 52.1% 27.00%\n",
      "BM25 without RAPTOR 49.9% 26.47%\n",
      "DPR with RAPTOR 54.7% 32.23%\n",
      "DPR without RAPTOR 53.1% 31.70%\n",
      "Table 3: Controlled comparison of F-1 scores on the QASPER dataset, using three different lan-\n",
      "guage models (GPT-3, GPT-4, UnifiedQA 3B) and various retrieval methods. The column ”Title +\n",
      "Abstract” reflects performance when only the title and abstract of the papers are used for context.\n",
      "RAPTOR outperforms the established baselines BM25 and DPR across all tested language models.\n",
      "Specifically, RAPTOR’s F-1 scores are at least 1.8% points higher than DPR and at least 5.3% points\n",
      "higher than BM25.\n",
      "Retriever GPT-3 F-1 Match GPT-4 F-1 Match UnifiedQA F-1 Match\n",
      "Title + Abstract 25.2 22.2 17.5\n",
      "BM25 46.6 50.2 26.4\n",
      "DPR 51.3 53.0 32.1\n",
      "RAPTOR 53.1 55.7 36.6\n",
      "Table 4: Comparison of accuracies on the QuAL-\n",
      "ITY dev dataset for two different language mod-\n",
      "els (GPT-3, UnifiedQA 3B) using various retrieval\n",
      "methods. RAPTOR outperforms the baselines of\n",
      "BM25 and DPR by at least 2.0% in accuracy.\n",
      "Model GPT-3 Acc. UnifiedQA Acc.\n",
      "BM25 57.3 49.9\n",
      "DPR 60.4 53.9\n",
      "RAPTOR 62.4 56.6\n",
      "Table 5: Results on F-1 Match scores of various\n",
      "models on the QASPER dataset.\n",
      "Model F-1 Match\n",
      "LongT5 XL (Guo et al., 2022) 53.1\n",
      "CoLT5 XL (Ainslie et al., 2023) 53.9\n",
      "RAPTOR + GPT-4 55.7Comparison to State-of-the-art Systems\n",
      "Building upon our controlled comparisons,\n",
      "we examine RAPTOR’s performance relative\n",
      "to other state-of-the-art models. As shown\n",
      "in Table 5, RAPTOR with GPT-4 sets a new\n",
      "benchmark on QASPER, with a 55.7% F-1\n",
      "score, surpassing the CoLT5 XL’s score of\n",
      "53.9%.\n",
      "In the QuALITY dataset, as shown in Table 7,\n",
      "RAPTOR paired with GPT-4 sets a new state-\n",
      "of-the-art with an accuracy of 82.6%, surpass-\n",
      "ing the previous best result of 62.3%. In par-\n",
      "ticular, it outperforms CoLISA by 21.5% on\n",
      "QuALITY-HARD, which represents questions\n",
      "that humans took unusually long to correctly\n",
      "answer, requiring rereading parts of the text,\n",
      "difficult reasoning, or both.\n",
      "For the NarrativeQA dataset, as represented in\n",
      "Table 6, RAPTOR paired with UnifiedQA sets\n",
      "a new state-of-the-art METEOR score. When compared to the recursively summarizing model by\n",
      "Wu et al. (2021), which also employs UnifiedQA, RAPTOR outperforms it on all metrics. While\n",
      "Wu et al. (2021) rely solely on the summary in the top root node of the tree structure, RAPTOR\n",
      "benefits from its intermediate layers and clustering approaches, which allows it to capture a range of\n",
      "information, from general themes to specific details, contributing to its overall strong performance.\n",
      "4.1 C ONTRIBUTION OF THE TREE STRUCTURE\n",
      "We examine the contribution of each layer of nodes to RAPTOR’s retrieval capabilities. We hy-\n",
      "pothesized that upper nodes play a crucial role in handling thematic or multi-hop queries requiring\n",
      "a broader understanding of the text.\n",
      "8 Published as a conference paper at ICLR 2024\n",
      "Table 6: Performance comparison on the NarrativeQA dataset across multiple models, focusing\n",
      "on four metrics: ROUGE-L, BLEU-1, BLEU-4, and METEOR. RAPTOR, when paired with Uni-\n",
      "fiedQA 3B, not only surpasses retrieval methods like BM25 and DPR but also sets a new state-of-\n",
      "the-art in the METEOR metric.\n",
      "Model ROUGE-L BLEU-1 BLEU-4 METEOR\n",
      "BiDAF (Ko ˇcisk`y et al., 2018) 6.2 5 .7 0 .3 3 .7\n",
      "BM25 + BERT (Mou et al., 2020) 15.5 14 .5 1 .4 5 .0\n",
      "Recursively Summarizing Books (Wu et al., 2021) 21.6 22 .3 4 .2 10 .6\n",
      "Retriever + Reader (Izacard & Grave, 2022) 32.0 35.3 7.5 11.1\n",
      "RAPTOR + UnifiedQA 30.8 23.5 6.4 19.1\n",
      "Table 7: Accuracies of the QuALITY dataset on both the overall test set and the more challenging\n",
      "hard subset. GPT-4 with RAPTOR sets a new state-of-the-art.\n",
      "ModelAccuracy\n",
      "Test Set Hard Subset\n",
      "Longformer-base (Beltagy et al., 2020) 39.5 35 .3\n",
      "DPR and DeBERTaV3-large (Pang et al., 2022) 55.4 46 .1\n",
      "CoLISA (DeBERTaV3-large) (Dong et al., 2023a) 62.3 54 .7\n",
      "RAPTOR + GPT-4 82.6 76.2\n",
      "Table 8: Performance of RAPTOR when querying different tree layers for Story 1 from the QuAL-\n",
      "ITY dataset. Columns represent different starting points (highest layer) and rows represent different\n",
      "numbers of layers queried.\n",
      "Layers Queried / Start Layer Layer 0 (Leaf Nodes) Layer 1 Layer 2\n",
      "1 layer 57.9 57.8 57.9\n",
      "2 layers - 52.6 63.15\n",
      "3 layers - - 73.68\n",
      "We validated this hypothesis both quantitatively and qualitatively. We present qualitative analysis in\n",
      "appendix G. To quantitatively understand the contribution of the upper-level nodes, we used stories\n",
      "from the QuALITY dataset. The RAPTOR tree is built for each of these stories, as described in\n",
      "Section 3. However, during retrieval, we limit the search to different subsets of layers. For example,\n",
      "we exclusively retrieve from the leaf nodes and each upper layer, as well as from different contiguous\n",
      "subsets of the layers. We show findings specific to one story in Table 8, revealing that a full-tree\n",
      "search, utilizing all layers, outperformed retrieval strategies that focused only on specific layers.\n",
      "These findings highlight the importance of the full tree structure in RAPTOR. By providing both\n",
      "the original text and higher-level summaries for retrieval, RAPTOR can effectively handle a wider\n",
      "range of questions, from higher-order thematic queries to detail-oriented questions. Detailed results\n",
      "for additional stories and an ablation study on layer contributions can be found in Appendix I.\n",
      "5 C ONCLUSION\n",
      "In this paper, we have presented RAPTOR, a novel tree-based retrieval system that augments the\n",
      "parametric knowledge of large language models with contextual information at various levels of\n",
      "abstraction. By employing recursive clustering and summarization techniques, RAPTOR creates a\n",
      "hierarchical tree structure that is capable of synthesizing information across various sections of the\n",
      "retrieval corpora. During the query phase, RAPTOR leverages this tree structure for more effective\n",
      "retrieval. Our controlled experiments demonstrated that RAPTOR not only outperforms traditional\n",
      "retrieval methods but also sets new performance benchmarks on several question-answering tasks.\n",
      "9 Published as a conference paper at ICLR 2024\n",
      "6 R EPRODUCIBILITY STATEMENT\n",
      "Language Models for QA and Summarization Four language models are used in our RAPTOR\n",
      "experiments: GPT-3 and GPT-4 for QA tasks, and GPT-3.5-turbo for summarization. The gpt-3 ,\n",
      "gpt-4 , and gpt-3.5-turbo models can be accessed via API calls (OpenAI API). UnifiedQA,\n",
      "which is used for QA tasks, is publicly available at Hugging Face.\n",
      "Evaluation Datasets The three evaluation datasets used in our experiments—QuALITY,\n",
      "QASPER, and NarrativeQA—are all publicly accessible. These datasets ensure that the retrieval\n",
      "and QA tests conducted in this study can be replicated.\n",
      "Source Code The source code for RAPTOR will be publicly available here.\n",
      "REFERENCES\n",
      "Charu C Aggarwal, Alexander Hinneburg, and Daniel A Keim. On the Surprising Behavior of Dis-\n",
      "tance Metrics in High Dimensional Space. In Database Theory—ICDT 2001: 8th International\n",
      "Conference London, UK, January 4–6, 2001 Proceedings 8 , pp. 420–434. Springer, 2001. URL\n",
      "https://link.springer.com/chapter/10.1007/3-540-44503-x_27 .\n",
      "Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Onta ˜n´on, Siddhartha Brahma, Yury Zemlyan-\n",
      "skiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, et al. CoLT5: Faster long-range\n",
      "transformers with conditional computation. arXiv preprint arXiv:2303.09752 , 2023. URL\n",
      "https://arxiv.org/abs/2303.09752 .\n",
      "Ekin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and\n",
      "Kelvin Guu. Towards tracing knowledge in language models back to the training data. In\n",
      "Findings of the Association for Computational Linguistics: EMNLP 2022 , pp. 2429–2446,\n",
      "Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.\n",
      "doi: 10.18653/v1/2022.findings-emnlp.180. URL https://aclanthology.org/2022.\n",
      "findings-emnlp.180 .\n",
      "Stefanos Angelidis and Mirella Lapata. Summarizing opinions: Aspect extraction meets sentiment\n",
      "prediction and they are both weakly supervised. arXiv preprint arXiv:1808.08858 , 2018. URL\n",
      "https://arxiv.org/abs/1808.08858 .\n",
      "Manoj Ghuhan Arivazhagan, Lan Liu, Peng Qi, Xinchi Chen, William Yang Wang, and Zhiheng\n",
      "Huang. Hybrid hierarchical retrieval for open-domain question answering. In Anna Rogers,\n",
      "Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational\n",
      "Linguistics: ACL 2023 , pp. 10680–10689, Toronto, Canada, July 2023. Association for Computa-\n",
      "tional Linguistics. doi: 10.18653/v1/2023.findings-acl.679. URL https://aclanthology.\n",
      "org/2023.findings-acl.679 .\n",
      "Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The Long-document Transformer,\n",
      "2020. URL https://arxiv.org/abs/2004.05150 . arXiv preprint arXiv:2004.05150.\n",
      "Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Milli-\n",
      "can, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al.\n",
      "Improving language models by retrieving from trillions of tokens. In International conference on\n",
      "machine learning , pp. 2206–2240. PMLR, 2022. URL https://arxiv.org/abs/2112.\n",
      "04426 .\n",
      "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\n",
      "wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\n",
      "Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel\n",
      "Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\n",
      "Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-\n",
      "ford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. In\n",
      "H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neu-\n",
      "ral Information Processing Systems , volume 33, pp. 1877–1901. Curran Associates, Inc.,\n",
      "10 Published as a conference paper at ICLR 2024\n",
      "2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/\n",
      "file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .\n",
      "S´ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka-\n",
      "mar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of Artificial General\n",
      "Intelligence: Early Experiments with GPT-4. arXiv preprint arXiv:2303.12712 , 2023. URL\n",
      "https://arxiv.org/abs/2303.12712 .\n",
      "Shuyang Cao and Lu Wang. HIBRIDS: Attention with hierarchical biases for structure-aware long\n",
      "document summarization. In Proceedings of the 60th Annual Meeting of the Association for\n",
      "Computational Linguistics (Volume 1: Long Papers) , pp. 786–807, Dublin, Ireland, May 2022.\n",
      "Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.58. URL https:\n",
      "//aclanthology.org/2022.acl-long.58 .\n",
      "Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer\n",
      "Open-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for\n",
      "Computational Linguistics (Volume 1: Long Papers) , pp. 1870–1879, Vancouver, Canada, July\n",
      "2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https:\n",
      "//aclanthology.org/P17-1171 .\n",
      "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\n",
      "Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM:\n",
      "Scaling Language Modeling with Pathways. arXiv preprint arXiv:2204.02311 , 2022. URL\n",
      "https://arxiv.org/abs/2204.02311 .\n",
      "Arman Cohan and Nazli Goharian. Contextualizing citations for scientific summarization using\n",
      "word embeddings and domain knowledge. In Proceedings of the 40th International ACM SIGIR\n",
      "Conference on Research and Development in Information Retrieval , pp. 1133–1136, 2017. URL\n",
      "https://dl.acm.org/doi/abs/10.1145/3077136.3080740 .\n",
      "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.\n",
      "Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the\n",
      "57th Annual Meeting of the Association for Computational Linguistics , pp. 2978–2988, Florence,\n",
      "Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL\n",
      "https://aclanthology.org/P19-1285 .\n",
      "Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R ´e. FlashAttention: Fast and\n",
      "memory-efficient exact attention with IO-Awareness. Advances in Neural Information Processing\n",
      "Systems , 35:16344–16359, 2022. URL https://arxiv.org/abs/2205.14135 .\n",
      "Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A Dataset\n",
      "of Information-Seeking Questions and Answers Anchored in Research Papers. In Proceed-\n",
      "ings of the 2021 Conference of the North American Chapter of the Association for Computa-\n",
      "tional Linguistics: Human Language Technologies , pp. 4599–4610, Online, June 2021. Asso-\n",
      "ciation for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.365. URL https:\n",
      "//aclanthology.org/2021.naacl-main.365 .\n",
      "Mengxing Dong, Bowei Zou, Yanling Li, and Yu Hong. CoLISA: Inner Interaction via Contrastive\n",
      "Learning for Multi-choice Reading Comprehension. In Advances in Information Retrieval: 45th\n",
      "European Conference on Information Retrieval, ECIR 2023, Dublin, Ireland, April 2–6, 2023,\n",
      "Proceedings, Part I , pp. 264–278. Springer, 2023a. URL https://link.springer.com/\n",
      "chapter/10.1007/978-3-031-28244-7_17 .\n",
      "Zican Dong, Tianyi Tang, Lunyi Li, and Wayne Xin Zhao. A survey on long text modeling with\n",
      "transformers. arXiv preprint arXiv:2302.14502 , 2023b. URL https://arxiv.org/abs/\n",
      "2302.14502 .\n",
      "Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to generate\n",
      "text with citations. arXiv preprint arXiv:2305.14627 , 2023. URL https://arxiv.org/\n",
      "abs/2305.14627 .\n",
      "11 Published as a conference paper at ICLR 2024\n",
      "Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and\n",
      "Yinfei Yang. LongT5: Efficient text-to-text transformer for long sequences. In Findings of the\n",
      "Association for Computational Linguistics: NAACL 2022 , pp. 724–736, Seattle, United States,\n",
      "July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-naacl.55.\n",
      "URLhttps://aclanthology.org/2022.findings-naacl.55 .\n",
      "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval Augmented\n",
      "Language Model Pre-Training. In International conference on machine learning , pp. 3929–3938.\n",
      "PMLR, 2020. URL https://doi.org/10.48550/arXiv.2002.08909 .\n",
      "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\n",
      "Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.\n",
      "Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022. URL\n",
      "https://arxiv.org/abs/2203.15556 .\n",
      "Gautier Izacard and Edouard Grave. Distilling Knowledge from Reader to Retriever for Ques-\n",
      "tion Answering, 2022. URL https://arxiv.org/abs/2012.04584 . arXiv preprint\n",
      "arXiv:2012.04584.\n",
      "Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\n",
      "Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with re-\n",
      "trieval augmented language models. arXiv preprint arXiv:2208.03299 , 2022. URL https:\n",
      "//arxiv.org/abs/2208.03299 .\n",
      "Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language\n",
      "models know? Transactions of the Association for Computational Linguistics , 8:423–438, 2020.\n",
      "URL https://arxiv.org/abs/1911.12543 .\n",
      "Jeff Johnson, Matthijs Douze, and Herv ´e J´egou. Billion-Scale Similarity Search with GPUs. IEEE\n",
      "Transactions on Big Data , 7(3):535–547, 2019. URL https://arxiv.org/abs/1702.\n",
      "08734 .\n",
      "Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large Language\n",
      "Models struggle to learn Long-Tail Knowledge. In International Conference on Machine Learn-\n",
      "ing, pp. 15696–15707. PMLR, 2023. URL https://proceedings.mlr.press/v202/\n",
      "kandpal23a/kandpal23a.pdf .\n",
      "Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi\n",
      "Chen, and Wen-tau Yih. Dense Passage Retrieval for Open-Domain Question Answering. In\n",
      "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n",
      "(EMNLP) , pp. 6769–6781, Online, November 2020. Association for Computational Linguis-\n",
      "tics. doi: 10.18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.\n",
      "emnlp-main.550 .\n",
      "Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and\n",
      "Hannaneh Hajishirzi. UNIFIEDQA: Crossing format boundaries with a single QA system.\n",
      "InFindings of the Association for Computational Linguistics: EMNLP 2020 , pp. 1896–1907,\n",
      "Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\n",
      "findings-emnlp.171. URL https://aclanthology.org/2020.findings-emnlp.\n",
      "171.\n",
      "Omar Khattab and Matei Zaharia. ColBERT: Efficient and effective passage search via con-\n",
      "textualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR\n",
      "conference on research and development in Information Retrieval , pp. 39–48, 2020. URL\n",
      "https://arxiv.org/abs/2004.12832 .\n",
      "Tom´aˇs Ko ˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G ´abor Melis,\n",
      "and Edward Grefenstette. The NarrativeQA Reading Comprehension Challenge. Transactions\n",
      "of the Association for Computational Linguistics , 6:317–328, 2018. URL https://arxiv.\n",
      "org/abs/1712.07040 .\n",
      "12 Published as a conference paper at ICLR 2024\n",
      "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\n",
      "Heinrich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt ¨aschel, et al. Retrieval-Augmented Gener-\n",
      "ation for Knowledge-Intensive NLP Tasks. Advances in Neural Information Processing Systems ,\n",
      "33:9459–9474, 2020. URL https://doi.org/10.48550/arXiv.2005.11401 .\n",
      "Jerry Liu. LlamaIndex, 2022. URL https://github.com/jerryjliu/llama_index .\n",
      "Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and\n",
      "Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint\n",
      "arXiv:2307.03172 , 2023. URL https://arxiv.org/abs/2307.03172 .\n",
      "Ye Liu, Kazuma Hashimoto, Yingbo Zhou, Semih Yavuz, Caiming Xiong, and Philip Yu. Dense\n",
      "hierarchical retrieval for open-domain question answering. In Marie-Francine Moens, Xuanjing\n",
      "Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Findings of the Association for Compu-\n",
      "tational Linguistics: EMNLP 2021 , pp. 188–200, Punta Cana, Dominican Republic, Novem-\n",
      "ber 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.19.\n",
      "URL https://aclanthology.org/2021.findings-emnlp.19 .\n",
      "Leland McInnes, John Healy, and James Melville. UMAP: Uniform Manifold Approximation\n",
      "and Projection for Dimension Reduction, 2018. URL https://arxiv.org/abs/1802.\n",
      "03426 . arXiv preprint arXiv:1802.03426.\n",
      "Sewon Min, Kenton Lee, Ming-Wei Chang, Kristina Toutanova, and Hannaneh Hajishirzi. Joint\n",
      "passage ranking for diverse multi-answer retrieval. In Marie-Francine Moens, Xuanjing Huang,\n",
      "Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical\n",
      "Methods in Natural Language Processing , pp. 6997–7008, Online and Punta Cana, Dominican\n",
      "Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.\n",
      "emnlp-main.560. URL https://aclanthology.org/2021.emnlp-main.560 .\n",
      "Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and Luke\n",
      "Zettlemoyer. Nonparametric masked language modeling. In Findings of the Association for\n",
      "Computational Linguistics: ACL 2023 , pp. 2097–2118, Toronto, Canada, July 2023. Associ-\n",
      "ation for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.132. URL https:\n",
      "//aclanthology.org/2023.findings-acl.132 .\n",
      "Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn.\n",
      "Memory-based model editing at scale. In International Conference on Machine Learning ,\n",
      "pp. 15817–15831. PMLR, 2022. URL https://proceedings.mlr.press/v162/\n",
      "mitchell22a/mitchell22a.pdf .\n",
      "Xiangyang Mou, Mo Yu, Bingsheng Yao, Chenghao Yang, Xiaoxiao Guo, Saloni Potdar, and Hui\n",
      "Su. Frustratingly hard evidence retrieval for QA over books. In Proceedings of the First Joint\n",
      "Workshop on Narrative Understanding, Storylines, and Events , pp. 108–113, Online, July 2020.\n",
      "Association for Computational Linguistics. doi: 10.18653/v1/2020.nuse-1.13. URL https:\n",
      "//aclanthology.org/2020.nuse-1.13 .\n",
      "Inderjeet Nair, Aparna Garimella, Balaji Vasan Srinivasan, Natwar Modani, Niyati Chhaya, Srikr-\n",
      "ishna Karanam, and Sumit Shekhar. A neural CRF-based hierarchical approach for lin-\n",
      "ear text segmentation. In Findings of the Association for Computational Linguistics: EACL\n",
      "2023 , pp. 883–893, Dubrovnik, Croatia, May 2023. Association for Computational Linguis-\n",
      "tics. doi: 10.18653/v1/2023.findings-eacl.65. URL https://aclanthology.org/2023.\n",
      "findings-eacl.65 .\n",
      "Benjamin Newman, Luca Soldaini, Raymond Fok, Arman Cohan, and Kyle Lo. A controllable qa-\n",
      "based framework for decontextualization. arXiv preprint arXiv:2305.14772 , 2023. URL https:\n",
      "//arxiv.org/pdf/2305.14772.pdf .\n",
      "OpenAI. GPT-4 Technical Report. ArXiv , abs/2303.08774, 2023. URL https://arxiv.org/\n",
      "abs/2303.08774 .\n",
      "Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen,\n",
      "Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. QuALITY:\n",
      "13 \n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b905cce2-b7ce-4a52-a269-7b9899060eac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:09:30,614 - Creating Leaf Nodes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ecf6b4a99844ccb0afdcdf0474a37b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6400b6e62a346ab93b5f20725213125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d92da50ba34b93972eb4eea0137f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2e767c0b1f4120aac239deafe5221b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50da5c867ac40ed969fafc553532fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed2125c21ab46a794b69f56ef043c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e020df2796b4652a74064644b692831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c497af470a43ce97dd75c42d3788fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4aeeae0693a408dbacee1cd0b2f0b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ab45c99d5c417b9e2831869eaafdce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "befa007dd2ce4adda36b95499c68e770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67a29f530714d5fa68066a9d1db258d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f48aa628d0849d59eb630406a9e4283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c712d365ec4113a8dd4c3f8d17a7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf17bc40614411cadb8f0aca441bf0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2b279503cc4698b687c8174d7380b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35b855f8b424324b48781275e44cdd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ecc2e5308d42f89331efc85816fad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "977a4aabf9e4409a9e8423deba11fff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8552e637974c788f692de8ad496b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc14c2503d2940b78191f3c211b7d150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d648ef8125ac44fca9b693d449585ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5667f6d85c8f4f2a88afd1c5849278e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958bc323fb7344099697cae401844b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b954fc036949cda62ead7f60db70b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba6f6a5695a49048651f19092e6dd61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa1a73c47ad41bf958d709152ca30db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1599c4c6374f83b98b29b1dcbc61e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fdc42d656444a3aa8d3e33127cc7aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba80912d3bd4a15a2212a49e0a69231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c3de2d38fa40bb81518d525a551200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936bc34ad8bb488399c0987b2b571c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cac3e6b07b44efbb92e5a2d78d1aa2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17befcd9bfa4e40a2c6fd533700186b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6fb23f1ae24f5aa9c6ff9caf67bbde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae0f73ed5614921b10aa6a788e8fb8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f05648d7edbb4f90be31c81ce407c244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c65389f694c465e948486244ca6d1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a2010a36a645ed801bc703ac2293ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f5bba1b919476d883e3a90caed0573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5d5047040e4fcabde90f70b9606e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79971a3f7d9c42f2872b14fd3b0c5e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62eeb7ca20ff4f15998065e69293b83b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e941ffce3c4d529aeabe2d04627851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009133554503436c814273410941a18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47fa85a340b43bbaf14459f83bbf000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926b351ed0a946df9d524ed5a32410ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eea531497804ae9918bd6621ff01c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30652faa6066458f9758da60243cb56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026c90e189804943b96bbbb13542a50d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b73fdb6dc1549218d9c9ff9b0140a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650818d8c6114159a248265c8a0f4141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0ac3a6e7b345c8a8b936b97814fabf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a3ca4f9656475fa08edda647d6f2a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec91fd962454f5dbaafcc81416fb1d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5acdb10638e41158b07460dd25b98d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59a3992dc68442faaec38a8dd203b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a8bbd834f044ddacfc6395725b0605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b890c449dc4374beef273ba222acbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a32b8b1ae564ee698c52fcee7c03897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12136ff687f14510b32bba86e3d480c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07db47d60f84a9b8ded8f48f8c82d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469dd51d11eb4058bd523e05c60c18ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864dcd2801a448de986af38e37a85538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af97c7a41f9f4c909126bdd69338939a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f98a3147f8475b85e45125a135c0bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066685610637404d8e3f0b7ee1df99d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882f96230128495d9d7940ac8103a876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9865bb6fc94a8cb2a69b460704b0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0451774d9302487a849cf06f09cd5c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914187a8219f416a82a00bfc7b07582d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33aa4d6a46a54987b1c710e751731cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54406f31013747b9b6a50181e0a24281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa2c9dfee1f474996306bc0524c89fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8d50b854b940959142526f0088f8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687117e6f4364ee5a372d73fe4f91728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de4b6de09aa4654b5a133a3cff42ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a9b0eaf9264e558e2c518c4f1029b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6eeda2ab456406f905bf1bf2c720da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961e40944f57418e9b564c09baeaaee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de580f14b4a4c7f88339ce7f9cddeda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027bd93b0ee647789519a37ab937a3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443b4f8abcd148a9bea4fd6cdd1dbb43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bdeaacc01804cb094edd3360a1a944c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1354c3c3a0bb42908812f78bba554637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12042b1897d42928dfb923c6f6ac771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b94436918e245a5bddfaca677415c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2217a982b44647d4bc18fd4a5dd2d568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a50cadd65a14b84bed9d34c2dd59b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f8e69eaaca4c3da588a6c50d3cb659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9304a10b07954854a6f2c7ac6dfa6cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8dfd0eb757e44308d02c4e5b1f50b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3fa4caec0243178ad0f53a851c67b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8cd34aa27c4442cb016e23a6f046cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10cdea26fb134351a82cb7fc84a67e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96fe64d6e26d49d6963dbc6f50136d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b8a2612cdf74b7cada64f4715dd86e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d8bda8d71e4a2a8f2d607e85da3b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d57feb190443e0b626697ad9af97a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254ed59eae8a4e5eaa2591278c3ab79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c6bddb6c0e45b7bead0fa5c4d573f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37f3e5693b0410fb7ba178df0a85a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3277916b2c4545a0b59adbb2adb6ab5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:09:34,568 - Created 103 Leaf Embeddings\n",
      "2024-06-05 12:09:34,568 - Building All Nodes\n",
      "2024-06-05 12:09:34,571 - Using Cluster TreeBuilder\n",
      "2024-06-05 12:09:34,572 - Constructing Layer 0\n",
      "2024-06-05 12:09:42,759 - Summarization Length: 100\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "2024-06-05 12:09:48,024 - Node Texts Length: 840, Summarized Text Length: 966\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd85c895308449578c0ea25fba1bda58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:09:53,030 - Node Texts Length: 671, Summarized Text Length: 803\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0649af1dca54900ab5811a00d060aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:09:57,434 - Node Texts Length: 385, Summarized Text Length: 521\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d1fdb0a4564ec7bd24baad2451a5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:10:01,460 - Node Texts Length: 202, Summarized Text Length: 340\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfcaeaaca92c47a18d55901f183fa654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:10:02,842 - Node Texts Length: 152, Summarized Text Length: 216\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1972ff657c4992a2013571071aae67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:10:08,597 - Node Texts Length: 1067, Summarized Text Length: 1199\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60953843d1ff4d1889904b57707a36fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:10:13,469 - Node Texts Length: 658, Summarized Text Length: 795\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99411ef31cb4e688ef761fed91e43a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:10:18,127 - Node Texts Length: 486, Summarized Text Length: 626\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0cfc000ec145d18850e94d96b50e19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:10:22,163 - Node Texts Length: 185, Summarized Text Length: 317\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9234d676f8ea433fbfeabf23c3760827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:10:26,685 - Node Texts Length: 383, Summarized Text Length: 514\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53977dd12d7346e3b3fe9c7bdc17613b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:10:31,578 - Node Texts Length: 641, Summarized Text Length: 776\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6646e14ea6e246ae98380e7e999302f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:10:36,843 - Node Texts Length: 764, Summarized Text Length: 901\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496919127c094cf590808dc56999e551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:10:42,662 - Node Texts Length: 1015, Summarized Text Length: 1143\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08afa8f41174339bf01745fdc6ed97c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:10:47,774 - Node Texts Length: 683, Summarized Text Length: 818\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed43fd001e914739a6c4fb3e1100c4b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:10:53,168 - Node Texts Length: 922, Summarized Text Length: 1053\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6963e670e14b7eab7b92edb2355fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:10:58,184 - Node Texts Length: 655, Summarized Text Length: 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df67ec856934a47aa6bc8aea833ead3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:10:58,238 - Constructing Layer 1\n",
      "2024-06-05 12:11:02,916 - Summarization Length: 100\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "2024-06-05 12:11:08,329 - Node Texts Length: 946, Summarized Text Length: 1083\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5605d902b88f49169f4f0afc96408624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:11:14,543 - Node Texts Length: 1243, Summarized Text Length: 1376\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd7f9c63d46466c95d548f851d5a21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:11:23,955 - Node Texts Length: 2490, Summarized Text Length: 2624\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8678097d1b0f4699a37682f4491a67c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:11:25,505 - Node Texts Length: 215, Summarized Text Length: 280\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b275966fdd741d997ad90f307861e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:11:34,045 - Node Texts Length: 2286, Summarized Text Length: 2418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f17fd6c2d7974da2bea7a679b09a9ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:11:42,932 - Node Texts Length: 2377, Summarized Text Length: 2512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4ba5584a0243b48f9c33918ef89211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:11:51,361 - Node Texts Length: 2258, Summarized Text Length: 2390\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee41efba296499cb3a13b0d2ea215cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:11:51,416 - Constructing Layer 2\n",
      "2024-06-05 12:11:51,417 - Stopping Layer construction: Cannot Create More Layers. Total Layers in tree: 2\n",
      "2024-06-05 12:11:51,418 - Successfully initialized TreeRetriever with Config \n",
      "        TreeRetrieverConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Context Embedding Model: EMB\n",
      "            Embedding Model: <__main__.SBertEmbeddingModel object at 0x7f9677eeee20>\n",
      "            Num Layers: None\n",
      "            Start Layer: None\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "RA.add_documents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0ba205d-9389-4c68-ac7d-05daf37ab0c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "311edd55-e184-4848-ae67-a0e623569f67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:11:51,429 - Using collapsed_tree\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2468e66ca88443e49ebdf27a63e01c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer :  Sure, here's a summary of the text:\n",
      "\n",
      "**RAPTOR is a novel tree-based retrieval system that augments the parametric knowledge of large language models with contextual information at various levels of abstraction.**\n",
      "\n",
      "**Key features of RAPTOR:**\n",
      "\n",
      "- **Tree traversal:** Selects the top-k most relevant root nodes based on cosine similarity to the query embedding.\n",
      "- **Collapsed tree:** Uses a hierarchical structure to capture information from different tree layers.\n",
      "\n",
      "**Advantages of RAPTOR:**\n",
      "\n",
      "- More comprehensive information retrieval due to intermediate layers' clustering.\n",
      "- Effective retrieval of both general themes and specific details.\n",
      "\n",
      "**Performance:**\n",
      "\n",
      "- Outperforms traditional retrieval methods.\n",
      "- Sets new performance benchmarks on several question-answering tasks.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is RAPTOR?\"\n",
    "answer = RA.answer_question(question = question)\n",
    "print(\"Answer : \",answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "991da735-9824-464b-8ab9-dec1946245d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:14:08,138 - Tree successfully saved to RAPTOR_index\n"
     ]
    }
   ],
   "source": [
    "SAVE_PATH = 'RAPTOR_index'\n",
    "RA.save(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f0f6a2f-5bc3-4a1e-8839-42340d2ef1d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 13:00:35,814 - Successfully initialized TreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <__main__.GEMMASummarizationModel object at 0x7f2f12a42370>\n",
      "            Embedding Models: {'EMB': <__main__.SBertEmbeddingModel object at 0x7f2f0c6777c0>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2024-06-05 13:00:35,815 - Successfully initialized ClusterTreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <__main__.GEMMASummarizationModel object at 0x7f2f12a42370>\n",
      "            Embedding Models: {'EMB': <__main__.SBertEmbeddingModel object at 0x7f2f0c6777c0>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2024-06-05 13:00:35,815 - Successfully initialized TreeRetriever with Config \n",
      "        TreeRetrieverConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Context Embedding Model: EMB\n",
      "            Embedding Model: <__main__.SBertEmbeddingModel object at 0x7f2f0c6777c0>\n",
      "            Num Layers: None\n",
      "            Start Layer: None\n",
      "        \n",
      "2024-06-05 13:00:35,816 - Successfully initialized RetrievalAugmentation with Config \n",
      "        RetrievalAugmentationConfig:\n",
      "            \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <__main__.GEMMASummarizationModel object at 0x7f2f12a42370>\n",
      "            Embedding Models: {'EMB': <__main__.SBertEmbeddingModel object at 0x7f2f0c6777c0>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "            \n",
      "            \n",
      "        TreeRetrieverConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Context Embedding Model: EMB\n",
      "            Embedding Model: <__main__.SBertEmbeddingModel object at 0x7f2f0c6777c0>\n",
      "            Num Layers: None\n",
      "            Start Layer: None\n",
      "        \n",
      "            \n",
      "            QA Model: <__main__.GEMMAQAModel object at 0x7f301006b2b0>\n",
      "            Tree Builder Type: cluster\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "from raptor import RetrievalAugmentation\n",
    "SAVE_PATH = 'RAPTOR_index'\n",
    "RA = RetrievalAugmentation(config = RAC ,tree=SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e8b2378-5515-435d-b775-d7cc81446137",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 13:00:36,375 - Using collapsed_tree\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05900c82b5f14fada88310547efe26dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer :  Sure, here's a summary of the provided context about RAPTOR:\n",
      "\n",
      "**RAPTOR** is a novel tree-based retrieval system that enhances the parametric knowledge of large language models with contextual information at various levels of abstraction.\n",
      "\n",
      "**Key features of RAPTOR:**\n",
      "\n",
      "- It employs recursive clustering and summarization techniques to create a hierarchical tree structure that captures information from different tree layers.\n",
      "- This hierarchical structure allows RAPTOR to leverage both general themes and specific details of the query.\n",
      "- It provides both the original text and higher-level summaries for retrieval, enabling more comprehensive information access.\n",
      "- RAPTOR outperforms traditional retrieval methods on various question-answering tasks, including the QASPER dataset.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is RAPTOR?\"\n",
    "answer = RA.answer_question(question = question)\n",
    "print(\"Answer : \",answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da2a1240-148b-4af3-aa99-72ed6ab69331",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree = RA.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "196ac97a-ceb6-4c3c-830c-15fa71aaf7f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: <raptor.tree_structures.Node at 0x7f2f151c29a0>,\n",
       " 1: <raptor.tree_structures.Node at 0x7f2f12bd5610>,\n",
       " 3: <raptor.tree_structures.Node at 0x7f2f12a42400>,\n",
       " 6: <raptor.tree_structures.Node at 0x7f3010072a90>,\n",
       " 5: <raptor.tree_structures.Node at 0x7f2f08052f10>,\n",
       " 0: <raptor.tree_structures.Node at 0x7f2f0805e700>,\n",
       " 7: <raptor.tree_structures.Node at 0x7f2f0805e430>,\n",
       " 4: <raptor.tree_structures.Node at 0x7f2f0805e4f0>,\n",
       " 8: <raptor.tree_structures.Node at 0x7f2f0805e730>,\n",
       " 12: <raptor.tree_structures.Node at 0x7f2f0805e670>,\n",
       " 9: <raptor.tree_structures.Node at 0x7f2f0805e8b0>,\n",
       " 10: <raptor.tree_structures.Node at 0x7f2f0805e7f0>,\n",
       " 11: <raptor.tree_structures.Node at 0x7f2f0805e2e0>,\n",
       " 13: <raptor.tree_structures.Node at 0x7f2f0805ea30>,\n",
       " 14: <raptor.tree_structures.Node at 0x7f2f0805e940>,\n",
       " 15: <raptor.tree_structures.Node at 0x7f2f0805e130>,\n",
       " 16: <raptor.tree_structures.Node at 0x7f2f0805eb80>,\n",
       " 19: <raptor.tree_structures.Node at 0x7f2f0805e280>,\n",
       " 17: <raptor.tree_structures.Node at 0x7f2f0805e160>,\n",
       " 20: <raptor.tree_structures.Node at 0x7f2f12a424c0>,\n",
       " 18: <raptor.tree_structures.Node at 0x7f2f0805ef10>,\n",
       " 21: <raptor.tree_structures.Node at 0x7f2f0805efa0>,\n",
       " 22: <raptor.tree_structures.Node at 0x7f2f081450a0>,\n",
       " 23: <raptor.tree_structures.Node at 0x7f2f08145160>,\n",
       " 24: <raptor.tree_structures.Node at 0x7f2f08145220>,\n",
       " 25: <raptor.tree_structures.Node at 0x7f2f081452e0>,\n",
       " 26: <raptor.tree_structures.Node at 0x7f2f081453a0>,\n",
       " 27: <raptor.tree_structures.Node at 0x7f2f08145460>,\n",
       " 28: <raptor.tree_structures.Node at 0x7f2f08145520>,\n",
       " 29: <raptor.tree_structures.Node at 0x7f2f081455e0>,\n",
       " 30: <raptor.tree_structures.Node at 0x7f2f081456a0>,\n",
       " 31: <raptor.tree_structures.Node at 0x7f2f08145760>,\n",
       " 32: <raptor.tree_structures.Node at 0x7f2f08145820>,\n",
       " 33: <raptor.tree_structures.Node at 0x7f2f081458e0>,\n",
       " 35: <raptor.tree_structures.Node at 0x7f2f081459a0>,\n",
       " 34: <raptor.tree_structures.Node at 0x7f2f08145a60>,\n",
       " 36: <raptor.tree_structures.Node at 0x7f2f08145b20>,\n",
       " 38: <raptor.tree_structures.Node at 0x7f2f08145be0>,\n",
       " 37: <raptor.tree_structures.Node at 0x7f2f12a8fd90>,\n",
       " 39: <raptor.tree_structures.Node at 0x7f2f12a8f640>,\n",
       " 40: <raptor.tree_structures.Node at 0x7f2f12a422e0>,\n",
       " 43: <raptor.tree_structures.Node at 0x7f2f12a428b0>,\n",
       " 41: <raptor.tree_structures.Node at 0x7f2f08145cd0>,\n",
       " 44: <raptor.tree_structures.Node at 0x7f2f08145d90>,\n",
       " 42: <raptor.tree_structures.Node at 0x7f2f08145e50>,\n",
       " 45: <raptor.tree_structures.Node at 0x7f2f08145f10>,\n",
       " 46: <raptor.tree_structures.Node at 0x7f2f08145fd0>,\n",
       " 47: <raptor.tree_structures.Node at 0x7f2f081290d0>,\n",
       " 48: <raptor.tree_structures.Node at 0x7f2f08129190>,\n",
       " 49: <raptor.tree_structures.Node at 0x7f2f08129250>,\n",
       " 50: <raptor.tree_structures.Node at 0x7f2f08129310>,\n",
       " 51: <raptor.tree_structures.Node at 0x7f2f081293d0>,\n",
       " 52: <raptor.tree_structures.Node at 0x7f2f08129490>,\n",
       " 53: <raptor.tree_structures.Node at 0x7f2f08129550>,\n",
       " 54: <raptor.tree_structures.Node at 0x7f2f08129610>,\n",
       " 55: <raptor.tree_structures.Node at 0x7f2f081296d0>,\n",
       " 56: <raptor.tree_structures.Node at 0x7f2f08129790>,\n",
       " 57: <raptor.tree_structures.Node at 0x7f2f08129850>,\n",
       " 58: <raptor.tree_structures.Node at 0x7f2f08129910>,\n",
       " 59: <raptor.tree_structures.Node at 0x7f2f081299d0>,\n",
       " 61: <raptor.tree_structures.Node at 0x7f2f08129a90>,\n",
       " 60: <raptor.tree_structures.Node at 0x7f2f08129b50>,\n",
       " 62: <raptor.tree_structures.Node at 0x7f2f08129c10>,\n",
       " 63: <raptor.tree_structures.Node at 0x7f2f08129cd0>,\n",
       " 64: <raptor.tree_structures.Node at 0x7f2f08129d90>,\n",
       " 65: <raptor.tree_structures.Node at 0x7f2f08129e50>,\n",
       " 66: <raptor.tree_structures.Node at 0x7f2f08129f10>,\n",
       " 67: <raptor.tree_structures.Node at 0x7f2f080521f0>,\n",
       " 68: <raptor.tree_structures.Node at 0x7f2f03024040>,\n",
       " 69: <raptor.tree_structures.Node at 0x7f2f03024100>,\n",
       " 70: <raptor.tree_structures.Node at 0x7f2f030241c0>,\n",
       " 71: <raptor.tree_structures.Node at 0x7f2f03024280>,\n",
       " 72: <raptor.tree_structures.Node at 0x7f2f03024340>,\n",
       " 74: <raptor.tree_structures.Node at 0x7f2f03024400>,\n",
       " 73: <raptor.tree_structures.Node at 0x7f2f030244c0>,\n",
       " 75: <raptor.tree_structures.Node at 0x7f2f03024580>,\n",
       " 76: <raptor.tree_structures.Node at 0x7f2f03024640>,\n",
       " 77: <raptor.tree_structures.Node at 0x7f2f03024700>,\n",
       " 78: <raptor.tree_structures.Node at 0x7f2f030247c0>,\n",
       " 79: <raptor.tree_structures.Node at 0x7f2f03024880>,\n",
       " 80: <raptor.tree_structures.Node at 0x7f2f03024940>,\n",
       " 82: <raptor.tree_structures.Node at 0x7f2f03024a00>,\n",
       " 81: <raptor.tree_structures.Node at 0x7f2f03024ac0>,\n",
       " 84: <raptor.tree_structures.Node at 0x7f2f03024b80>,\n",
       " 83: <raptor.tree_structures.Node at 0x7f2f03024c40>,\n",
       " 85: <raptor.tree_structures.Node at 0x7f2f03024d00>,\n",
       " 86: <raptor.tree_structures.Node at 0x7f2f03024dc0>,\n",
       " 87: <raptor.tree_structures.Node at 0x7f2f03024e80>,\n",
       " 88: <raptor.tree_structures.Node at 0x7f2f03024f40>,\n",
       " 89: <raptor.tree_structures.Node at 0x7f2f03011040>,\n",
       " 90: <raptor.tree_structures.Node at 0x7f2f03011100>,\n",
       " 91: <raptor.tree_structures.Node at 0x7f2f030111c0>,\n",
       " 92: <raptor.tree_structures.Node at 0x7f2f03011280>,\n",
       " 93: <raptor.tree_structures.Node at 0x7f2f03011340>,\n",
       " 94: <raptor.tree_structures.Node at 0x7f2f03011400>,\n",
       " 95: <raptor.tree_structures.Node at 0x7f2f030114c0>,\n",
       " 96: <raptor.tree_structures.Node at 0x7f2f03011580>,\n",
       " 97: <raptor.tree_structures.Node at 0x7f2f03011640>,\n",
       " 98: <raptor.tree_structures.Node at 0x7f2f03011700>,\n",
       " 99: <raptor.tree_structures.Node at 0x7f2f030117c0>,\n",
       " 100: <raptor.tree_structures.Node at 0x7f2f03011880>,\n",
       " 101: <raptor.tree_structures.Node at 0x7f2f03011940>,\n",
       " 102: <raptor.tree_structures.Node at 0x7f2f03011a00>,\n",
       " 103: <raptor.tree_structures.Node at 0x7f2f03011ac0>,\n",
       " 104: <raptor.tree_structures.Node at 0x7f2f03011b80>,\n",
       " 105: <raptor.tree_structures.Node at 0x7f2f03011c40>,\n",
       " 106: <raptor.tree_structures.Node at 0x7f2f03011d00>,\n",
       " 107: <raptor.tree_structures.Node at 0x7f2f03011dc0>,\n",
       " 108: <raptor.tree_structures.Node at 0x7f2f03011e80>,\n",
       " 109: <raptor.tree_structures.Node at 0x7f2f03011f40>,\n",
       " 110: <raptor.tree_structures.Node at 0x7f2f03019040>,\n",
       " 111: <raptor.tree_structures.Node at 0x7f2f03019100>,\n",
       " 112: <raptor.tree_structures.Node at 0x7f2f030191c0>,\n",
       " 113: <raptor.tree_structures.Node at 0x7f2f03019280>,\n",
       " 114: <raptor.tree_structures.Node at 0x7f2f03019340>,\n",
       " 115: <raptor.tree_structures.Node at 0x7f2f03019400>,\n",
       " 116: <raptor.tree_structures.Node at 0x7f2f030194c0>,\n",
       " 117: <raptor.tree_structures.Node at 0x7f2f03019580>,\n",
       " 118: <raptor.tree_structures.Node at 0x7f2f03019640>,\n",
       " 119: <raptor.tree_structures.Node at 0x7f2f03019700>,\n",
       " 120: <raptor.tree_structures.Node at 0x7f2f030197c0>,\n",
       " 121: <raptor.tree_structures.Node at 0x7f2f03019880>,\n",
       " 122: <raptor.tree_structures.Node at 0x7f2f03019940>,\n",
       " 123: <raptor.tree_structures.Node at 0x7f2f03019a00>,\n",
       " 124: <raptor.tree_structures.Node at 0x7f2f03019ac0>,\n",
       " 125: <raptor.tree_structures.Node at 0x7f2f03019b80>}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = tree.all_nodes\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bbf0793-f32c-4d88-8ea6-dd26431585cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91a6c56e-94af-4b32-92df-d01cf9bd9b3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_tree_layers(root_nodes):\n",
    "    all_nodes = tree.all_nodes\n",
    "    current_layer = list(root_nodes.values())\n",
    "    level = 0\n",
    "    while current_layer:\n",
    "        print(f\"============================================level {level}============================================\")\n",
    "        next_layer = []\n",
    "        for node in current_layer:\n",
    "            print(f\"Index: {node.index}, Text: {node.text}\\n\")\n",
    "            next_layer.extend(all_nodes.get(child_index) for child_index in node.children)\n",
    "            \n",
    "        current_layer = next_layer\n",
    "        level+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f453b887-2d1d-4b82-a80f-c8dc99684f67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================level 0============================================\n",
      "Index: 119, Text: <bos><start_of_turn>user\n",
      "Write a summary of the following, including as many key details as possible: <bos><start_of_turn>user Write a summary of the following, including as many key details as possible:  As shown in Table 3, RAPTOR consistently outperforms BM25 and DPR across all three Language Models on the QASPER dataset  RAPTOR’s F-1 Match scores are 53 1%, 55 7%, and 36 6% when using GPT-3, GPT-4, and UnifiedQA, respectively  These scores surpass DPR by margins of 1 8, 2 7, and 4  Abstract” reflects performance when only the title and abstract of the papers are used for context RAPTOR outperforms the established baselines BM25 and DPR across all tested language models Specifically, RAPTOR’s F-1 scores are at least 1 8% points higher than DPR and at least 5 3% points higher than BM25 Retriever GPT-3 F-1 Match GPT-4 F-1 Match UnifiedQA F-1 Match   RAPTOR outperforms the baselines of BM25 and DPR by at least 2 0% in accuracy Model GPT-3 Acc  UnifiedQA Acc BM25 57 3 49 9 DPR 60 4 53 9 RAPTOR 62 4 56 6 Table 5: Results on F-1 Match scores of various models on the QASPER dataset Model F-1 Match LongT5 XL (Guo et al  , 2022) 53 1 CoLT5 XL (Ainslie et al , 2023) 53 9 RAPTOR + GPT-4 55 7Comparison to State-of-the-art Systems Building upon our controlled comparisons, we examine RAPTOR’s performance relative to other state-of-the-art models  As shown in Table 5, RAPTOR with GPT-4 sets a new benchmark on QASPER, with a 55  7% F-1 score, surpassing the CoLT5 XL’s score of 53 9% In the QuALITY dataset, as shown in Table 7, RAPTOR paired with GPT-4 sets a new state- of-the-art with an accuracy of 82 6%, surpass- ing the previous best result of 62 3%  In par- ticular, it outperforms CoLISA by 21 5% on  :<end_of_turn> <start_of_turn>model Sure, here is a summary of the passage:  * RAPTOR consistently outperforms BM25 and DPR across all tested language models on the QASPER dataset. * RAPTOR’s F-1 Match scores are at least 1 8% points higher than DPR and at least 5 3% points higher than BM25 Retriever GPT-3 F-1 Match GPT-4 F-1 Match UnifiedQA F-1 Match. * RAPTOR outperforms the\n",
      "\n",
      "<bos><start_of_turn>user Write a summary of the following, including as many key details as possible: fiedQA is employed, with RAPTOR outperforming DPR and BM25 by 2 7% and 6 7%, respectively Finally, in the NarrativeQA dataset, as presented in Table 6, RAPTOR excels across multiple met- rics  For ROUGE-L, it surpasses BM25 and DPR by 7 3 and 2 7 points, respectively  In other  fiedQA 3B, not only surpasses retrieval methods like BM25 and DPR but also sets a new state-of- the-art in the METEOR metric Model ROUGE-L BLEU-1 BLEU-4 METEOR BiDAF (Ko ˇcisk`y et al , 2018) 6 2 5  7 0  3 3  7 BM25 + BERT (Mou et al  :<end_of_turn> <start_of_turn>model Sure, here's a summary of the passage:  * fiedQA outperforms other retrieval methods, including DPR and BM25, by a significant margin in the NarrativeQA dataset. * RAPTOR outperforms DPR and BM25 by 2 7% and 6 7%, respectively. * In the ROUGE-L metric, RAPTOR surpasses BM25 and DPR by 7 3 and 2 7 points, respectively. * In the\n",
      "\n",
      ":<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here's a summary of the passage:\n",
      "\n",
      "The passage compares the performance of RAPTOR, a large language model, against other state-of-the-art retrieval models on the QASPER dataset. Results indicate that RAPTOR achieves the highest F-1 Match scores among the models tested. Specifically, it outperforms DPR by 2 7%, and at least 5 3% higher than BM25 Retriever GPT-4 F-1 Match GPT-4 F-\n",
      "\n",
      "Index: 120, Text: <bos><start_of_turn>user\n",
      "Write a summary of the following, including as many key details as possible: <bos><start_of_turn>user Write a summary of the following, including as many key details as possible: Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad- ford, Ilya Sutskever, and Dario Amodei  Language Models are Few-Shot Learners  In H  Larochelle, M  Ranzato, R  Hadsell, M F  Balcan, and H  Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave  Few-shot learning with re- trieval augmented language models  arXiv preprint arXiv:2208 03299 , 2022  URL https: //arxiv org/abs/2208 03299   :<end_of_turn> <start_of_turn>model Sure, here is a summary of the paper:  The paper explores the Few-Shot Learning (FSL) task for language models, focusing on the re-trieval augmented language model approach. The study investigates the effectiveness of different augmentation strategies and tri-evaluation methods on this task.  **Key details:**  * **Language models:**     * Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess\n",
      "\n",
      "<bos><start_of_turn>user Write a summary of the following, including as many key details as possible: Kelvin Guu  Towards tracing knowledge in language models back to the training data  In Findings of the Association for Computational Linguistics: EMNLP 2022 , pp  2429–2446, Abu Dhabi, United Arab Emirates, December 2022  Association for Computational Linguistics doi: 10 18653/v1/2022 findings-emnlp 180  URL https://aclanthology org/2022 findings-emnlp 180   can, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al Improving language models by retrieving from trillions of tokens  In International conference on machine learning , pp  2206–2240  PMLR, 2022  URL https://arxiv org/abs/2112 04426   18653/v1/P17-1171  URL https: //aclanthology org/P17-1171  Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al  PaLM: Scaling Language Modeling with Pathways  arXiv preprint arXiv:2204   arXiv preprint arXiv:2302 14502 , 2023b  URL https://arxiv org/abs/ 2302 14502  Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen  Enabling large language models to generate text with citations  arXiv preprint arXiv:2305 14627 , 2023  URL https://arxiv org/ abs/2305   URL https://doi org/10 48550/arXiv 2002 08909  Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al Training compute-optimal large language models  arXiv preprint arXiv:2203 15556 , 2022  URL  Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig  How can we know what language models know  Transactions of the Association for Computational Linguistics , 8:423–438, 2020 URL https://arxiv org/abs/1911 12543  Jeff Johnson, Matthijs Douze, and Herv ´e J´egou  Billion-Scale Similarity Search with GPUs  IEEE  Transactions on Big Data , 7(3):535–547, 2019  URL https://arxiv org/abs/1702 08734  Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel  Large Language Models struggle to learn Long-Tail Knowledge  In International Conference on Machine Learn- ing, pp  15696–15707  PMLR, 2023  URL https://proceedings  ation for Computational Linguistics  doi: 10 18653/v1/2023 findings-acl 132  URL https: //aclanthology org/2023 findings-acl 132  Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn Memory-based model editing at scale  In International Conference on Machine Learning , pp  15817–15831  PMLR, 2022  URL https://proceedings mlr  :<end_of_turn> <start_of_turn>model Sure, here is a summary of the paper:  **Title:** Improving Language Models by Retrieving from Trillions of Tokens  **Authors:** Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann  **Conference:** International Conference on Machine Learning (ICML)  **Abstract:** The paper proposes a novel approach for improving language\n",
      "\n",
      ":<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here is a summary of the paper:\n",
      "\n",
      "The paper explores the Few-Shot Learning (FSL) task for language models, focusing on the re-trieval augmented language model approach. The study investigates the effectiveness of different augmentation strategies and tri-evaluation methods on this task.\n",
      "\n",
      "**Key details:**\n",
      "\n",
      "* The paper investigates the effectiveness of different augmentation strategies, including fine-tuning, transfer learning, and tri-evaluation.\n",
      "* The paper also explores the effectiveness of different tri-\n",
      "\n",
      "Index: 121, Text: <bos><start_of_turn>user\n",
      "Write a summary of the following, including as many key details as possible: <bos><start_of_turn>user Write a summary of the following, including as many key details as possible: S´ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka- mar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al  Sparks of Artificial General Intelligence: Early Experiments with GPT-4  arXiv preprint arXiv:2303 12712 , 2023  URL https://arxiv   doi: 10 18653/v1/P19-1285  URL https://aclanthology org/P19-1285  Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R ´e  FlashAttention: Fast and memory-efficient exact attention with IO-Awareness  Advances in Neural Information Processing Systems , 35:16344–16359, 2022  URL https://arxiv org/abs/2205  14135  Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A  Smith, and Matt Gardner  A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers  In Proceed- ings of the 2021 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies , pp  4599–4610, Online, June 2021  Asso-  12 Published as a conference paper at ICLR 2024 Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt ¨aschel, et al  Retrieval-Augmented Gener- ation for Knowledge-Intensive NLP Tasks  Advances in Neural Information Processing Systems ,  :<end_of_turn> <start_of_turn>model **Summary:**  The paper presents early experiments with the GPT-4 language model, exploring its potential for knowledge-intensive NLP tasks. The researchers focus on the \"Retrieval-Augmented Gener-a-tion for Knowledge-Intensive NLP Tasks\" approach, which combines the strengths of retrieval-based models and large language models.  **Key Details:**  * GPT-4 is a large language model (LLM) with a massive dataset and advanced capabilities. * The paper explores the ability\n",
      "\n",
      "<bos><start_of_turn>user Write a summary of the following, including as many key details as possible: , 2020) 15 5 14  5 1  4 5  0 Recursively Summarizing Books (Wu et al , 2021) 21 6 22  3 4  2 10  6 Retriever + Reader (Izacard & Grave, 2022) 32 0 35 3 7 5 11 1 RAPTOR + UnifiedQA 30  02311 , 2022  URL https://arxiv org/abs/2204 02311  Arman Cohan and Nazli Goharian  Contextualizing citations for scientific summarization using word embeddings and domain knowledge  In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval , pp  1133–1136, 2017  URL https://dl acm org/doi/abs/10  https://arxiv org/abs/2203 15556  Gautier Izacard and Edouard Grave  Distilling Knowledge from Reader to Retriever for Ques- tion Answering, 2022  URL https://arxiv org/abs/2012 04584   arXiv preprint arXiv:2012 04584  mlr press/v202/ kandpal23a/kandpal23a pdf  Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih  Dense Passage Retrieval for Open-Domain Question Answering  In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp  Hannaneh Hajishirzi  UNIFIEDQA: Crossing format boundaries with a single QA system InFindings of the Association for Computational Linguistics: EMNLP 2020 , pp  1896–1907, Online, November 2020  Association for Computational Linguistics  doi: 10 18653/v1/2020 findings-emnlp 171  URL https://aclanthology org/2020 findings-emnlp 171  Omar Khattab and Matei Zaharia  ColBERT: Efficient and effective passage search via con- textualized late interaction over bert  In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval , pp  39–48, 2020  URL https://arxiv org/abs/2004 12832   Tom´aˇs Ko ˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G ´abor Melis, and Edward Grefenstette  The NarrativeQA Reading Comprehension Challenge  Transactions of the Association for Computational Linguistics , 6:317–328, 2018  URL https://arxiv org/abs/1712 07040   passage ranking for diverse multi-answer retrieval  In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds ), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pp  6997–7008, Online and Punta Cana, Dominican Republic, November 2021  Association for Computational Linguistics  doi: 10 18653/v1/2021 emnlp-main 560  press/v162/ mitchell22a/mitchell22a pdf  Xiangyang Mou, Mo Yu, Bingsheng Yao, Chenghao Yang, Xiaoxiao Guo, Saloni Potdar, and Hui Su  Frustratingly hard evidence retrieval for QA over books  In Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events , pp  108–113, Online, July 2020 Association for Computational Linguistics  ear text segmentation  In Findings of the Association for Computational Linguistics: EACL 2023 , pp  883–893, Dubrovnik, Croatia, May 2023  Association for Computational Linguis- tics  doi: 10 18653/v1/2023 findings-eacl 65  URL https://aclanthology org/2023 findings-eacl 65   Benjamin Newman, Luca Soldaini, Raymond Fok, Arman Cohan, and Kyle Lo  A controllable qa- based framework for decontextualization  arXiv preprint arXiv:2305 14772 , 2023  URL https: //arxiv org/pdf/2305 14772 pdf  OpenAI  GPT-4 Technical Report  ArXiv , abs/2303 08774, 2023  URL https://arxiv  :<end_of_turn> <start_of_turn>model Sure, here's a summary of the text you provided:  **Title:** Recursively Summarizing Books  **Authors:** Wu et al. (2020)  **Key details:**  * The paper proposes a method for recursively summarizing books using a transformer-based approach. * The model is trained on a massive dataset of book summaries and context-free texts. * The summary generation process is designed to be self-supervised, requiring no human intervention. * The\n",
      "\n",
      "<bos><start_of_turn>user Write a summary of the following, including as many key details as possible: skiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, et al  CoLT5: Faster long-range transformers with conditional computation  arXiv preprint arXiv:2303 09752 , 2023  URL https://arxiv org/abs/2303 09752  Ekin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and  org/2023 findings-acl 679  Iz Beltagy, Matthew E  Peters, and Arman Cohan  Longformer: The Long-document Transformer, 2020  URL https://arxiv org/abs/2004 05150   arXiv preprint arXiv:2004 05150 Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Milli-  org/abs/2303 12712  Shuyang Cao and Lu Wang  HIBRIDS: Attention with hierarchical biases for structure-aware long document summarization  In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp  786–807, Dublin, Ireland, May 2022 Association for Computational Linguistics  doi: 10 18653/v1/2022 acl-long 58  1145/3077136 3080740  Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov Transformer-XL: Attentive language models beyond a fixed-length context  In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp  2978–2988, Florence, Italy, July 2019  Association for Computational Linguistics  European Conference on Information Retrieval, ECIR 2023, Dublin, Ireland, April 2–6, 2023, Proceedings, Part I , pp  264–278  Springer, 2023a  URL https://link springer com/ chapter/10 1007/978-3-031-28244-7_17  Zican Dong, Tianyi Tang, Lunyi Li, and Wayne Xin Zhao  A survey on long text modeling with transformers  14627  11 Published as a conference paper at ICLR 2024 Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang  LongT5: Efficient text-to-text transformer for long sequences  In Findings of the Association for Computational Linguistics: NAACL 2022 , pp  724–736, Seattle, United States, July 2022   Lost in the middle: How language models use long contexts  arXiv preprint arXiv:2307 03172 , 2023  URL https://arxiv org/abs/2307 03172  Ye Liu, Kazuma Hashimoto, Yingbo Zhou, Semih Yavuz, Caiming Xiong, and Philip Yu  Dense hierarchical retrieval for open-domain question answering  In Marie-Francine Moens, Xuanjing  :<end_of_turn> <start_of_turn>model Sure, here's a summary of the article:  **Title:** CoLT5: Faster long-range transformers with conditional computation  **Authors:** Ekin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and Arman Cohan  **Key Points:**  * The paper introduces CoLT5, a faster and more efficient implementation of Longformer. * CoLT5 uses a novel conditional computation mechanism to reduce\n",
      "\n",
      ":<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here's a summary of the article:\n",
      "\n",
      "The paper proposes CoLT5, a faster and more efficient implementation of the Longformer transformer. Longformer is a powerful transformer-based language model that can be used for various natural language processing (NLP) tasks. However, its long training times and high computational cost make it unsuitable for some applications.\n",
      "\n",
      "CoLT5 addresses these limitations by introducing a novel conditional computation mechanism that allows the model to process long contexts in an efficient manner. This\n",
      "\n",
      "Index: 122, Text: <bos><start_of_turn>user\n",
      "Write a summary of the following, including as many key details as possible: <bos><start_of_turn>user Write a summary of the following, including as many key details as possible: Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel  org/ abs/2303 08774  Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman  QuALITY: 13   :<end_of_turn> <start_of_turn>model The context does not provide any information about the quality of the individuals or the context of their lives, so I cannot generate the requested summary.\n",
      "\n",
      ":<end_of_turn>\n",
      "<start_of_turn>model\n",
      "I am unable to generate a summary of the context as I am unable to access external sources or provide any context about the individuals or their lives.\n",
      "\n",
      "Index: 123, Text: <bos><start_of_turn>user\n",
      "Write a summary of the following, including as many key details as possible: <bos><start_of_turn>user Write a summary of the following, including as many key details as possible: tiveQA, QASPER, and QuALITY  NarrativeQA is a dataset that comprises question-answer pairs based on the full texts of books and movie transcripts, totaling 1,572 documents (Ko ˇcisk`y et al , 2018; Wu et al , 2021)  The NarrativeQA-Story task requires a comprehensive understanding of the entire narrative in order to accurately answer its questions, thus testing the model’s ability to comprehend longer texts in  the literary domain  We measure performance on this dataset using the standard BLEU (B-1, B-4), ROUGE (R-L), and METEOR (M) metrics  Please see appendix H for more details on the Narra- tiveQA evaluation script used in our experiments The QASPER dataset includes 5,049 questions across 1,585 NLP papers, with each question probing for information embedded within the full text (Dasigi et al  , 2021)  The answer types in QASPER are categorized as Answerable/Unanswerable, Yes/No, Abstractive, and Extractive  Accuracy is measured using standard F1 Lastly, the QuALITY dataset consists of multiple-choice questions, each accompanied by context passages averaging approximately 5,000 tokens in length (Pang et al , 2022)  This dataset calls for  reasoning over the entire document for QA tasks, enabling us to measure the performance of our re- trieval system on medium-length documents  The dataset includes a challenging subset, QuALITY- HARD, which contains questions that a majority of human annotators answered incorrectly in a speed-setting  We report accuracies for both the entire test set and the HARD subset Controlled Baseline Comparisons We first present controlled comparisons using the UnifiedQA  QuALITY-HARD, which represents questions that humans took unusually long to correctly answer, requiring rereading parts of the text, difficult reasoning, or both For the NarrativeQA dataset, as represented in Table 6, RAPTOR paired with UnifiedQA sets a new state-of-the-art METEOR score  When compared to the recursively summarizing model by Wu et al  (2021), which also employs UnifiedQA, RAPTOR outperforms it on all metrics  While  pothesized that upper nodes play a crucial role in handling thematic or multi-hop queries requiring a broader understanding of the text 8 Published as a conference paper at ICLR 2024 Table 6: Performance comparison on the NarrativeQA dataset across multiple models, focusing on four metrics: ROUGE-L, BLEU-1, BLEU-4, and METEOR  RAPTOR, when paired with Uni-  8 23 5 6 4 19 1 Table 7: Accuracies of the QuALITY dataset on both the overall test set and the more challenging hard subset  GPT-4 with RAPTOR sets a new state-of-the-art ModelAccuracy Test Set Hard Subset Longformer-base (Beltagy et al , 2020) 39 5 35  3 DPR and DeBERTaV3-large (Pang et al  experiments: GPT-3 and GPT-4 for QA tasks, and GPT-3 5-turbo for summarization  The gpt-3 , gpt-4 , and gpt-3 5-turbo models can be accessed via API calls (OpenAI API)  UnifiedQA, which is used for QA tasks, is publicly available at Hugging Face Evaluation Datasets The three evaluation datasets used in our experiments—QuALITY,  QASPER, and NarrativeQA—are all publicly accessible  These datasets ensure that the retrieval and QA tests conducted in this study can be replicated Source Code The source code for RAPTOR will be publicly available here REFERENCES Charu C Aggarwal, Alexander Hinneburg, and Daniel A Keim  On the Surprising Behavior of Dis- tance Metrics in High Dimensional Space  In Database Theory—ICDT 2001: 8th International  :<end_of_turn> <start_of_turn>model Sure, here's a summary of the provided information:  **NarrativeQA Dataset**  - A dataset of question-answer pairs based on the full texts of books and movie transcripts. - 1,572 documents, with each document containing questions and answers. - Measures performance on this dataset using BLEU, ROUGE, and METEOR metrics.  **QASPER Dataset**  - A dataset of questions across 1,585 NLP papers. -\n",
      "\n",
      "<bos><start_of_turn>user Write a summary of the following, including as many key details as possible: kens of context, as UnifiedQA has a max con- text length of 512 tokens  We provide the same amount of tokens of context to RAPTOR and to the baselines Qualitative Study We conduct a qualitative analysis to understand the benefits of RAP- TOR’s retrieval process compared to Dense Passage Retrieval (DPR) methods  Our study focuses on thematic, multi-hop questions using a 1500-word Cinderella fairytale  As illustrated  directly or within higher-layer summaries our results demonstrate that RAPTOR, when combined with any retriever, consistently outperforms the respective retriever across all datasets 2 Since RAPTOR with SBERT has the best performance, we use it in all subsequent experiments We now compare RAPTOR with BM25 and DPR, using three different LLMs: GPT-3, GPT-4, and UnifiedQA  5 points, and outdo BM25 by 6 5, 5 5, and 10 2 points across the respective LLMs  QASPER requires synthesizing information within NLP papers, so it is unsurprising that RAPTOR’s higher-level summary nodes would allow it to outperform methods that can only extract the top- kmost similar raw chunks of text, which may not contain the correct response in isolation  Table 1: NarrativeQA Performance With + Without RAPTOR: Performance comparison of various retrieval methods (SBERT, BM25, DPR) with and without RAPTOR on the NarrativeQA dataset, using UnifiedQA-3B as the language model  RAPTOR outperforms baselines of each re- spective retrieval method Model ROUGE BLEU-1 BLEU-4 METEOR SBERT with RAPTOR 30 87% 23 50% 6   This decision was based on the performance observed in Karpukhin et al  (2020), where dpr-multiset-base showed superior results 7 Published as a conference paper at ICLR 2024 Table 2: QuALITY and QASPER Performance With + Without RAPTOR: Performance com- parison across the QuALITY and QASPER datasets of various retrieval methods (SBERT, BM25, DPR) with and without RAPTOR   UnifiedQA-3B is used as the language model  RAPTOR outper- forms baselines of each respective retrieval method for both datasets Model Accuracy (QuALITY) Answer F1 (QASPER) SBERT with RAPTOR 56 6% 36 70% SBERT without RAPTOR 54 9% 36 23% BM25 with RAPTOR 52 1% 27 00%  Title + Abstract 25 2 22 2 17 5 BM25 46 6 50 2 26 4 DPR 51 3 53 0 32 1 RAPTOR 53 1 55 7 36 6 Table 4: Comparison of accuracies on the QuAL- ITY dev dataset for two different language mod- els (GPT-3, UnifiedQA 3B) using various retrieval methods  :<end_of_turn> <start_of_turn>model Sure, here is the summary you requested:  The study compares the performance of RAPTOR, a multi-hop language model, against other retrieval methods on a Cinderella fairytale dataset.  **Key Details:**  * RAPTOR outperforms baselines of each respective retrieval method across all datasets. * When combined with SBERT, RAPTOR consistently outperforms BM25 and DPR by 6, 5, 5, and 10 points, respectively. * QASPER requires\n",
      "\n",
      "<bos><start_of_turn>user Write a summary of the following, including as many key details as possible: 42% 19 20% SBERT without RAPTOR 29 26% 22 56% 5 95% 18 15% BM25 with RAPTOR 27 93% 21 17% 5 70% 17 03% BM25 without RAPTOR 23 52% 17 73% 4 65% 13 98% DPR with RAPTOR 30 94% 23  51% 6 45% 19 05% DPR without RAPTOR 29 56% 22 84% 6 12% 18 44% Likewise, in the QuALITY dataset as shown in Table 4, RAPTOR achieves an accuracy of 62 4%, which is a 2% and 5 1% improvement over DPR and BM25  Similar trends are observed when Uni-  metrics like BLEU-1, BLEU-4, and METEOR, RAPTOR outperforms BM25 and DPR by margins ranging from 1 7 to 5 8 and 0 7 to 2 1 points, respectively 2For the DPR experiments in Tables 1 and 2, we used the dpr-multiset-base model as opposed to dpr-single-nq-base which was used in rest of the experiments done earlier  BM25 without RAPTOR 49 9% 26 47% DPR with RAPTOR 54 7% 32 23% DPR without RAPTOR 53 1% 31 70% Table 3: Controlled comparison of F-1 scores on the QASPER dataset, using three different lan- guage models (GPT-3, GPT-4, UnifiedQA 3B) and various retrieval methods  The column ”Title +  :<end_of_turn> <start_of_turn>model Sure, here is the summary you requested:  The provided context provides a comparison between different language models, including RAPTOR, BM25, and DPR.  **RAPTOR:** - Achieves high accuracy (95%) on the QASPER dataset. - Outperforms other models (DPR and BM25) by a significant margin. - The results suggest that RAPTOR is effective in improving the quality of generated text.  **BM25:** - Achie\n",
      "\n",
      ":<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here's the summary you requested:\n",
      "\n",
      "The provided context provides a comparison between different language models, including RAPTOR, BM25, and DPR on the NarrativeQA dataset. \n",
      "\n",
      "**Key Details:**\n",
      "\n",
      "* The NarrativeQA dataset consists of question-answer pairs based on the full texts of books and movie transcripts, totaling 1,572 documents.\n",
      "* RAPTOR outperforms baselines of each respective retrieval method on all datasets, achieving high accuracy and outperforming BM\n",
      "\n",
      "Index: 124, Text: <bos><start_of_turn>user\n",
      "Write a summary of the following, including as many key details as possible: <bos><start_of_turn>user Write a summary of the following, including as many key details as possible: Stefanos Angelidis and Mirella Lapata  Summarizing opinions: Aspect extraction meets sentiment prediction and they are both weakly supervised  arXiv preprint arXiv:1808 08858 , 2018  URL https://arxiv org/abs/1808 08858  Manoj Ghuhan Arivazhagan, Lan Liu, Peng Qi, Xinchi Chen, William Yang Wang, and Zhiheng Huang   Hybrid hierarchical retrieval for open-domain question answering  In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds ), Findings of the Association for Computational Linguistics: ACL 2023 , pp  10680–10689, Toronto, Canada, July 2023  Association for Computa- tional Linguistics  doi: 10 18653/v1/2023 findings-acl 679  URL https://aclanthology   URL https: //aclanthology org/2022 acl-long 58  Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes  Reading Wikipedia to Answer Open-Domain Questions  In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp  1870–1879, Vancouver, Canada, July 2017  Association for Computational Linguistics  doi: 10  ciation for Computational Linguistics  doi: 10 18653/v1/2021 naacl-main 365  URL https: //aclanthology org/2021 naacl-main 365  Mengxing Dong, Bowei Zou, Yanling Li, and Yu Hong  CoLISA: Inner Interaction via Contrastive Learning for Multi-choice Reading Comprehension  In Advances in Information Retrieval: 45th   Association for Computational Linguistics  doi: 10 18653/v1/2022 findings-naacl 55 URLhttps://aclanthology org/2022 findings-naacl 55  Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang  Retrieval Augmented Language Model Pre-Training  In International conference on machine learning , pp  3929–3938 PMLR, 2020  Huang, Lucia Specia, and Scott Wen-tau Yih (eds ), Findings of the Association for Compu- tational Linguistics: EMNLP 2021 , pp  188–200, Punta Cana, Dominican Republic, Novem- ber 2021  Association for Computational Linguistics  doi: 10 18653/v1/2021 findings-emnlp 19 URL https://aclanthology org/2021 findings-emnlp   URL https://aclanthology org/2021 emnlp-main 560  Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer  Nonparametric masked language modeling  In Findings of the Association for Computational Linguistics: ACL 2023 , pp  2097–2118, Toronto, Canada, July 2023  Associ-  :<end_of_turn> <start_of_turn>model Sure, here's a summary of the paper:  The paper presents a novel hybrid hierarchical retrieval method for open-domain question answering. This method combines the strengths of aspect extraction and sentiment prediction to improve the quality of retrieved passages.  The paper introduces a new dataset called \"Hybrid Retrieval\" that consists of pairs of question-answer pairs. The first question is used to query a large text corpus, and the corresponding answers are retrieved from a smaller subset of documents that are considered to be relevant to\n",
      "\n",
      "<bos><start_of_turn>user Write a summary of the following, including as many key details as possible: Published as a conference paper at ICLR 2024 Given a set of Ntext segments, each represented as a d-dimensional dense vector embedding, the likelihood of a text vector, x, given its membership in the kthGaussian distribution, is denoted by P(x|k) =N(x;µk,Σk)  The overall probability distribution is a weighted combination P(x) =PK  k=1πkN(x;µk,Σk), where πksignifies the mixture weight for the kthGaussian distribution The high dimensionality of vector embeddings presents a challenge for traditional GMMs, as dis- tance metrics may behave poorly when used to measure similarity in high-dimensional spaces (Ag- garwal et al , 2001)  To mitigate this, we employ Uniform Manifold Approximation and Projection  among the text data, from broad themes to specific details Should a local cluster’s combined context ever exceed the summarization model’s token threshold, our algorithm recursively applies clustering within the cluster, ensuring that the context remains within the token threshold To determine the optimal number of clusters, we employ the Bayesian Information Criterion (BIC) for model selection  BIC not only penalizes model complexity but also rewards goodness of fit (Schwarz, 1978)   The BIC for a given GMM is BIC = ln( N)k−2 ln(ˆL), where Nis the number of text segments (or data points), kis the number of model parameters, and ˆLis the maximized value of the likelihood function of the model  In the context of GMM, the number of parameters k is a function of the dimensionality of the input vectors and the number of clusters  With the optimal number of clusters determined by BIC, the Expectation-Maximization algorithm is then used to estimate the GMM parameters, namely the means, covariances, and mixture weights While the Gaussian assumption in GMMs may not perfectly align with the nature of text data, which often exhibits a sparse and skewed distribution, our empirical observations suggest that it offers an effective model for our purpose  We run an ablation comparing GMM Clustering with summarizing  contiguous chunks and provide details in Appendix B Model-Based Summarization After clustering the nodes using Gaussian Mixture Models, the nodes in each cluster are sent to a language model for summarization  This step allows the model to transform large chunks of text into concise, coherent summaries of the selected nodes  For our experiments, we use gpt-3 5-turbo to generate the summaries  The summarization step con- denses the potentially large volume of retrieved information into a manageable size   We provide statistics on the compression due to the summarization in Appendix C and the prompt used for summarization in Appendix D While the summarization model generally produces reliable summaries, a focused annotation study revealed that about 4% of the summaries contained minor hallucinations  These did not propagate to parent nodes and had no discernible impact on question-answering tasks  For an in-depth analysis of hallucinations, refer to the appendix E  :<end_of_turn> <start_of_turn>model Sure, here's a summary of the paper:  **Title:** Exploring Text Data with Uniform Manifold Approximation and Projection  **Abstract:** The paper proposes a novel approach for text data analysis that leverages the Uniform Manifold Approximation and Projection (UMAAP) framework to address the challenges posed by high-dimensional vector embeddings in GMMs.  **Key Concepts:** * **Uniform Manifold Approximation and Projection (UMAAP):** A framework that projects high-dimensional vectors onto a\n",
      "\n",
      "<bos><start_of_turn>user Write a summary of the following, including as many key details as possible: (UMAP), a manifold learning technique for dimensionality reduction (McInnes et al , 2018)  The number of nearest neighbors parameter, nneighbors , in UMAP determines the balance between the preservation of local and global structures  Our algorithm varies nneighbors to create a hierar- chical clustering structure: it first identifies global clusters and then performs local clustering within these global clusters  This two-step clustering process captures a broad spectrum of relationships  Conference London, UK, January 4–6, 2001 Proceedings 8 , pp  420–434  Springer, 2001  URL https://link springer com/chapter/10 1007/3-540-44503-x_27  Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Onta ˜n´on, Siddhartha Brahma, Yury Zemlyan-   Lin (eds ), Advances in Neu- ral Information Processing Systems , volume 33, pp  1877–1901  Curran Associates, Inc , 10 Published as a conference paper at ICLR 2024 2020  URL https://proceedings neurips cc/paper_files/paper/2020/ file/1457c0d6bfcb4967418bfb8ac142f64a-Paper pdf    6769–6781, Online, November 2020  Association for Computational Linguis- tics  doi: 10 18653/v1/2020 emnlp-main 550  URL https://aclanthology org/2020 emnlp-main 550  Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and  33:9459–9474, 2020  URL https://doi org/10 48550/arXiv 2005 11401  Jerry Liu  LlamaIndex, 2022  URL https://github com/jerryjliu/llama_index  Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang  19  Leland McInnes, John Healy, and James Melville  UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction, 2018  URL https://arxiv org/abs/1802 03426   arXiv preprint arXiv:1802 03426 Sewon Min, Kenton Lee, Ming-Wei Chang, Kristina Toutanova, and Hannaneh Hajishirzi  Joint   doi: 10 18653/v1/2020 nuse-1 13  URL https: //aclanthology org/2020 nuse-1 13  Inderjeet Nair, Aparna Garimella, Balaji Vasan Srinivasan, Natwar Modani, Niyati Chhaya, Srikr- ishna Karanam, and Sumit Shekhar  A neural CRF-based hierarchical approach for lin-  :<end_of_turn> <start_of_turn>model Sure, here's a summary of the provided text:  **UMAP (Uniform Manifold Approximation and Projection for Dimension Reduction)** is a manifold learning technique for dimensionality reduction. It uses a hierarchical clustering approach to capture a broad spectrum of relationships between data points.  **Key features of UMAP:**  - **Number of nearest neighbors (nneighbors):** This parameter determines the balance between the preservation of local and global structures in the data. - **Hierarchical clustering:** It first identifies global\n",
      "\n",
      ":<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here's a summary of the paper:\n",
      "\n",
      "**Title:** Exploring Text Data with Uniform Manifold Approximation and Projection\n",
      "\n",
      "**Abstract:** This paper proposes a novel approach for text data analysis that leverages the Uniform Manifold Approximation and Projection (UMAP) framework to address the challenges posed by high-dimensional vector embeddings in Gaussian mixture models (GMMs).\n",
      "\n",
      "**Key Concepts:**\n",
      "\n",
      "- **Uniform Manifold Approximation and Projection (UMAP):** A framework that projects high-dimensional\n",
      "\n",
      "Index: 125, Text: <bos><start_of_turn>user\n",
      "Write a summary of the following, including as many key details as possible: <bos><start_of_turn>user Write a summary of the following, including as many key details as possible:  The children of these selected nodes are considered at the next layer and the top-k nodes are selected from this pool again based on their cosine similarity to the query vector  This process is repeated until we reach the leaf nodes  Finally, the text from all selected nodes is concatenated to form the retrieved context  The algorithm’s steps are outlined below: 1  Start at the root layer of the RAPTOR tree  Compute the cosine similarity between the  query embedding and the embeddings of all nodes present at this initial layer 2  Choose the top- knodes based on the highest cosine similarity scores, forming the set S1 4 Published as a conference paper at ICLR 2024 Figure 2: Illustration of the tree traversal and collapsed tree retrieval mechanisms  Tree traver- sal starts at the root level of the tree and retrieves the top- k(here, top- 1) node(s) based on cosine  similarity to the query vector  At each level, it retrieves the top- knode(s) from the child nodes of the previous layer’s top- k  Collapsed tree collapses the tree into a single layer and retrieves nodes until a threshold number of tokens is reached, based on cosine similarity to the query vector  The nodes on which cosine similarity search is performed are highlighted in both illustrations 3  Proceed to the child nodes of the elements in set S1  Compute the cosine similarity between  the query vector and the vector embeddings of these child nodes 4  Select the top kchild nodes with the highest cosine similarity scores to the query, forming the set S2 5  Continue this process recursively for dlayers, producing sets S1, S2,   , S d 6  Concatenate sets S1through Sdto assemble the relevant context to the query By adjusting the depth dand the number of nodes kselected at each layer, the tree traversal method  offers control over the specificity and breadth of the information retrieved  The algorithm starts with a broad outlook by considering the top layers of the tree and progressively focuses on finer details as it descends through the lower layers Thecollapsed tree approach offers a simpler way to search for relevant information by considering all nodes in the tree simultaneously, as depicted in Figure 2  Instead of going layer-by-layer, this method flattens the multi-layered tree into a single layer, essentially bringing all the nodes onto the  same level for comparison  The steps for this method are outlined below: 1  First, collapse the entire RAPTOR tree into a single layer  This new set of nodes, denoted asC, contains nodes from every layer of the original tree 2  Next, calculate the cosine similarity between the query embedding and the embeddings of all nodes present in the collapsed set C 3  Finally, pick the top- knodes that have the highest cosine similarity scores with the query  Keep adding nodes to the result set until you reach a predefined maximum number of tokens, ensuring you don’t exceed the model’s input limitations We tested both approaches on 20 stories from the QASPER dataset  Figure 3 shows the performance of tree traversal with different top- sizes and collapsed tree with different maximum token numbers The collapsed tree approach consistently performs better  We believe collapsed tree retrieval is better due to offering greater flexibility than tree traversal; i e , by searching through all the nodes  simultaneously, it retrieves information that is at the correct level of granularity for a given question In comparison, while using tree traversal with the same values of dandk, the ratio of nodes from each level of the tree will be constant  So, the ratio of higher-order thematic information to granular details will remain the same regardless of the question 5 Published as a conference paper at ICLR 2024 One drawback, however, of the collapsed tree approach is that it requires cosine similarity search to  be performed on all nodes in the tree  However, this can be made more efficient with fast k-nearest neighbor libraries such as FAISS (Johnson et al , 2019) Figure 3: Comparison of querying methods Results on 20 stories from the QASPER dataset using tree traversal with different top-k values, and collapsed tree with different context lengths Collapsed tree with 2000 tokens produces the best results, so we use this querying strategy for our main results  Overall, given the collapsed tree approach’s greater flexibility and its superior performance on the subset of the QASPER dataset, this is the querying approach with which we proceed Specifically, we use the collapsed tree with 2000 maximum tokens, which approximately equates to retrieving the top-20 nodes  Using a token-based approach ensures the context does not exceed model context constraints as token counts can vary across nodes  For experiments with the UnifiedQA model, we provide 400 to-   Columns represent different starting points (highest layer) and rows represent different numbers of layers queried Layers Queried / Start Layer Layer 0 (Leaf Nodes) Layer 1 Layer 2 1 layer 57 9 57 8 57 9 2 layers - 52 6 63 15 3 layers - - 73 68 We validated this hypothesis both quantitatively and qualitatively  We present qualitative analysis in appendix G  :<end_of_turn> <start_of_turn>model Sure, here's a summary of the text you provided:  **Raptor Tree Retrieval Algorithm:**  1. Start with the root layer of the RAPTOR tree. 2. Compute the cosine similarity between the query embedding and the embeddings of all nodes in the tree. 3. Choose the top-k nodes with the highest cosine similarity scores to form the set S1. 4. Repeat step 2 for the child nodes of the elements in set S1. 5.\n",
      "\n",
      "<bos><start_of_turn>user Write a summary of the following, including as many key details as possible: Querying In this section, we elaborate on the two querying mechanisms employed by RAPTOR: tree traversal and collapsed tree  These methods offer unique ways of traversing the multi-layered RAPTOR tree to retrieve relevant information, each with its own advantages and trade-offs  We provide the pseudocode of both methods in Appendix F  Note that we embed all nodes using SBERT The tree traversal method first selects the top-k most relevant root nodes based on their cosine similarity to the query embedding  in Figure 4, RAPTOR’s tree-based retrieval allows it to choose nodes from different tree layers, matching the question’s detail level  This approach often yields more relevant and comprehensive information for downstream tasks than DPR  For a detailed discussion and examples, including the text retrieved by both RAPTOR and DPR for specific questions, please refer to the appendix G 4 E XPERIMENTS Datasets We measure RAPTOR’s performance across three question-answering datasets: Narra-  3B as the reader, with SBERT (Reimers & Gurevych, 2019), BM25 (Robertson et al , 1995; 2009), and DPR (Karpukhin et al , 2020) as the embedding models with and without the RAPTOR tree structure, on three datasets: QASPER, NarrativeQA, and QuALITY   As shown in Tables 1 and 2,  6 Published as a conference paper at ICLR 2024 Figure 4: Querying Process: Illustration of how RAPTOR retrieves information for two questions about the Cinderella story: “What is the central theme of the story ” and “How did Cinderella find a happy ending ”  Highlighted nodes indicate RAPTOR’s selections, while arrows point to DPR’s leaf nodes  Notably, RAPTOR’s context often encompasses the information retrieved by DPR, either  Wu et al  (2021) rely solely on the summary in the top root node of the tree structure, RAPTOR benefits from its intermediate layers and clustering approaches, which allows it to capture a range of information, from general themes to specific details, contributing to its overall strong performance 4 1 C ONTRIBUTION OF THE TREE STRUCTURE We examine the contribution of each layer of nodes to RAPTOR’s retrieval capabilities  We hy-  , 2022) 55 4 46  1 CoLISA (DeBERTaV3-large) (Dong et al , 2023a) 62 3 54  7 RAPTOR + GPT-4 82 6 76 2 Table 8: Performance of RAPTOR when querying different tree layers for Story 1 from the QuAL- ITY dataset   To quantitatively understand the contribution of the upper-level nodes, we used stories from the QuALITY dataset  The RAPTOR tree is built for each of these stories, as described in Section 3  However, during retrieval, we limit the search to different subsets of layers  For example, we exclusively retrieve from the leaf nodes and each upper layer, as well as from different contiguous subsets of the layers   We show findings specific to one story in Table 8, revealing that a full-tree search, utilizing all layers, outperformed retrieval strategies that focused only on specific layers These findings highlight the importance of the full tree structure in RAPTOR  By providing both the original text and higher-level summaries for retrieval, RAPTOR can effectively handle a wider range of questions, from higher-order thematic queries to detail-oriented questions  Detailed results  for additional stories and an ablation study on layer contributions can be found in Appendix I 5 C ONCLUSION In this paper, we have presented RAPTOR, a novel tree-based retrieval system that augments the parametric knowledge of large language models with contextual information at various levels of abstraction  By employing recursive clustering and summarization techniques, RAPTOR creates a hierarchical tree structure that is capable of synthesizing information across various sections of the retrieval corpora   During the query phase, RAPTOR leverages this tree structure for more effective retrieval  Our controlled experiments demonstrated that RAPTOR not only outperforms traditional retrieval methods but also sets new performance benchmarks on several question-answering tasks 9 Published as a conference paper at ICLR 2024 6 R EPRODUCIBILITY STATEMENT Language Models for QA and Summarization Four language models are used in our RAPTOR  :<end_of_turn> <start_of_turn>model **Summary:**  **Querying Mechanisms:** - Tree traversal: Selects top-k most relevant root nodes based on cosine similarity to query embedding. - Collapsed tree: Uses a hierarchical structure to capture information from different tree layers.  **Tree traversal method:** - Focuses on top-k most relevant root nodes. - Provides more comprehensive information due to intermediate layers' clustering.  **Collapsed tree method:** - Uses a hierarchical structure to capture information from different tree layers\n",
      "\n",
      ":<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here's a summary of the text you provided, including as many key details as possible:\n",
      "\n",
      "**Key Details:**\n",
      "\n",
      "* The RAPTOR tree is a multi-layered retrieval system that combines parametric and contextual information.\n",
      "* It employs recursive clustering and summarization techniques to generate a hierarchical tree structure.\n",
      "* The tree structure allows for efficient retrieval of relevant information from various levels of abstraction.\n",
      "* Tree traversal selects top-k most relevant root nodes based on cosine similarity to the query\n",
      "\n",
      "============================================level 1============================================\n",
      "Index: 110, Text: <bos><start_of_turn>user\n",
      "Write a summary of the following, including as many key details as possible:  As shown in Table 3, RAPTOR consistently outperforms BM25 and DPR across all three Language Models on the QASPER dataset  RAPTOR’s F-1 Match scores are 53 1%, 55 7%, and 36 6% when using GPT-3, GPT-4, and UnifiedQA, respectively  These scores surpass DPR by margins of 1 8, 2 7, and 4\n",
      "\n",
      "Abstract” reflects performance when only the title and abstract of the papers are used for context RAPTOR outperforms the established baselines BM25 and DPR across all tested language models Specifically, RAPTOR’s F-1 scores are at least 1 8% points higher than DPR and at least 5 3% points higher than BM25 Retriever GPT-3 F-1 Match GPT-4 F-1 Match UnifiedQA F-1 Match\n",
      "\n",
      " RAPTOR outperforms the baselines of BM25 and DPR by at least 2 0% in accuracy Model GPT-3 Acc  UnifiedQA Acc BM25 57 3 49 9 DPR 60 4 53 9 RAPTOR 62 4 56 6 Table 5: Results on F-1 Match scores of various models on the QASPER dataset Model F-1 Match LongT5 XL (Guo et al\n",
      "\n",
      ", 2022) 53 1 CoLT5 XL (Ainslie et al , 2023) 53 9 RAPTOR + GPT-4 55 7Comparison to State-of-the-art Systems Building upon our controlled comparisons, we examine RAPTOR’s performance relative to other state-of-the-art models  As shown in Table 5, RAPTOR with GPT-4 sets a new benchmark on QASPER, with a 55\n",
      "\n",
      "7% F-1 score, surpassing the CoLT5 XL’s score of 53 9% In the QuALITY dataset, as shown in Table 7, RAPTOR paired with GPT-4 sets a new state- of-the-art with an accuracy of 82 6%, surpass- ing the previous best result of 62 3%  In par- ticular, it outperforms CoLISA by 21 5% on\n",
      "\n",
      ":<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here is a summary of the passage:\n",
      "\n",
      "* RAPTOR consistently outperforms BM25 and DPR across all tested language models on the QASPER dataset.\n",
      "* RAPTOR’s F-1 Match scores are at least 1 8% points higher than DPR and at least 5 3% points higher than BM25 Retriever GPT-3 F-1 Match GPT-4 F-1 Match UnifiedQA F-1 Match.\n",
      "* RAPTOR outperforms the\n",
      "\n",
      "Index: 111, Text: <bos><start_of_turn>user\n",
      "Write a summary of the following, including as many key details as possible: fiedQA is employed, with RAPTOR outperforming DPR and BM25 by 2 7% and 6 7%, respectively Finally, in the NarrativeQA dataset, as presented in Table 6, RAPTOR excels across multiple met- rics  For ROUGE-L, it surpasses BM25 and DPR by 7 3 and 2 7 points, respectively  In other\n",
      "\n",
      "fiedQA 3B, not only surpasses retrieval methods like BM25 and DPR but also sets a new state-of- the-art in the METEOR metric Model ROUGE-L BLEU-1 BLEU-4 METEOR BiDAF (Ko ˇcisk`y et al , 2018) 6 2 5  7 0  3 3  7 BM25 + BERT (Mou et al\n",
      "\n",
      ":<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here's a summary of the passage:\n",
      "\n",
      "* fiedQA outperforms other retrieval methods, including DPR and BM25, by a significant margin in the NarrativeQA dataset.\n",
      "* RAPTOR outperforms DPR and BM25 by 2 7% and 6 7%, respectively.\n",
      "* In the ROUGE-L metric, RAPTOR surpasses BM25 and DPR by 7 3 and 2 7 points, respectively.\n",
      "* In the\n",
      "\n",
      "Index: 106, Text: <bos><start_of_turn>user\n",
      "Write a summary of the following, including as many key details as possible: Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad- ford, Ilya Sutskever, and Dario Amodei  Language Models are Few-Shot Learners  In H  Larochelle, M  Ranzato, R  Hadsell, M F  Balcan, and H\n",
      "\n",
      "Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave  Few-shot learning with re- trieval augmented language models  arXiv preprint arXiv:2208 03299 , 2022  URL https: //arxiv org/abs/2208 03299 \n",
      "\n",
      ":<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here is a summary of the paper:\n",
      "\n",
      "The paper explores the Few-Shot Learning (FSL) task for language models, focusing on the re-trieval augmented language model approach. The study investigates the effectiveness of different augmentation strategies and tri-evaluation methods on this task.\n",
      "\n",
      "**Key details:**\n",
      "\n",
      "* **Language models:**\n",
      "    * Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess\n",
      "\n",
      "Index: 114, Text: <bos><start_of_turn>user\n",
      "Write a summary of the following, including as many key details as possible: Kelvin Guu  Towards tracing knowledge in language models back to the training data  In Findings of the Association for Computational Linguistics: EMNLP 2022 , pp  2429–2446, Abu Dhabi, United Arab Emirates, December 2022  Association for Computational Linguistics doi: 10 18653/v1/2022 findings-emnlp 180  URL https://aclanthology org/2022 findings-emnlp 180 \n",
      "\n",
      "can, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al Improving language models by retrieving from trillions of tokens  In International conference on machine learning , pp  2206–2240  PMLR, 2022  URL https://arxiv org/abs/2112 04426 \n",
      "\n",
      "18653/v1/P17-1171  URL https: //aclanthology org/P17-1171  Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al  PaLM: Scaling Language Modeling with Pathways  arXiv preprint arXiv:2204\n",
      "\n",
      " arXiv preprint arXiv:2302 14502 , 2023b  URL https://arxiv org/abs/ 2302 14502  Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen  Enabling large language models to generate text with citations  arXiv preprint arXiv:2305 14627 , 2023  URL https://arxiv org/ abs/2305\n",
      "\n",
      " URL https://doi org/10 48550/arXiv 2002 08909  Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al Training compute-optimal large language models  arXiv preprint arXiv:2203 15556 , 2022  URL\n",
      "\n",
      "Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig  How can we know what language models know  Transactions of the Association for Computational Linguistics , 8:423–438, 2020 URL https://arxiv org/abs/1911 12543  Jeff Johnson, Matthijs Douze, and Herv ´e J´egou  Billion-Scale Similarity Search with GPUs  IEEE\n",
      "\n",
      "Transactions on Big Data , 7(3):535–547, 2019  URL https://arxiv org/abs/1702 08734  Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel  Large Language Models struggle to learn Long-Tail Knowledge  In International Conference on Machine Learn- ing, pp  15696–15707  PMLR, 2023  URL https://proceedings\n",
      "\n",
      "ation for Computational Linguistics  doi: 10 18653/v1/2023 findings-acl 132  URL https: //aclanthology org/2023 findings-acl 132  Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn Memory-based model editing at scale  In International Conference on Machine Learning , pp  15817–15831  PMLR, 2022  URL https://proceedings mlr\n",
      "\n",
      ":<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here is a summary of the paper:\n",
      "\n",
      "**Title:** Improving Language Models by Retrieving from Trillions of Tokens\n",
      "\n",
      "**Authors:** Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann\n",
      "\n",
      "**Conference:** International Conference on Machine Learning (ICML)\n",
      "\n",
      "**Abstract:**\n",
      "The paper proposes a novel approach for improving language\n",
      "\n",
      "Index: 105, Text: <bos><start_of_turn>user\n",
      "Write a summary of the following, including as many key details as possible: S´ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka- mar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al  Sparks of Artificial General Intelligence: Early Experiments with GPT-4  arXiv preprint arXiv:2303 12712 , 2023  URL https://arxiv\n",
      "\n",
      " doi: 10 18653/v1/P19-1285  URL https://aclanthology org/P19-1285  Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R ´e  FlashAttention: Fast and memory-efficient exact attention with IO-Awareness  Advances in Neural Information Processing Systems , 35:16344–16359, 2022  URL https://arxiv org/abs/2205\n",
      "\n",
      "14135  Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A  Smith, and Matt Gardner  A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers  In Proceed- ings of the 2021 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies , pp  4599–4610, Online, June 2021  Asso-\n",
      "\n",
      "12 Published as a conference paper at ICLR 2024 Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt ¨aschel, et al  Retrieval-Augmented Gener- ation for Knowledge-Intensive NLP Tasks  Advances in Neural Information Processing Systems ,\n",
      "\n",
      ":<end_of_turn>\n",
      "<start_of_turn>model\n",
      "**Summary:**\n",
      "\n",
      "The paper presents early experiments with the GPT-4 language model, exploring its potential for knowledge-intensive NLP tasks. The researchers focus on the \"Retrieval-Augmented Gener-a-tion for Knowledge-Intensive NLP Tasks\" approach, which combines the strengths of retrieval-based models and large language models.\n",
      "\n",
      "**Key Details:**\n",
      "\n",
      "* GPT-4 is a large language model (LLM) with a massive dataset and advanced capabilities.\n",
      "* The paper explores the ability\n",
      "\n",
      "Index: 115, Text: <bos><start_of_turn>user\n",
      "Write a summary of the following, including as many key details as possible: , 2020) 15 5 14  5 1  4 5  0 Recursively Summarizing Books (Wu et al , 2021) 21 6 22  3 4  2 10  6 Retriever + Reader (Izacard & Grave, 2022) 32 0 35 3 7 5 11 1 RAPTOR + UnifiedQA 30\n",
      "\n",
      "02311 , 2022  URL https://arxiv org/abs/2204 02311  Arman Cohan and Nazli Goharian  Contextualizing citations for scientific summarization using word embeddings and domain knowledge  In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval , pp  1133–1136, 2017  URL https://dl acm org/doi/abs/10\n",
      "\n",
      "https://arxiv org/abs/2203 15556  Gautier Izacard and Edouard Grave  Distilling Knowledge from Reader to Retriever for Ques- tion Answering, 2022  URL https://arxiv org/abs/2012 04584   arXiv preprint arXiv:2012 04584\n",
      "\n",
      "mlr press/v202/ kandpal23a/kandpal23a pdf  Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih  Dense Passage Retrieval for Open-Domain Question Answering  In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp\n",
      "\n",
      "Hannaneh Hajishirzi  UNIFIEDQA: Crossing format boundaries with a single QA system InFindings of the Association for Computational Linguistics: EMNLP 2020 , pp  1896–1907, Online, November 2020  Association for Computational Linguistics  doi: 10 18653/v1/2020 findings-emnlp 171  URL https://aclanthology org/2020 findings-emnlp 171\n",
      "\n",
      "Omar Khattab and Matei Zaharia  ColBERT: Efficient and effective passage search via con- textualized late interaction over bert  In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval , pp  39–48, 2020  URL https://arxiv org/abs/2004 12832 \n",
      "\n",
      "Tom´aˇs Ko ˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G ´abor Melis, and Edward Grefenstette  The NarrativeQA Reading Comprehension Challenge  Transactions of the Association for Computational Linguistics , 6:317–328, 2018  URL https://arxiv org/abs/1712 07040 \n",
      "\n",
      "passage ranking for diverse multi-answer retrieval  In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds ), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pp  6997–7008, Online and Punta Cana, Dominican Republic, November 2021  Association for Computational Linguistics  doi: 10 18653/v1/2021 emnlp-main 560\n",
      "\n",
      "press/v162/ mitchell22a/mitchell22a pdf  Xiangyang Mou, Mo Yu, Bingsheng Yao, Chenghao Yang, Xiaoxiao Guo, Saloni Potdar, and Hui Su  Frustratingly hard evidence retrieval for QA over books  In Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events , pp  108–113, Online, July 2020 Association for Computational Linguistics\n",
      "\n",
      "ear text segmentation  In Findings of the Association for Computational Linguistics: EACL 2023 , pp  883–893, Dubrovnik, Croatia, May 2023  Association for Computational Linguis- tics  doi: 10 18653/v1/2023 findings-eacl 65  URL https://aclanthology org/2023 findings-eacl 65 \n",
      "\n",
      "Benjamin Newman, Luca Soldaini, Raymond Fok, Arman Cohan, and Kyle Lo  A controllable qa- based framework for decontextualization  arXiv preprint arXiv:2305 14772 , 2023  URL https: //arxiv org/pdf/2305 14772 pdf  OpenAI  GPT-4 Technical Report  ArXiv , abs/2303 08774, 2023  URL https://arxiv\n",
      "\n",
      ":<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here's a summary of the text you provided:\n",
      "\n",
      "**Title:** Recursively Summarizing Books\n",
      "\n",
      "**Authors:** Wu et al. (2020)\n",
      "\n",
      "**Key details:**\n",
      "\n",
      "* The paper proposes a method for recursively summarizing books using a transformer-based approach.\n",
      "* The model is trained on a massive dataset of book summaries and context-free texts.\n",
      "* The summary generation process is designed to be self-supervised, requiring no human intervention.\n",
      "* The\n",
      "\n",
      "Index: 116, Text: <bos><start_of_turn>user\n",
      "Write a summary of the following, including as many key details as possible: skiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, et al  CoLT5: Faster long-range transformers with conditional computation  arXiv preprint arXiv:2303 09752 , 2023  URL https://arxiv org/abs/2303 09752  Ekin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and\n",
      "\n",
      "org/2023 findings-acl 679  Iz Beltagy, Matthew E  Peters, and Arman Cohan  Longformer: The Long-document Transformer, 2020  URL https://arxiv org/abs/2004 05150   arXiv preprint arXiv:2004 05150 Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Milli-\n",
      "\n",
      "org/abs/2303 12712  Shuyang Cao and Lu Wang  HIBRIDS: Attention with hierarchical biases for structure-aware long document summarization  In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp  786–807, Dublin, Ireland, May 2022 Association for Computational Linguistics  doi: 10 18653/v1/2022 acl-long 58\n",
      "\n",
      "1145/3077136 3080740  Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov Transformer-XL: Attentive language models beyond a fixed-length context  In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp  2978–2988, Florence, Italy, July 2019  Association for Computational Linguistics\n",
      "\n",
      "European Conference on Information Retrieval, ECIR 2023, Dublin, Ireland, April 2–6, 2023, Proceedings, Part I , pp  264–278  Springer, 2023a  URL https://link springer com/ chapter/10 1007/978-3-031-28244-7_17  Zican Dong, Tianyi Tang, Lunyi Li, and Wayne Xin Zhao  A survey on long text modeling with transformers\n",
      "\n",
      "14627  11 Published as a conference paper at ICLR 2024 Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang  LongT5: Efficient text-to-text transformer for long sequences  In Findings of the Association for Computational Linguistics: NAACL 2022 , pp  724–736, Seattle, United States, July 2022\n",
      "\n",
      " Lost in the middle: How language models use long contexts  arXiv preprint arXiv:2307 03172 , 2023  URL https://arxiv org/abs/2307 03172  Ye Liu, Kazuma Hashimoto, Yingbo Zhou, Semih Yavuz, Caiming Xiong, and Philip Yu  Dense hierarchical retrieval for open-domain question answering  In Marie-Francine Moens, Xuanjing\n",
      "\n",
      ":<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here's a summary of the article:\n",
      "\n",
      "**Title:** CoLT5: Faster long-range transformers with conditional computation\n",
      "\n",
      "**Authors:** Ekin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and Arman Cohan\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* The paper introduces CoLT5, a faster and more efficient implementation of Longformer.\n",
      "* CoLT5 uses a novel conditional computation mechanism to reduce\n",
      "\n",
      "Index: 107, Text: <bos><start_of_turn>user\n",
      "Write a summary of the following, including as many key details as possible: Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel\n",
      "\n",
      "org/ abs/2303 08774  Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman  QuALITY: 13 \n",
      "\n",
      ":<end_of_turn>\n",
      "<start_of_turn>model\n",
      "The context does not provide any information about the quality of the individuals or the context of their lives, so I cannot generate the requested summary.\n",
      "\n",
      "Index: 112, Text: <bos><start_of_turn>user\n",
      "Write a summary of the following, including as many key details as possible: 42% 19 20% SBERT without RAPTOR 29 26% 22 56% 5 95% 18 15% BM25 with RAPTOR 27 93% 21 17% 5 70% 17 03% BM25 without RAPTOR 23 52% 17 73% 4 65% 13 98% DPR with RAPTOR 30 94% 23\n",
      "\n",
      "51% 6 45% 19 05% DPR without RAPTOR 29 56% 22 84% 6 12% 18 44% Likewise, in the QuALITY dataset as shown in Table 4, RAPTOR achieves an accuracy of 62 4%, which is a 2% and 5 1% improvement over DPR and BM25  Similar trends are observed when Uni-\n",
      "\n",
      "metrics like BLEU-1, BLEU-4, and METEOR, RAPTOR outperforms BM25 and DPR by margins ranging from 1 7 to 5 8 and 0 7 to 2 1 points, respectively 2For the DPR experiments in Tables 1 and 2, we used the dpr-multiset-base model as opposed to dpr-single-nq-base which was used in rest of the experiments done earlier\n",
      "\n",
      "BM25 without RAPTOR 49 9% 26 47% DPR with RAPTOR 54 7% 32 23% DPR without RAPTOR 53 1% 31 70% Table 3: Controlled comparison of F-1 scores on the QASPER dataset, using three different lan- guage models (GPT-3, GPT-4, UnifiedQA 3B) and various retrieval methods  The column ”Title +\n",
      "\n",
      ":<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here is the summary you requested:\n",
      "\n",
      "The provided context provides a comparison between different language models, including RAPTOR, BM25, and DPR.\n",
      "\n",
      "**RAPTOR:**\n",
      "- Achieves high accuracy (95%) on the QASPER dataset.\n",
      "- Outperforms other models (DPR and BM25) by a significant margin.\n",
      "- The results suggest that RAPTOR is effective in improving the quality of generated text.\n",
      "\n",
      "**BM25:**\n",
      "- Achie\n",
      "\n",
      "Index: 109, Text: <bos><start_of_turn>user\n",
      "Write a summary of the following, including as many key details as possible: kens of context, as UnifiedQA has a max con- text length of 512 tokens  We provide the same amount of tokens of context to RAPTOR and to the baselines Qualitative Study We conduct a qualitative analysis to understand the benefits of RAP- TOR’s retrieval process compared to Dense Passage Retrieval (DPR) methods  Our study focuses on thematic, multi-hop questions using a 1500-word Cinderella fairytale  As illustrated\n",
      "\n",
      "directly or within higher-layer summaries our results demonstrate that RAPTOR, when combined with any retriever, consistently outperforms the respective retriever across all datasets 2 Since RAPTOR with SBERT has the best performance, we use it in all subsequent experiments We now compare RAPTOR with BM25 and DPR, using three different LLMs: GPT-3, GPT-4, and UnifiedQA\n",
      "\n",
      "5 points, and outdo BM25 by 6 5, 5 5, and 10 2 points across the respective LLMs  QASPER requires synthesizing information within NLP papers, so it is unsurprising that RAPTOR’s higher-level summary nodes would allow it to outperform methods that can only extract the top- kmost similar raw chunks of text, which may not contain the correct response in isolation\n",
      "\n",
      "Table 1: NarrativeQA Performance With + Without RAPTOR: Performance comparison of various retrieval methods (SBERT, BM25, DPR) with and without RAPTOR on the NarrativeQA dataset, using UnifiedQA-3B as the language model  RAPTOR outperforms baselines of each re- spective retrieval method Model ROUGE BLEU-1 BLEU-4 METEOR SBERT with RAPTOR 30 87% 23 50% 6\n",
      "\n",
      " This decision was based on the performance observed in Karpukhin et al  (2020), where dpr-multiset-base showed superior results 7 Published as a conference paper at ICLR 2024 Table 2: QuALITY and QASPER Performance With + Without RAPTOR: Performance com- parison across the QuALITY and QASPER datasets of various retrieval methods (SBERT, BM25, DPR) with and without RAPTOR\n",
      "\n",
      " UnifiedQA-3B is used as the language model  RAPTOR outper- forms baselines of each respective retrieval method for both datasets Model Accuracy (QuALITY) Answer F1 (QASPER) SBERT with RAPTOR 56 6% 36 70% SBERT without RAPTOR 54 9% 36 23% BM25 with RAPTOR 52 1% 27 00%\n",
      "\n",
      "Title + Abstract 25 2 22 2 17 5 BM25 46 6 50 2 26 4 DPR 51 3 53 0 32 1 RAPTOR 53 1 55 7 36 6 Table 4: Comparison of accuracies on the QuAL- ITY dev dataset for two different language mod- els (GPT-3, UnifiedQA 3B) using various retrieval methods\n",
      "\n",
      ":<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here is the summary you requested:\n",
      "\n",
      "The study compares the performance of RAPTOR, a multi-hop language model, against other retrieval methods on a Cinderella fairytale dataset.\n",
      "\n",
      "**Key Details:**\n",
      "\n",
      "* RAPTOR outperforms baselines of each respective retrieval method across all datasets.\n",
      "* When combined with SBERT, RAPTOR consistently outperforms BM25 and DPR by 6, 5, 5, and 10 points, respectively.\n",
      "* QASPER requires\n",
      "\n",
      "Index: 103, Text: <bos><start_of_turn>user\n",
      "Write a summary of the following, including as many key details as possible: tiveQA, QASPER, and QuALITY  NarrativeQA is a dataset that comprises question-answer pairs based on the full texts of books and movie transcripts, totaling 1,572 documents (Ko ˇcisk`y et al , 2018; Wu et al , 2021)  The NarrativeQA-Story task requires a comprehensive understanding of the entire narrative in order to accurately answer its questions, thus testing the model’s ability to comprehend longer texts in\n",
      "\n",
      "the literary domain  We measure performance on this dataset using the standard BLEU (B-1, B-4), ROUGE (R-L), and METEOR (M) metrics  Please see appendix H for more details on the Narra- tiveQA evaluation script used in our experiments The QASPER dataset includes 5,049 questions across 1,585 NLP papers, with each question probing for information embedded within the full text (Dasigi et al\n",
      "\n",
      ", 2021)  The answer types in QASPER are categorized as Answerable/Unanswerable, Yes/No, Abstractive, and Extractive  Accuracy is measured using standard F1 Lastly, the QuALITY dataset consists of multiple-choice questions, each accompanied by context passages averaging approximately 5,000 tokens in length (Pang et al , 2022)  This dataset calls for\n",
      "\n",
      "reasoning over the entire document for QA tasks, enabling us to measure the performance of our re- trieval system on medium-length documents  The dataset includes a challenging subset, QuALITY- HARD, which contains questions that a majority of human annotators answered incorrectly in a speed-setting  We report accuracies for both the entire test set and the HARD subset Controlled Baseline Comparisons We first present controlled comparisons using the UnifiedQA\n",
      "\n",
      "QuALITY-HARD, which represents questions that humans took unusually long to correctly answer, requiring rereading parts of the text, difficult reasoning, or both For the NarrativeQA dataset, as represented in Table 6, RAPTOR paired with UnifiedQA sets a new state-of-the-art METEOR score  When compared to the recursively summarizing model by Wu et al  (2021), which also employs UnifiedQA, RAPTOR outperforms it on all metrics  While\n",
      "\n",
      "pothesized that upper nodes play a crucial role in handling thematic or multi-hop queries requiring a broader understanding of the text 8 Published as a conference paper at ICLR 2024 Table 6: Performance comparison on the NarrativeQA dataset across multiple models, focusing on four metrics: ROUGE-L, BLEU-1, BLEU-4, and METEOR  RAPTOR, when paired with Uni-\n",
      "\n",
      "8 23 5 6 4 19 1 Table 7: Accuracies of the QuALITY dataset on both the overall test set and the more challenging hard subset  GPT-4 with RAPTOR sets a new state-of-the-art ModelAccuracy Test Set Hard Subset Longformer-base (Beltagy et al , 2020) 39 5 35  3 DPR and DeBERTaV3-large (Pang et al\n",
      "\n",
      "experiments: GPT-3 and GPT-4 for QA tasks, and GPT-3 5-turbo for summarization  The gpt-3 , gpt-4 , and gpt-3 5-turbo models can be accessed via API calls (OpenAI API)  UnifiedQA, which is used for QA tasks, is publicly available at Hugging Face Evaluation Datasets The three evaluation datasets used in our experiments—QuALITY,\n",
      "\n",
      "QASPER, and NarrativeQA—are all publicly accessible  These datasets ensure that the retrieval and QA tests conducted in this study can be replicated Source Code The source code for RAPTOR will be publicly available here REFERENCES Charu C Aggarwal, Alexander Hinneburg, and Daniel A Keim  On the Surprising Behavior of Dis- tance Metrics in High Dimensional Space  In Database Theory—ICDT 2001: 8th International\n",
      "\n",
      ":<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here's a summary of the provided information:\n",
      "\n",
      "**NarrativeQA Dataset**\n",
      "\n",
      "- A dataset of question-answer pairs based on the full texts of books and movie transcripts.\n",
      "- 1,572 documents, with each document containing questions and answers.\n",
      "- Measures performance on this dataset using BLEU, ROUGE, and METEOR metrics.\n",
      "\n",
      "**QASPER Dataset**\n",
      "\n",
      "- A dataset of questions across 1,585 NLP papers.\n",
      "-\n",
      "\n",
      "Index: 104, Text: <bos><start_of_turn>user\n",
      "Write a summary of the following, including as many key details as possible: Stefanos Angelidis and Mirella Lapata  Summarizing opinions: Aspect extraction meets sentiment prediction and they are both weakly supervised  arXiv preprint arXiv:1808 08858 , 2018  URL https://arxiv org/abs/1808 08858  Manoj Ghuhan Arivazhagan, Lan Liu, Peng Qi, Xinchi Chen, William Yang Wang, and Zhiheng Huang\n",
      "\n",
      " Hybrid hierarchical retrieval for open-domain question answering  In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds ), Findings of the Association for Computational Linguistics: ACL 2023 , pp  10680–10689, Toronto, Canada, July 2023  Association for Computa- tional Linguistics  doi: 10 18653/v1/2023 findings-acl 679  URL https://aclanthology\n",
      "\n",
      " URL https: //aclanthology org/2022 acl-long 58  Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes  Reading Wikipedia to Answer Open-Domain Questions  In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp  1870–1879, Vancouver, Canada, July 2017  Association for Computational Linguistics  doi: 10\n",
      "\n",
      "ciation for Computational Linguistics  doi: 10 18653/v1/2021 naacl-main 365  URL https: //aclanthology org/2021 naacl-main 365  Mengxing Dong, Bowei Zou, Yanling Li, and Yu Hong  CoLISA: Inner Interaction via Contrastive Learning for Multi-choice Reading Comprehension  In Advances in Information Retrieval: 45th\n",
      "\n",
      " Association for Computational Linguistics  doi: 10 18653/v1/2022 findings-naacl 55 URLhttps://aclanthology org/2022 findings-naacl 55  Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang  Retrieval Augmented Language Model Pre-Training  In International conference on machine learning , pp  3929–3938 PMLR, 2020\n",
      "\n",
      "Huang, Lucia Specia, and Scott Wen-tau Yih (eds ), Findings of the Association for Compu- tational Linguistics: EMNLP 2021 , pp  188–200, Punta Cana, Dominican Republic, Novem- ber 2021  Association for Computational Linguistics  doi: 10 18653/v1/2021 findings-emnlp 19 URL https://aclanthology org/2021 findings-emnlp\n",
      "\n",
      " URL https://aclanthology org/2021 emnlp-main 560  Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer  Nonparametric masked language modeling  In Findings of the Association for Computational Linguistics: ACL 2023 , pp  2097–2118, Toronto, Canada, July 2023  Associ-\n",
      "\n",
      ":<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here's a summary of the paper:\n",
      "\n",
      "The paper presents a novel hybrid hierarchical retrieval method for open-domain question answering. This method combines the strengths of aspect extraction and sentiment prediction to improve the quality of retrieved passages.\n",
      "\n",
      "The paper introduces a new dataset called \"Hybrid Retrieval\" that consists of pairs of question-answer pairs. The first question is used to query a large text corpus, and the corresponding answers are retrieved from a smaller subset of documents that are considered to be relevant to\n",
      "\n",
      "Index: 113, Text: <bos><start_of_turn>user\n",
      "Write a summary of the following, including as many key details as possible: Published as a conference paper at ICLR 2024 Given a set of Ntext segments, each represented as a d-dimensional dense vector embedding, the likelihood of a text vector, x, given its membership in the kthGaussian distribution, is denoted by P(x|k) =N(x;µk,Σk)  The overall probability distribution is a weighted combination P(x) =PK\n",
      "\n",
      "k=1πkN(x;µk,Σk), where πksignifies the mixture weight for the kthGaussian distribution The high dimensionality of vector embeddings presents a challenge for traditional GMMs, as dis- tance metrics may behave poorly when used to measure similarity in high-dimensional spaces (Ag- garwal et al , 2001)  To mitigate this, we employ Uniform Manifold Approximation and Projection\n",
      "\n",
      "among the text data, from broad themes to specific details Should a local cluster’s combined context ever exceed the summarization model’s token threshold, our algorithm recursively applies clustering within the cluster, ensuring that the context remains within the token threshold To determine the optimal number of clusters, we employ the Bayesian Information Criterion (BIC) for model selection  BIC not only penalizes model complexity but also rewards goodness of fit (Schwarz, 1978)\n",
      "\n",
      " The BIC for a given GMM is BIC = ln( N)k−2 ln(ˆL), where Nis the number of text segments (or data points), kis the number of model parameters, and ˆLis the maximized value of the likelihood function of the model  In the context of GMM, the number of parameters k is a function of the dimensionality of the input vectors and the number of clusters\n",
      "\n",
      "With the optimal number of clusters determined by BIC, the Expectation-Maximization algorithm is then used to estimate the GMM parameters, namely the means, covariances, and mixture weights While the Gaussian assumption in GMMs may not perfectly align with the nature of text data, which often exhibits a sparse and skewed distribution, our empirical observations suggest that it offers an effective model for our purpose  We run an ablation comparing GMM Clustering with summarizing\n",
      "\n",
      "contiguous chunks and provide details in Appendix B Model-Based Summarization After clustering the nodes using Gaussian Mixture Models, the nodes in each cluster are sent to a language model for summarization  This step allows the model to transform large chunks of text into concise, coherent summaries of the selected nodes  For our experiments, we use gpt-3 5-turbo to generate the summaries  The summarization step con- denses the potentially large volume of retrieved information into a manageable size\n",
      "\n",
      " We provide statistics on the compression due to the summarization in Appendix C and the prompt used for summarization in Appendix D While the summarization model generally produces reliable summaries, a focused annotation study revealed that about 4% of the summaries contained minor hallucinations  These did not propagate to parent nodes and had no discernible impact on question-answering tasks  For an in-depth analysis of hallucinations, refer to the appendix E\n",
      "\n",
      ":<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here's a summary of the paper:\n",
      "\n",
      "**Title:** Exploring Text Data with Uniform Manifold Approximation and Projection\n",
      "\n",
      "**Abstract:**\n",
      "The paper proposes a novel approach for text data analysis that leverages the Uniform Manifold Approximation and Projection (UMAAP) framework to address the challenges posed by high-dimensional vector embeddings in GMMs.\n",
      "\n",
      "**Key Concepts:**\n",
      "* **Uniform Manifold Approximation and Projection (UMAAP):** A framework that projects high-dimensional vectors onto a\n",
      "\n",
      "Index: 118, Text: <bos><start_of_turn>user\n",
      "Write a summary of the following, including as many key details as possible: (UMAP), a manifold learning technique for dimensionality reduction (McInnes et al , 2018)  The number of nearest neighbors parameter, nneighbors , in UMAP determines the balance between the preservation of local and global structures  Our algorithm varies nneighbors to create a hierar- chical clustering structure: it first identifies global clusters and then performs local clustering within these global clusters  This two-step clustering process captures a broad spectrum of relationships\n",
      "\n",
      "Conference London, UK, January 4–6, 2001 Proceedings 8 , pp  420–434  Springer, 2001  URL https://link springer com/chapter/10 1007/3-540-44503-x_27  Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Onta ˜n´on, Siddhartha Brahma, Yury Zemlyan-\n",
      "\n",
      " Lin (eds ), Advances in Neu- ral Information Processing Systems , volume 33, pp  1877–1901  Curran Associates, Inc , 10 Published as a conference paper at ICLR 2024 2020  URL https://proceedings neurips cc/paper_files/paper/2020/ file/1457c0d6bfcb4967418bfb8ac142f64a-Paper pdf \n",
      "\n",
      " 6769–6781, Online, November 2020  Association for Computational Linguis- tics  doi: 10 18653/v1/2020 emnlp-main 550  URL https://aclanthology org/2020 emnlp-main 550  Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and\n",
      "\n",
      "33:9459–9474, 2020  URL https://doi org/10 48550/arXiv 2005 11401  Jerry Liu  LlamaIndex, 2022  URL https://github com/jerryjliu/llama_index  Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang\n",
      "\n",
      "19  Leland McInnes, John Healy, and James Melville  UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction, 2018  URL https://arxiv org/abs/1802 03426   arXiv preprint arXiv:1802 03426 Sewon Min, Kenton Lee, Ming-Wei Chang, Kristina Toutanova, and Hannaneh Hajishirzi  Joint\n",
      "\n",
      " doi: 10 18653/v1/2020 nuse-1 13  URL https: //aclanthology org/2020 nuse-1 13  Inderjeet Nair, Aparna Garimella, Balaji Vasan Srinivasan, Natwar Modani, Niyati Chhaya, Srikr- ishna Karanam, and Sumit Shekhar  A neural CRF-based hierarchical approach for lin-\n",
      "\n",
      ":<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here's a summary of the provided text:\n",
      "\n",
      "**UMAP (Uniform Manifold Approximation and Projection for Dimension Reduction)** is a manifold learning technique for dimensionality reduction. It uses a hierarchical clustering approach to capture a broad spectrum of relationships between data points.\n",
      "\n",
      "**Key features of UMAP:**\n",
      "\n",
      "- **Number of nearest neighbors (nneighbors):** This parameter determines the balance between the preservation of local and global structures in the data.\n",
      "- **Hierarchical clustering:** It first identifies global\n",
      "\n",
      "Index: 108, Text: <bos><start_of_turn>user\n",
      "Write a summary of the following, including as many key details as possible:  The children of these selected nodes are considered at the next layer and the top-k nodes are selected from this pool again based on their cosine similarity to the query vector  This process is repeated until we reach the leaf nodes  Finally, the text from all selected nodes is concatenated to form the retrieved context  The algorithm’s steps are outlined below: 1  Start at the root layer of the RAPTOR tree  Compute the cosine similarity between the\n",
      "\n",
      "query embedding and the embeddings of all nodes present at this initial layer 2  Choose the top- knodes based on the highest cosine similarity scores, forming the set S1 4 Published as a conference paper at ICLR 2024 Figure 2: Illustration of the tree traversal and collapsed tree retrieval mechanisms  Tree traver- sal starts at the root level of the tree and retrieves the top- k(here, top- 1) node(s) based on cosine\n",
      "\n",
      "similarity to the query vector  At each level, it retrieves the top- knode(s) from the child nodes of the previous layer’s top- k  Collapsed tree collapses the tree into a single layer and retrieves nodes until a threshold number of tokens is reached, based on cosine similarity to the query vector  The nodes on which cosine similarity search is performed are highlighted in both illustrations 3  Proceed to the child nodes of the elements in set S1  Compute the cosine similarity between\n",
      "\n",
      "the query vector and the vector embeddings of these child nodes 4  Select the top kchild nodes with the highest cosine similarity scores to the query, forming the set S2 5  Continue this process recursively for dlayers, producing sets S1, S2,   , S d 6  Concatenate sets S1through Sdto assemble the relevant context to the query By adjusting the depth dand the number of nodes kselected at each layer, the tree traversal method\n",
      "\n",
      "offers control over the specificity and breadth of the information retrieved  The algorithm starts with a broad outlook by considering the top layers of the tree and progressively focuses on finer details as it descends through the lower layers Thecollapsed tree approach offers a simpler way to search for relevant information by considering all nodes in the tree simultaneously, as depicted in Figure 2  Instead of going layer-by-layer, this method flattens the multi-layered tree into a single layer, essentially bringing all the nodes onto the\n",
      "\n",
      "same level for comparison  The steps for this method are outlined below: 1  First, collapse the entire RAPTOR tree into a single layer  This new set of nodes, denoted asC, contains nodes from every layer of the original tree 2  Next, calculate the cosine similarity between the query embedding and the embeddings of all nodes present in the collapsed set C 3  Finally, pick the top- knodes that have the highest cosine similarity scores with the query\n",
      "\n",
      "Keep adding nodes to the result set until you reach a predefined maximum number of tokens, ensuring you don’t exceed the model’s input limitations We tested both approaches on 20 stories from the QASPER dataset  Figure 3 shows the performance of tree traversal with different top- sizes and collapsed tree with different maximum token numbers The collapsed tree approach consistently performs better  We believe collapsed tree retrieval is better due to offering greater flexibility than tree traversal; i e , by searching through all the nodes\n",
      "\n",
      "simultaneously, it retrieves information that is at the correct level of granularity for a given question In comparison, while using tree traversal with the same values of dandk, the ratio of nodes from each level of the tree will be constant  So, the ratio of higher-order thematic information to granular details will remain the same regardless of the question 5 Published as a conference paper at ICLR 2024 One drawback, however, of the collapsed tree approach is that it requires cosine similarity search to\n",
      "\n",
      "be performed on all nodes in the tree  However, this can be made more efficient with fast k-nearest neighbor libraries such as FAISS (Johnson et al , 2019) Figure 3: Comparison of querying methods Results on 20 stories from the QASPER dataset using tree traversal with different top-k values, and collapsed tree with different context lengths Collapsed tree with 2000 tokens produces the best results, so we use this querying strategy for our main results\n",
      "\n",
      "Overall, given the collapsed tree approach’s greater flexibility and its superior performance on the subset of the QASPER dataset, this is the querying approach with which we proceed Specifically, we use the collapsed tree with 2000 maximum tokens, which approximately equates to retrieving the top-20 nodes  Using a token-based approach ensures the context does not exceed model context constraints as token counts can vary across nodes  For experiments with the UnifiedQA model, we provide 400 to-\n",
      "\n",
      " Columns represent different starting points (highest layer) and rows represent different numbers of layers queried Layers Queried / Start Layer Layer 0 (Leaf Nodes) Layer 1 Layer 2 1 layer 57 9 57 8 57 9 2 layers - 52 6 63 15 3 layers - - 73 68 We validated this hypothesis both quantitatively and qualitatively  We present qualitative analysis in appendix G\n",
      "\n",
      ":<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here's a summary of the text you provided:\n",
      "\n",
      "**Raptor Tree Retrieval Algorithm:**\n",
      "\n",
      "1. Start with the root layer of the RAPTOR tree.\n",
      "2. Compute the cosine similarity between the query embedding and the embeddings of all nodes in the tree.\n",
      "3. Choose the top-k nodes with the highest cosine similarity scores to form the set S1.\n",
      "4. Repeat step 2 for the child nodes of the elements in set S1.\n",
      "5.\n",
      "\n",
      "Index: 117, Text: <bos><start_of_turn>user\n",
      "Write a summary of the following, including as many key details as possible: Querying In this section, we elaborate on the two querying mechanisms employed by RAPTOR: tree traversal and collapsed tree  These methods offer unique ways of traversing the multi-layered RAPTOR tree to retrieve relevant information, each with its own advantages and trade-offs  We provide the pseudocode of both methods in Appendix F  Note that we embed all nodes using SBERT The tree traversal method first selects the top-k most relevant root nodes based on their cosine similarity to the query embedding\n",
      "\n",
      "in Figure 4, RAPTOR’s tree-based retrieval allows it to choose nodes from different tree layers, matching the question’s detail level  This approach often yields more relevant and comprehensive information for downstream tasks than DPR  For a detailed discussion and examples, including the text retrieved by both RAPTOR and DPR for specific questions, please refer to the appendix G 4 E XPERIMENTS Datasets We measure RAPTOR’s performance across three question-answering datasets: Narra-\n",
      "\n",
      "3B as the reader, with SBERT (Reimers & Gurevych, 2019), BM25 (Robertson et al , 1995; 2009), and DPR (Karpukhin et al , 2020) as the embedding models with and without the RAPTOR tree structure, on three datasets: QASPER, NarrativeQA, and QuALITY   As shown in Tables 1 and 2,\n",
      "\n",
      "6 Published as a conference paper at ICLR 2024 Figure 4: Querying Process: Illustration of how RAPTOR retrieves information for two questions about the Cinderella story: “What is the central theme of the story ” and “How did Cinderella find a happy ending ”  Highlighted nodes indicate RAPTOR’s selections, while arrows point to DPR’s leaf nodes  Notably, RAPTOR’s context often encompasses the information retrieved by DPR, either\n",
      "\n",
      "Wu et al  (2021) rely solely on the summary in the top root node of the tree structure, RAPTOR benefits from its intermediate layers and clustering approaches, which allows it to capture a range of information, from general themes to specific details, contributing to its overall strong performance 4 1 C ONTRIBUTION OF THE TREE STRUCTURE We examine the contribution of each layer of nodes to RAPTOR’s retrieval capabilities  We hy-\n",
      "\n",
      ", 2022) 55 4 46  1 CoLISA (DeBERTaV3-large) (Dong et al , 2023a) 62 3 54  7 RAPTOR + GPT-4 82 6 76 2 Table 8: Performance of RAPTOR when querying different tree layers for Story 1 from the QuAL- ITY dataset\n",
      "\n",
      " To quantitatively understand the contribution of the upper-level nodes, we used stories from the QuALITY dataset  The RAPTOR tree is built for each of these stories, as described in Section 3  However, during retrieval, we limit the search to different subsets of layers  For example, we exclusively retrieve from the leaf nodes and each upper layer, as well as from different contiguous subsets of the layers\n",
      "\n",
      " We show findings specific to one story in Table 8, revealing that a full-tree search, utilizing all layers, outperformed retrieval strategies that focused only on specific layers These findings highlight the importance of the full tree structure in RAPTOR  By providing both the original text and higher-level summaries for retrieval, RAPTOR can effectively handle a wider range of questions, from higher-order thematic queries to detail-oriented questions  Detailed results\n",
      "\n",
      "for additional stories and an ablation study on layer contributions can be found in Appendix I 5 C ONCLUSION In this paper, we have presented RAPTOR, a novel tree-based retrieval system that augments the parametric knowledge of large language models with contextual information at various levels of abstraction  By employing recursive clustering and summarization techniques, RAPTOR creates a hierarchical tree structure that is capable of synthesizing information across various sections of the retrieval corpora\n",
      "\n",
      " During the query phase, RAPTOR leverages this tree structure for more effective retrieval  Our controlled experiments demonstrated that RAPTOR not only outperforms traditional retrieval methods but also sets new performance benchmarks on several question-answering tasks 9 Published as a conference paper at ICLR 2024 6 R EPRODUCIBILITY STATEMENT Language Models for QA and Summarization Four language models are used in our RAPTOR\n",
      "\n",
      ":<end_of_turn>\n",
      "<start_of_turn>model\n",
      "**Summary:**\n",
      "\n",
      "**Querying Mechanisms:**\n",
      "- Tree traversal: Selects top-k most relevant root nodes based on cosine similarity to query embedding.\n",
      "- Collapsed tree: Uses a hierarchical structure to capture information from different tree layers.\n",
      "\n",
      "**Tree traversal method:**\n",
      "- Focuses on top-k most relevant root nodes.\n",
      "- Provides more comprehensive information due to intermediate layers' clustering.\n",
      "\n",
      "**Collapsed tree method:**\n",
      "- Uses a hierarchical structure to capture information from different tree layers\n",
      "\n",
      "============================================level 2============================================\n",
      "Index: 38, Text: Abstract” reflects performance when only the title and abstract of the papers are used for context RAPTOR outperforms the established baselines BM25 and DPR across all tested language models Specifically, RAPTOR’s F-1 scores are at least 1 8% points higher than DPR and at least 5 3% points higher than BM25 Retriever GPT-3 F-1 Match GPT-4 F-1 Match UnifiedQA F-1 Match\n",
      "\n",
      "Index: 40, Text:  RAPTOR outperforms the baselines of BM25 and DPR by at least 2 0% in accuracy Model GPT-3 Acc  UnifiedQA Acc BM25 57 3 49 9 DPR 60 4 53 9 RAPTOR 62 4 56 6 Table 5: Results on F-1 Match scores of various models on the QASPER dataset Model F-1 Match LongT5 XL (Guo et al\n",
      "\n",
      "Index: 41, Text: , 2022) 53 1 CoLT5 XL (Ainslie et al , 2023) 53 9 RAPTOR + GPT-4 55 7Comparison to State-of-the-art Systems Building upon our controlled comparisons, we examine RAPTOR’s performance relative to other state-of-the-art models  As shown in Table 5, RAPTOR with GPT-4 sets a new benchmark on QASPER, with a 55\n",
      "\n",
      "Index: 42, Text: 7% F-1 score, surpassing the CoLT5 XL’s score of 53 9% In the QuALITY dataset, as shown in Table 7, RAPTOR paired with GPT-4 sets a new state- of-the-art with an accuracy of 82 6%, surpass- ing the previous best result of 62 3%  In par- ticular, it outperforms CoLISA by 21 5% on\n",
      "\n",
      "Index: 28, Text:  As shown in Table 3, RAPTOR consistently outperforms BM25 and DPR across all three Language Models on the QASPER dataset  RAPTOR’s F-1 Match scores are 53 1%, 55 7%, and 36 6% when using GPT-3, GPT-4, and UnifiedQA, respectively  These scores surpass DPR by margins of 1 8, 2 7, and 4\n",
      "\n",
      "Index: 33, Text: fiedQA is employed, with RAPTOR outperforming DPR and BM25 by 2 7% and 6 7%, respectively Finally, in the NarrativeQA dataset, as presented in Table 6, RAPTOR excels across multiple met- rics  For ROUGE-L, it surpasses BM25 and DPR by 7 3 and 2 7 points, respectively  In other\n",
      "\n",
      "Index: 46, Text: fiedQA 3B, not only surpasses retrieval methods like BM25 and DPR but also sets a new state-of- the-art in the METEOR metric Model ROUGE-L BLEU-1 BLEU-4 METEOR BiDAF (Ko ˇcisk`y et al , 2018) 6 2 5  7 0  3 3  7 BM25 + BERT (Mou et al\n",
      "\n",
      "Index: 65, Text: Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad- ford, Ilya Sutskever, and Dario Amodei  Language Models are Few-Shot Learners  In H  Larochelle, M  Ranzato, R  Hadsell, M F  Balcan, and H\n",
      "\n",
      "Index: 82, Text: Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave  Few-shot learning with re- trieval augmented language models  arXiv preprint arXiv:2208 03299 , 2022  URL https: //arxiv org/abs/2208 03299 \n",
      "\n",
      "Index: 97, Text: ation for Computational Linguistics  doi: 10 18653/v1/2023 findings-acl 132  URL https: //aclanthology org/2023 findings-acl 132  Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn Memory-based model editing at scale  In International Conference on Machine Learning , pp  15817–15831  PMLR, 2022  URL https://proceedings mlr\n",
      "\n",
      "Index: 70, Text: 18653/v1/P17-1171  URL https: //aclanthology org/P17-1171  Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al  PaLM: Scaling Language Modeling with Pathways  arXiv preprint arXiv:2204\n",
      "\n",
      "Index: 77, Text:  arXiv preprint arXiv:2302 14502 , 2023b  URL https://arxiv org/abs/ 2302 14502  Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen  Enabling large language models to generate text with citations  arXiv preprint arXiv:2305 14627 , 2023  URL https://arxiv org/ abs/2305\n",
      "\n",
      "Index: 80, Text:  URL https://doi org/10 48550/arXiv 2002 08909  Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al Training compute-optimal large language models  arXiv preprint arXiv:2203 15556 , 2022  URL\n",
      "\n",
      "Index: 83, Text: Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig  How can we know what language models know  Transactions of the Association for Computational Linguistics , 8:423–438, 2020 URL https://arxiv org/abs/1911 12543  Jeff Johnson, Matthijs Douze, and Herv ´e J´egou  Billion-Scale Similarity Search with GPUs  IEEE\n",
      "\n",
      "Index: 84, Text: Transactions on Big Data , 7(3):535–547, 2019  URL https://arxiv org/abs/1702 08734  Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel  Large Language Models struggle to learn Long-Tail Knowledge  In International Conference on Machine Learn- ing, pp  15696–15707  PMLR, 2023  URL https://proceedings\n",
      "\n",
      "Index: 59, Text: Kelvin Guu  Towards tracing knowledge in language models back to the training data  In Findings of the Association for Computational Linguistics: EMNLP 2022 , pp  2429–2446, Abu Dhabi, United Arab Emirates, December 2022  Association for Computational Linguistics doi: 10 18653/v1/2022 findings-emnlp 180  URL https://aclanthology org/2022 findings-emnlp 180 \n",
      "\n",
      "Index: 63, Text: can, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al Improving language models by retrieving from trillions of tokens  In International conference on machine learning , pp  2206–2240  PMLR, 2022  URL https://arxiv org/abs/2112 04426 \n",
      "\n",
      "Index: 73, Text:  doi: 10 18653/v1/P19-1285  URL https://aclanthology org/P19-1285  Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R ´e  FlashAttention: Fast and memory-efficient exact attention with IO-Awareness  Advances in Neural Information Processing Systems , 35:16344–16359, 2022  URL https://arxiv org/abs/2205\n",
      "\n",
      "Index: 74, Text: 14135  Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A  Smith, and Matt Gardner  A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers  In Proceed- ings of the 2021 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies , pp  4599–4610, Online, June 2021  Asso-\n",
      "\n",
      "Index: 67, Text: S´ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka- mar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al  Sparks of Artificial General Intelligence: Early Experiments with GPT-4  arXiv preprint arXiv:2303 12712 , 2023  URL https://arxiv\n",
      "\n",
      "Index: 90, Text: 12 Published as a conference paper at ICLR 2024 Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt ¨aschel, et al  Retrieval-Augmented Gener- ation for Knowledge-Intensive NLP Tasks  Advances in Neural Information Processing Systems ,\n",
      "\n",
      "Index: 98, Text: press/v162/ mitchell22a/mitchell22a pdf  Xiangyang Mou, Mo Yu, Bingsheng Yao, Chenghao Yang, Xiaoxiao Guo, Saloni Potdar, and Hui Su  Frustratingly hard evidence retrieval for QA over books  In Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events , pp  108–113, Online, July 2020 Association for Computational Linguistics\n",
      "\n",
      "Index: 100, Text: ear text segmentation  In Findings of the Association for Computational Linguistics: EACL 2023 , pp  883–893, Dubrovnik, Croatia, May 2023  Association for Computational Linguis- tics  doi: 10 18653/v1/2023 findings-eacl 65  URL https://aclanthology org/2023 findings-eacl 65 \n",
      "\n",
      "Index: 101, Text: Benjamin Newman, Luca Soldaini, Raymond Fok, Arman Cohan, and Kyle Lo  A controllable qa- based framework for decontextualization  arXiv preprint arXiv:2305 14772 , 2023  URL https: //arxiv org/pdf/2305 14772 pdf  OpenAI  GPT-4 Technical Report  ArXiv , abs/2303 08774, 2023  URL https://arxiv\n",
      "\n",
      "Index: 71, Text: 02311 , 2022  URL https://arxiv org/abs/2204 02311  Arman Cohan and Nazli Goharian  Contextualizing citations for scientific summarization using word embeddings and domain knowledge  In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval , pp  1133–1136, 2017  URL https://dl acm org/doi/abs/10\n",
      "\n",
      "Index: 47, Text: , 2020) 15 5 14  5 1  4 5  0 Recursively Summarizing Books (Wu et al , 2021) 21 6 22  3 4  2 10  6 Retriever + Reader (Izacard & Grave, 2022) 32 0 35 3 7 5 11 1 RAPTOR + UnifiedQA 30\n",
      "\n",
      "Index: 81, Text: https://arxiv org/abs/2203 15556  Gautier Izacard and Edouard Grave  Distilling Knowledge from Reader to Retriever for Ques- tion Answering, 2022  URL https://arxiv org/abs/2012 04584   arXiv preprint arXiv:2012 04584\n",
      "\n",
      "Index: 85, Text: mlr press/v202/ kandpal23a/kandpal23a pdf  Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih  Dense Passage Retrieval for Open-Domain Question Answering  In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp\n",
      "\n",
      "Index: 87, Text: Hannaneh Hajishirzi  UNIFIEDQA: Crossing format boundaries with a single QA system InFindings of the Association for Computational Linguistics: EMNLP 2020 , pp  1896–1907, Online, November 2020  Association for Computational Linguistics  doi: 10 18653/v1/2020 findings-emnlp 171  URL https://aclanthology org/2020 findings-emnlp 171\n",
      "\n",
      "Index: 88, Text: Omar Khattab and Matei Zaharia  ColBERT: Efficient and effective passage search via con- textualized late interaction over bert  In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval , pp  39–48, 2020  URL https://arxiv org/abs/2004 12832 \n",
      "\n",
      "Index: 89, Text: Tom´aˇs Ko ˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G ´abor Melis, and Edward Grefenstette  The NarrativeQA Reading Comprehension Challenge  Transactions of the Association for Computational Linguistics , 6:317–328, 2018  URL https://arxiv org/abs/1712 07040 \n",
      "\n",
      "Index: 95, Text: passage ranking for diverse multi-answer retrieval  In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds ), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pp  6997–7008, Online and Punta Cana, Dominican Republic, November 2021  Association for Computational Linguistics  doi: 10 18653/v1/2021 emnlp-main 560\n",
      "\n",
      "Index: 68, Text: org/abs/2303 12712  Shuyang Cao and Lu Wang  HIBRIDS: Attention with hierarchical biases for structure-aware long document summarization  In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp  786–807, Dublin, Ireland, May 2022 Association for Computational Linguistics  doi: 10 18653/v1/2022 acl-long 58\n",
      "\n",
      "Index: 72, Text: 1145/3077136 3080740  Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov Transformer-XL: Attentive language models beyond a fixed-length context  In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp  2978–2988, Florence, Italy, July 2019  Association for Computational Linguistics\n",
      "\n",
      "Index: 76, Text: European Conference on Information Retrieval, ECIR 2023, Dublin, Ireland, April 2–6, 2023, Proceedings, Part I , pp  264–278  Springer, 2023a  URL https://link springer com/ chapter/10 1007/978-3-031-28244-7_17  Zican Dong, Tianyi Tang, Lunyi Li, and Wayne Xin Zhao  A survey on long text modeling with transformers\n",
      "\n",
      "Index: 78, Text: 14627  11 Published as a conference paper at ICLR 2024 Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang  LongT5: Efficient text-to-text transformer for long sequences  In Findings of the Association for Computational Linguistics: NAACL 2022 , pp  724–736, Seattle, United States, July 2022\n",
      "\n",
      "Index: 58, Text: skiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, et al  CoLT5: Faster long-range transformers with conditional computation  arXiv preprint arXiv:2303 09752 , 2023  URL https://arxiv org/abs/2303 09752  Ekin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and\n",
      "\n",
      "Index: 92, Text:  Lost in the middle: How language models use long contexts  arXiv preprint arXiv:2307 03172 , 2023  URL https://arxiv org/abs/2307 03172  Ye Liu, Kazuma Hashimoto, Yingbo Zhou, Semih Yavuz, Caiming Xiong, and Philip Yu  Dense hierarchical retrieval for open-domain question answering  In Marie-Francine Moens, Xuanjing\n",
      "\n",
      "Index: 62, Text: org/2023 findings-acl 679  Iz Beltagy, Matthew E  Peters, and Arman Cohan  Longformer: The Long-document Transformer, 2020  URL https://arxiv org/abs/2004 05150   arXiv preprint arXiv:2004 05150 Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Milli-\n",
      "\n",
      "Index: 64, Text: Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel\n",
      "\n",
      "Index: 102, Text: org/ abs/2303 08774  Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman  QuALITY: 13 \n",
      "\n",
      "Index: 32, Text: 51% 6 45% 19 05% DPR without RAPTOR 29 56% 22 84% 6 12% 18 44% Likewise, in the QuALITY dataset as shown in Table 4, RAPTOR achieves an accuracy of 62 4%, which is a 2% and 5 1% improvement over DPR and BM25  Similar trends are observed when Uni-\n",
      "\n",
      "Index: 34, Text: metrics like BLEU-1, BLEU-4, and METEOR, RAPTOR outperforms BM25 and DPR by margins ranging from 1 7 to 5 8 and 0 7 to 2 1 points, respectively 2For the DPR experiments in Tables 1 and 2, we used the dpr-multiset-base model as opposed to dpr-single-nq-base which was used in rest of the experiments done earlier\n",
      "\n",
      "Index: 37, Text: BM25 without RAPTOR 49 9% 26 47% DPR with RAPTOR 54 7% 32 23% DPR without RAPTOR 53 1% 31 70% Table 3: Controlled comparison of F-1 scores on the QASPER dataset, using three different lan- guage models (GPT-3, GPT-4, UnifiedQA 3B) and various retrieval methods  The column ”Title +\n",
      "\n",
      "Index: 31, Text: 42% 19 20% SBERT without RAPTOR 29 26% 22 56% 5 95% 18 15% BM25 with RAPTOR 27 93% 21 17% 5 70% 17 03% BM25 without RAPTOR 23 52% 17 73% 4 65% 13 98% DPR with RAPTOR 30 94% 23\n",
      "\n",
      "Index: 35, Text:  This decision was based on the performance observed in Karpukhin et al  (2020), where dpr-multiset-base showed superior results 7 Published as a conference paper at ICLR 2024 Table 2: QuALITY and QASPER Performance With + Without RAPTOR: Performance com- parison across the QuALITY and QASPER datasets of various retrieval methods (SBERT, BM25, DPR) with and without RAPTOR\n",
      "\n",
      "Index: 36, Text:  UnifiedQA-3B is used as the language model  RAPTOR outper- forms baselines of each respective retrieval method for both datasets Model Accuracy (QuALITY) Answer F1 (QASPER) SBERT with RAPTOR 56 6% 36 70% SBERT without RAPTOR 54 9% 36 23% BM25 with RAPTOR 52 1% 27 00%\n",
      "\n",
      "Index: 39, Text: Title + Abstract 25 2 22 2 17 5 BM25 46 6 50 2 26 4 DPR 51 3 53 0 32 1 RAPTOR 53 1 55 7 36 6 Table 4: Comparison of accuracies on the QuAL- ITY dev dataset for two different language mod- els (GPT-3, UnifiedQA 3B) using various retrieval methods\n",
      "\n",
      "Index: 19, Text: kens of context, as UnifiedQA has a max con- text length of 512 tokens  We provide the same amount of tokens of context to RAPTOR and to the baselines Qualitative Study We conduct a qualitative analysis to understand the benefits of RAP- TOR’s retrieval process compared to Dense Passage Retrieval (DPR) methods  Our study focuses on thematic, multi-hop questions using a 1500-word Cinderella fairytale  As illustrated\n",
      "\n",
      "Index: 27, Text: directly or within higher-layer summaries our results demonstrate that RAPTOR, when combined with any retriever, consistently outperforms the respective retriever across all datasets 2 Since RAPTOR with SBERT has the best performance, we use it in all subsequent experiments We now compare RAPTOR with BM25 and DPR, using three different LLMs: GPT-3, GPT-4, and UnifiedQA\n",
      "\n",
      "Index: 29, Text: 5 points, and outdo BM25 by 6 5, 5 5, and 10 2 points across the respective LLMs  QASPER requires synthesizing information within NLP papers, so it is unsurprising that RAPTOR’s higher-level summary nodes would allow it to outperform methods that can only extract the top- kmost similar raw chunks of text, which may not contain the correct response in isolation\n",
      "\n",
      "Index: 30, Text: Table 1: NarrativeQA Performance With + Without RAPTOR: Performance comparison of various retrieval methods (SBERT, BM25, DPR) with and without RAPTOR on the NarrativeQA dataset, using UnifiedQA-3B as the language model  RAPTOR outperforms baselines of each re- spective retrieval method Model ROUGE BLEU-1 BLEU-4 METEOR SBERT with RAPTOR 30 87% 23 50% 6\n",
      "\n",
      "Index: 43, Text: QuALITY-HARD, which represents questions that humans took unusually long to correctly answer, requiring rereading parts of the text, difficult reasoning, or both For the NarrativeQA dataset, as represented in Table 6, RAPTOR paired with UnifiedQA sets a new state-of-the-art METEOR score  When compared to the recursively summarizing model by Wu et al  (2021), which also employs UnifiedQA, RAPTOR outperforms it on all metrics  While\n",
      "\n",
      "Index: 45, Text: pothesized that upper nodes play a crucial role in handling thematic or multi-hop queries requiring a broader understanding of the text 8 Published as a conference paper at ICLR 2024 Table 6: Performance comparison on the NarrativeQA dataset across multiple models, focusing on four metrics: ROUGE-L, BLEU-1, BLEU-4, and METEOR  RAPTOR, when paired with Uni-\n",
      "\n",
      "Index: 48, Text: 8 23 5 6 4 19 1 Table 7: Accuracies of the QuALITY dataset on both the overall test set and the more challenging hard subset  GPT-4 with RAPTOR sets a new state-of-the-art ModelAccuracy Test Set Hard Subset Longformer-base (Beltagy et al , 2020) 39 5 35  3 DPR and DeBERTaV3-large (Pang et al\n",
      "\n",
      "Index: 21, Text: tiveQA, QASPER, and QuALITY  NarrativeQA is a dataset that comprises question-answer pairs based on the full texts of books and movie transcripts, totaling 1,572 documents (Ko ˇcisk`y et al , 2018; Wu et al , 2021)  The NarrativeQA-Story task requires a comprehensive understanding of the entire narrative in order to accurately answer its questions, thus testing the model’s ability to comprehend longer texts in\n",
      "\n",
      "Index: 22, Text: the literary domain  We measure performance on this dataset using the standard BLEU (B-1, B-4), ROUGE (R-L), and METEOR (M) metrics  Please see appendix H for more details on the Narra- tiveQA evaluation script used in our experiments The QASPER dataset includes 5,049 questions across 1,585 NLP papers, with each question probing for information embedded within the full text (Dasigi et al\n",
      "\n",
      "Index: 23, Text: , 2021)  The answer types in QASPER are categorized as Answerable/Unanswerable, Yes/No, Abstractive, and Extractive  Accuracy is measured using standard F1 Lastly, the QuALITY dataset consists of multiple-choice questions, each accompanied by context passages averaging approximately 5,000 tokens in length (Pang et al , 2022)  This dataset calls for\n",
      "\n",
      "Index: 24, Text: reasoning over the entire document for QA tasks, enabling us to measure the performance of our re- trieval system on medium-length documents  The dataset includes a challenging subset, QuALITY- HARD, which contains questions that a majority of human annotators answered incorrectly in a speed-setting  We report accuracies for both the entire test set and the HARD subset Controlled Baseline Comparisons We first present controlled comparisons using the UnifiedQA\n",
      "\n",
      "Index: 55, Text: experiments: GPT-3 and GPT-4 for QA tasks, and GPT-3 5-turbo for summarization  The gpt-3 , gpt-4 , and gpt-3 5-turbo models can be accessed via API calls (OpenAI API)  UnifiedQA, which is used for QA tasks, is publicly available at Hugging Face Evaluation Datasets The three evaluation datasets used in our experiments—QuALITY,\n",
      "\n",
      "Index: 56, Text: QASPER, and NarrativeQA—are all publicly accessible  These datasets ensure that the retrieval and QA tests conducted in this study can be replicated Source Code The source code for RAPTOR will be publicly available here REFERENCES Charu C Aggarwal, Alexander Hinneburg, and Daniel A Keim  On the Surprising Behavior of Dis- tance Metrics in High Dimensional Space  In Database Theory—ICDT 2001: 8th International\n",
      "\n",
      "Index: 96, Text:  URL https://aclanthology org/2021 emnlp-main 560  Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer  Nonparametric masked language modeling  In Findings of the Association for Computational Linguistics: ACL 2023 , pp  2097–2118, Toronto, Canada, July 2023  Associ-\n",
      "\n",
      "Index: 69, Text:  URL https: //aclanthology org/2022 acl-long 58  Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes  Reading Wikipedia to Answer Open-Domain Questions  In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp  1870–1879, Vancouver, Canada, July 2017  Association for Computational Linguistics  doi: 10\n",
      "\n",
      "Index: 75, Text: ciation for Computational Linguistics  doi: 10 18653/v1/2021 naacl-main 365  URL https: //aclanthology org/2021 naacl-main 365  Mengxing Dong, Bowei Zou, Yanling Li, and Yu Hong  CoLISA: Inner Interaction via Contrastive Learning for Multi-choice Reading Comprehension  In Advances in Information Retrieval: 45th\n",
      "\n",
      "Index: 79, Text:  Association for Computational Linguistics  doi: 10 18653/v1/2022 findings-naacl 55 URLhttps://aclanthology org/2022 findings-naacl 55  Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang  Retrieval Augmented Language Model Pre-Training  In International conference on machine learning , pp  3929–3938 PMLR, 2020\n",
      "\n",
      "Index: 61, Text:  Hybrid hierarchical retrieval for open-domain question answering  In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds ), Findings of the Association for Computational Linguistics: ACL 2023 , pp  10680–10689, Toronto, Canada, July 2023  Association for Computa- tional Linguistics  doi: 10 18653/v1/2023 findings-acl 679  URL https://aclanthology\n",
      "\n",
      "Index: 60, Text: Stefanos Angelidis and Mirella Lapata  Summarizing opinions: Aspect extraction meets sentiment prediction and they are both weakly supervised  arXiv preprint arXiv:1808 08858 , 2018  URL https://arxiv org/abs/1808 08858  Manoj Ghuhan Arivazhagan, Lan Liu, Peng Qi, Xinchi Chen, William Yang Wang, and Zhiheng Huang\n",
      "\n",
      "Index: 93, Text: Huang, Lucia Specia, and Scott Wen-tau Yih (eds ), Findings of the Association for Compu- tational Linguistics: EMNLP 2021 , pp  188–200, Punta Cana, Dominican Republic, Novem- ber 2021  Association for Computational Linguistics  doi: 10 18653/v1/2021 findings-emnlp 19 URL https://aclanthology org/2021 findings-emnlp\n",
      "\n",
      "Index: 0, Text: Published as a conference paper at ICLR 2024 Given a set of Ntext segments, each represented as a d-dimensional dense vector embedding, the likelihood of a text vector, x, given its membership in the kthGaussian distribution, is denoted by P(x|k) =N(x;µk,Σk)  The overall probability distribution is a weighted combination P(x) =PK\n",
      "\n",
      "Index: 1, Text: k=1πkN(x;µk,Σk), where πksignifies the mixture weight for the kthGaussian distribution The high dimensionality of vector embeddings presents a challenge for traditional GMMs, as dis- tance metrics may behave poorly when used to measure similarity in high-dimensional spaces (Ag- garwal et al , 2001)  To mitigate this, we employ Uniform Manifold Approximation and Projection\n",
      "\n",
      "Index: 3, Text: among the text data, from broad themes to specific details Should a local cluster’s combined context ever exceed the summarization model’s token threshold, our algorithm recursively applies clustering within the cluster, ensuring that the context remains within the token threshold To determine the optimal number of clusters, we employ the Bayesian Information Criterion (BIC) for model selection  BIC not only penalizes model complexity but also rewards goodness of fit (Schwarz, 1978)\n",
      "\n",
      "Index: 4, Text:  The BIC for a given GMM is BIC = ln( N)k−2 ln(ˆL), where Nis the number of text segments (or data points), kis the number of model parameters, and ˆLis the maximized value of the likelihood function of the model  In the context of GMM, the number of parameters k is a function of the dimensionality of the input vectors and the number of clusters\n",
      "\n",
      "Index: 5, Text: With the optimal number of clusters determined by BIC, the Expectation-Maximization algorithm is then used to estimate the GMM parameters, namely the means, covariances, and mixture weights While the Gaussian assumption in GMMs may not perfectly align with the nature of text data, which often exhibits a sparse and skewed distribution, our empirical observations suggest that it offers an effective model for our purpose  We run an ablation comparing GMM Clustering with summarizing\n",
      "\n",
      "Index: 6, Text: contiguous chunks and provide details in Appendix B Model-Based Summarization After clustering the nodes using Gaussian Mixture Models, the nodes in each cluster are sent to a language model for summarization  This step allows the model to transform large chunks of text into concise, coherent summaries of the selected nodes  For our experiments, we use gpt-3 5-turbo to generate the summaries  The summarization step con- denses the potentially large volume of retrieved information into a manageable size\n",
      "\n",
      "Index: 7, Text:  We provide statistics on the compression due to the summarization in Appendix C and the prompt used for summarization in Appendix D While the summarization model generally produces reliable summaries, a focused annotation study revealed that about 4% of the summaries contained minor hallucinations  These did not propagate to parent nodes and had no discernible impact on question-answering tasks  For an in-depth analysis of hallucinations, refer to the appendix E\n",
      "\n",
      "Index: 2, Text: (UMAP), a manifold learning technique for dimensionality reduction (McInnes et al , 2018)  The number of nearest neighbors parameter, nneighbors , in UMAP determines the balance between the preservation of local and global structures  Our algorithm varies nneighbors to create a hierar- chical clustering structure: it first identifies global clusters and then performs local clustering within these global clusters  This two-step clustering process captures a broad spectrum of relationships\n",
      "\n",
      "Index: 99, Text:  doi: 10 18653/v1/2020 nuse-1 13  URL https: //aclanthology org/2020 nuse-1 13  Inderjeet Nair, Aparna Garimella, Balaji Vasan Srinivasan, Natwar Modani, Niyati Chhaya, Srikr- ishna Karanam, and Sumit Shekhar  A neural CRF-based hierarchical approach for lin-\n",
      "\n",
      "Index: 66, Text:  Lin (eds ), Advances in Neu- ral Information Processing Systems , volume 33, pp  1877–1901  Curran Associates, Inc , 10 Published as a conference paper at ICLR 2024 2020  URL https://proceedings neurips cc/paper_files/paper/2020/ file/1457c0d6bfcb4967418bfb8ac142f64a-Paper pdf \n",
      "\n",
      "Index: 86, Text:  6769–6781, Online, November 2020  Association for Computational Linguis- tics  doi: 10 18653/v1/2020 emnlp-main 550  URL https://aclanthology org/2020 emnlp-main 550  Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and\n",
      "\n",
      "Index: 57, Text: Conference London, UK, January 4–6, 2001 Proceedings 8 , pp  420–434  Springer, 2001  URL https://link springer com/chapter/10 1007/3-540-44503-x_27  Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Onta ˜n´on, Siddhartha Brahma, Yury Zemlyan-\n",
      "\n",
      "Index: 91, Text: 33:9459–9474, 2020  URL https://doi org/10 48550/arXiv 2005 11401  Jerry Liu  LlamaIndex, 2022  URL https://github com/jerryjliu/llama_index  Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang\n",
      "\n",
      "Index: 94, Text: 19  Leland McInnes, John Healy, and James Melville  UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction, 2018  URL https://arxiv org/abs/1802 03426   arXiv preprint arXiv:1802 03426 Sewon Min, Kenton Lee, Ming-Wei Chang, Kristina Toutanova, and Hannaneh Hajishirzi  Joint\n",
      "\n",
      "Index: 9, Text:  The children of these selected nodes are considered at the next layer and the top-k nodes are selected from this pool again based on their cosine similarity to the query vector  This process is repeated until we reach the leaf nodes  Finally, the text from all selected nodes is concatenated to form the retrieved context  The algorithm’s steps are outlined below: 1  Start at the root layer of the RAPTOR tree  Compute the cosine similarity between the\n",
      "\n",
      "Index: 10, Text: query embedding and the embeddings of all nodes present at this initial layer 2  Choose the top- knodes based on the highest cosine similarity scores, forming the set S1 4 Published as a conference paper at ICLR 2024 Figure 2: Illustration of the tree traversal and collapsed tree retrieval mechanisms  Tree traver- sal starts at the root level of the tree and retrieves the top- k(here, top- 1) node(s) based on cosine\n",
      "\n",
      "Index: 11, Text: similarity to the query vector  At each level, it retrieves the top- knode(s) from the child nodes of the previous layer’s top- k  Collapsed tree collapses the tree into a single layer and retrieves nodes until a threshold number of tokens is reached, based on cosine similarity to the query vector  The nodes on which cosine similarity search is performed are highlighted in both illustrations 3  Proceed to the child nodes of the elements in set S1  Compute the cosine similarity between\n",
      "\n",
      "Index: 12, Text: the query vector and the vector embeddings of these child nodes 4  Select the top kchild nodes with the highest cosine similarity scores to the query, forming the set S2 5  Continue this process recursively for dlayers, producing sets S1, S2,   , S d 6  Concatenate sets S1through Sdto assemble the relevant context to the query By adjusting the depth dand the number of nodes kselected at each layer, the tree traversal method\n",
      "\n",
      "Index: 13, Text: offers control over the specificity and breadth of the information retrieved  The algorithm starts with a broad outlook by considering the top layers of the tree and progressively focuses on finer details as it descends through the lower layers Thecollapsed tree approach offers a simpler way to search for relevant information by considering all nodes in the tree simultaneously, as depicted in Figure 2  Instead of going layer-by-layer, this method flattens the multi-layered tree into a single layer, essentially bringing all the nodes onto the\n",
      "\n",
      "Index: 14, Text: same level for comparison  The steps for this method are outlined below: 1  First, collapse the entire RAPTOR tree into a single layer  This new set of nodes, denoted asC, contains nodes from every layer of the original tree 2  Next, calculate the cosine similarity between the query embedding and the embeddings of all nodes present in the collapsed set C 3  Finally, pick the top- knodes that have the highest cosine similarity scores with the query\n",
      "\n",
      "Index: 15, Text: Keep adding nodes to the result set until you reach a predefined maximum number of tokens, ensuring you don’t exceed the model’s input limitations We tested both approaches on 20 stories from the QASPER dataset  Figure 3 shows the performance of tree traversal with different top- sizes and collapsed tree with different maximum token numbers The collapsed tree approach consistently performs better  We believe collapsed tree retrieval is better due to offering greater flexibility than tree traversal; i e , by searching through all the nodes\n",
      "\n",
      "Index: 16, Text: simultaneously, it retrieves information that is at the correct level of granularity for a given question In comparison, while using tree traversal with the same values of dandk, the ratio of nodes from each level of the tree will be constant  So, the ratio of higher-order thematic information to granular details will remain the same regardless of the question 5 Published as a conference paper at ICLR 2024 One drawback, however, of the collapsed tree approach is that it requires cosine similarity search to\n",
      "\n",
      "Index: 17, Text: be performed on all nodes in the tree  However, this can be made more efficient with fast k-nearest neighbor libraries such as FAISS (Johnson et al , 2019) Figure 3: Comparison of querying methods Results on 20 stories from the QASPER dataset using tree traversal with different top-k values, and collapsed tree with different context lengths Collapsed tree with 2000 tokens produces the best results, so we use this querying strategy for our main results\n",
      "\n",
      "Index: 18, Text: Overall, given the collapsed tree approach’s greater flexibility and its superior performance on the subset of the QASPER dataset, this is the querying approach with which we proceed Specifically, we use the collapsed tree with 2000 maximum tokens, which approximately equates to retrieving the top-20 nodes  Using a token-based approach ensures the context does not exceed model context constraints as token counts can vary across nodes  For experiments with the UnifiedQA model, we provide 400 to-\n",
      "\n",
      "Index: 50, Text:  Columns represent different starting points (highest layer) and rows represent different numbers of layers queried Layers Queried / Start Layer Layer 0 (Leaf Nodes) Layer 1 Layer 2 1 layer 57 9 57 8 57 9 2 layers - 52 6 63 15 3 layers - - 73 68 We validated this hypothesis both quantitatively and qualitatively  We present qualitative analysis in appendix G\n",
      "\n",
      "Index: 8, Text: Querying In this section, we elaborate on the two querying mechanisms employed by RAPTOR: tree traversal and collapsed tree  These methods offer unique ways of traversing the multi-layered RAPTOR tree to retrieve relevant information, each with its own advantages and trade-offs  We provide the pseudocode of both methods in Appendix F  Note that we embed all nodes using SBERT The tree traversal method first selects the top-k most relevant root nodes based on their cosine similarity to the query embedding\n",
      "\n",
      "Index: 44, Text: Wu et al  (2021) rely solely on the summary in the top root node of the tree structure, RAPTOR benefits from its intermediate layers and clustering approaches, which allows it to capture a range of information, from general themes to specific details, contributing to its overall strong performance 4 1 C ONTRIBUTION OF THE TREE STRUCTURE We examine the contribution of each layer of nodes to RAPTOR’s retrieval capabilities  We hy-\n",
      "\n",
      "Index: 49, Text: , 2022) 55 4 46  1 CoLISA (DeBERTaV3-large) (Dong et al , 2023a) 62 3 54  7 RAPTOR + GPT-4 82 6 76 2 Table 8: Performance of RAPTOR when querying different tree layers for Story 1 from the QuAL- ITY dataset\n",
      "\n",
      "Index: 51, Text:  To quantitatively understand the contribution of the upper-level nodes, we used stories from the QuALITY dataset  The RAPTOR tree is built for each of these stories, as described in Section 3  However, during retrieval, we limit the search to different subsets of layers  For example, we exclusively retrieve from the leaf nodes and each upper layer, as well as from different contiguous subsets of the layers\n",
      "\n",
      "Index: 20, Text: in Figure 4, RAPTOR’s tree-based retrieval allows it to choose nodes from different tree layers, matching the question’s detail level  This approach often yields more relevant and comprehensive information for downstream tasks than DPR  For a detailed discussion and examples, including the text retrieved by both RAPTOR and DPR for specific questions, please refer to the appendix G 4 E XPERIMENTS Datasets We measure RAPTOR’s performance across three question-answering datasets: Narra-\n",
      "\n",
      "Index: 52, Text:  We show findings specific to one story in Table 8, revealing that a full-tree search, utilizing all layers, outperformed retrieval strategies that focused only on specific layers These findings highlight the importance of the full tree structure in RAPTOR  By providing both the original text and higher-level summaries for retrieval, RAPTOR can effectively handle a wider range of questions, from higher-order thematic queries to detail-oriented questions  Detailed results\n",
      "\n",
      "Index: 53, Text: for additional stories and an ablation study on layer contributions can be found in Appendix I 5 C ONCLUSION In this paper, we have presented RAPTOR, a novel tree-based retrieval system that augments the parametric knowledge of large language models with contextual information at various levels of abstraction  By employing recursive clustering and summarization techniques, RAPTOR creates a hierarchical tree structure that is capable of synthesizing information across various sections of the retrieval corpora\n",
      "\n",
      "Index: 54, Text:  During the query phase, RAPTOR leverages this tree structure for more effective retrieval  Our controlled experiments demonstrated that RAPTOR not only outperforms traditional retrieval methods but also sets new performance benchmarks on several question-answering tasks 9 Published as a conference paper at ICLR 2024 6 R EPRODUCIBILITY STATEMENT Language Models for QA and Summarization Four language models are used in our RAPTOR\n",
      "\n",
      "Index: 25, Text: 3B as the reader, with SBERT (Reimers & Gurevych, 2019), BM25 (Robertson et al , 1995; 2009), and DPR (Karpukhin et al , 2020) as the embedding models with and without the RAPTOR tree structure, on three datasets: QASPER, NarrativeQA, and QuALITY   As shown in Tables 1 and 2,\n",
      "\n",
      "Index: 26, Text: 6 Published as a conference paper at ICLR 2024 Figure 4: Querying Process: Illustration of how RAPTOR retrieves information for two questions about the Cinderella story: “What is the central theme of the story ” and “How did Cinderella find a happy ending ”  Highlighted nodes indicate RAPTOR’s selections, while arrows point to DPR’s leaf nodes  Notably, RAPTOR’s context often encompasses the information retrieved by DPR, either\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_tree_layers(tree.root_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "835a912d-fcfd-456e-b2d3-3be9db18e2b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.10.3\n",
      "  latest version: 24.5.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/studio-lab-user/.conda/envs/default\n",
      "\n",
      "  added / updated specs:\n",
      "    - python-graphviz\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    atk-1.0-2.38.0             |       h04ea711_2         348 KB  conda-forge\n",
      "    ca-certificates-2024.6.2   |       hbcca054_0         152 KB  conda-forge\n",
      "    cairo-1.18.0               |       h3faef2a_0         959 KB  conda-forge\n",
      "    certifi-2024.2.2           |     pyhd8ed1ab_0         157 KB  conda-forge\n",
      "    expat-2.6.2                |       h59595ed_0         134 KB  conda-forge\n",
      "    font-ttf-dejavu-sans-mono-2.37|       hab24e00_0         388 KB  conda-forge\n",
      "    font-ttf-inconsolata-3.000 |       h77eed37_0          94 KB  conda-forge\n",
      "    font-ttf-source-code-pro-2.038|       h77eed37_0         684 KB  conda-forge\n",
      "    font-ttf-ubuntu-0.83       |       h77eed37_2         1.5 MB  conda-forge\n",
      "    fontconfig-2.14.2          |       h14ed4e7_0         266 KB  conda-forge\n",
      "    fonts-conda-ecosystem-1    |                0           4 KB  conda-forge\n",
      "    fonts-conda-forge-1        |                0           4 KB  conda-forge\n",
      "    freetype-2.12.1            |       h267a509_2         620 KB  conda-forge\n",
      "    fribidi-1.0.10             |       h36c2ea0_0         112 KB  conda-forge\n",
      "    gdk-pixbuf-2.42.12         |       hb9ae30d_0         516 KB  conda-forge\n",
      "    giflib-5.2.2               |       hd590300_0          75 KB  conda-forge\n",
      "    graphite2-1.3.13           |    h59595ed_1003          95 KB  conda-forge\n",
      "    graphviz-11.0.0            |       hc68bbd7_0         2.2 MB  conda-forge\n",
      "    gtk2-2.24.33               |       h280cfa0_4         6.2 MB  conda-forge\n",
      "    gts-0.7.6                  |       h977cf35_4         311 KB  conda-forge\n",
      "    harfbuzz-8.5.0             |       hfac3d4d_0         1.5 MB  conda-forge\n",
      "    icu-73.2                   |       h59595ed_0        11.5 MB  conda-forge\n",
      "    lerc-4.0.0                 |       h27087fc_0         275 KB  conda-forge\n",
      "    libdeflate-1.20            |       hd590300_0          70 KB  conda-forge\n",
      "    libexpat-2.6.2             |       h59595ed_0          72 KB  conda-forge\n",
      "    libgd-2.3.3                |       h119a65a_9         219 KB  conda-forge\n",
      "    libglib-2.80.2             |       hf974151_0         3.7 MB  conda-forge\n",
      "    libiconv-1.17              |       hd590300_2         689 KB  conda-forge\n",
      "    libjpeg-turbo-3.0.0        |       hd590300_1         604 KB  conda-forge\n",
      "    libpng-1.6.43              |       h2797004_0         281 KB  conda-forge\n",
      "    librsvg-2.58.0             |       hadf69e7_1         5.7 MB  conda-forge\n",
      "    libtiff-4.6.0              |       h1dd3fc0_3         276 KB  conda-forge\n",
      "    libwebp-1.4.0              |       h2c329e2_0          90 KB  conda-forge\n",
      "    libwebp-base-1.4.0         |       hd590300_0         429 KB  conda-forge\n",
      "    libxcb-1.15                |       h0b41bf4_0         375 KB  conda-forge\n",
      "    libxml2-2.12.7             |       hc051c1a_1         688 KB  conda-forge\n",
      "    openssl-3.3.1              |       h4ab18f5_0         2.8 MB  conda-forge\n",
      "    pango-1.52.2               |       ha41ecd1_0         436 KB  conda-forge\n",
      "    pcre2-10.43                |       hcad00b1_0         929 KB  conda-forge\n",
      "    pixman-0.43.2              |       h59595ed_0         378 KB  conda-forge\n",
      "    pthread-stubs-0.4          |    h36c2ea0_1001           5 KB  conda-forge\n",
      "    python-graphviz-0.20.3     |     pyh717bed2_0          37 KB  conda-forge\n",
      "    xorg-kbproto-1.0.7         |    h7f98852_1002          27 KB  conda-forge\n",
      "    xorg-libice-1.1.1          |       hd590300_0          57 KB  conda-forge\n",
      "    xorg-libsm-1.2.4           |       h7391055_0          27 KB  conda-forge\n",
      "    xorg-libx11-1.8.9          |       h8ee46fc_0         809 KB  conda-forge\n",
      "    xorg-libxau-1.0.11         |       hd590300_0          14 KB  conda-forge\n",
      "    xorg-libxdmcp-1.1.3        |       h7f98852_0          19 KB  conda-forge\n",
      "    xorg-libxext-1.3.4         |       h0b41bf4_2          49 KB  conda-forge\n",
      "    xorg-libxrender-0.9.11     |       hd590300_0          37 KB  conda-forge\n",
      "    xorg-renderproto-0.11.1    |    h7f98852_1002           9 KB  conda-forge\n",
      "    xorg-xextproto-7.3.0       |    h0b41bf4_1003          30 KB  conda-forge\n",
      "    xorg-xproto-7.0.31         |    h7f98852_1007          73 KB  conda-forge\n",
      "    zlib-1.2.13                |       hd590300_5          91 KB  conda-forge\n",
      "    zstd-1.5.6                 |       ha6fb4c9_0         542 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        47.5 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  atk-1.0            conda-forge/linux-64::atk-1.0-2.38.0-h04ea711_2\n",
      "  cairo              conda-forge/linux-64::cairo-1.18.0-h3faef2a_0\n",
      "  expat              conda-forge/linux-64::expat-2.6.2-h59595ed_0\n",
      "  font-ttf-dejavu-s~ conda-forge/noarch::font-ttf-dejavu-sans-mono-2.37-hab24e00_0\n",
      "  font-ttf-inconsol~ conda-forge/noarch::font-ttf-inconsolata-3.000-h77eed37_0\n",
      "  font-ttf-source-c~ conda-forge/noarch::font-ttf-source-code-pro-2.038-h77eed37_0\n",
      "  font-ttf-ubuntu    conda-forge/noarch::font-ttf-ubuntu-0.83-h77eed37_2\n",
      "  fontconfig         conda-forge/linux-64::fontconfig-2.14.2-h14ed4e7_0\n",
      "  fonts-conda-ecosy~ conda-forge/noarch::fonts-conda-ecosystem-1-0\n",
      "  fonts-conda-forge  conda-forge/noarch::fonts-conda-forge-1-0\n",
      "  freetype           conda-forge/linux-64::freetype-2.12.1-h267a509_2\n",
      "  fribidi            conda-forge/linux-64::fribidi-1.0.10-h36c2ea0_0\n",
      "  gdk-pixbuf         conda-forge/linux-64::gdk-pixbuf-2.42.12-hb9ae30d_0\n",
      "  giflib             conda-forge/linux-64::giflib-5.2.2-hd590300_0\n",
      "  graphite2          conda-forge/linux-64::graphite2-1.3.13-h59595ed_1003\n",
      "  graphviz           conda-forge/linux-64::graphviz-11.0.0-hc68bbd7_0\n",
      "  gtk2               conda-forge/linux-64::gtk2-2.24.33-h280cfa0_4\n",
      "  gts                conda-forge/linux-64::gts-0.7.6-h977cf35_4\n",
      "  harfbuzz           conda-forge/linux-64::harfbuzz-8.5.0-hfac3d4d_0\n",
      "  icu                conda-forge/linux-64::icu-73.2-h59595ed_0\n",
      "  lerc               conda-forge/linux-64::lerc-4.0.0-h27087fc_0\n",
      "  libdeflate         conda-forge/linux-64::libdeflate-1.20-hd590300_0\n",
      "  libexpat           conda-forge/linux-64::libexpat-2.6.2-h59595ed_0\n",
      "  libgd              conda-forge/linux-64::libgd-2.3.3-h119a65a_9\n",
      "  libglib            conda-forge/linux-64::libglib-2.80.2-hf974151_0\n",
      "  libiconv           conda-forge/linux-64::libiconv-1.17-hd590300_2\n",
      "  libjpeg-turbo      conda-forge/linux-64::libjpeg-turbo-3.0.0-hd590300_1\n",
      "  libpng             conda-forge/linux-64::libpng-1.6.43-h2797004_0\n",
      "  librsvg            conda-forge/linux-64::librsvg-2.58.0-hadf69e7_1\n",
      "  libtiff            conda-forge/linux-64::libtiff-4.6.0-h1dd3fc0_3\n",
      "  libwebp            conda-forge/linux-64::libwebp-1.4.0-h2c329e2_0\n",
      "  libwebp-base       conda-forge/linux-64::libwebp-base-1.4.0-hd590300_0\n",
      "  libxcb             conda-forge/linux-64::libxcb-1.15-h0b41bf4_0\n",
      "  libxml2            conda-forge/linux-64::libxml2-2.12.7-hc051c1a_1\n",
      "  pango              conda-forge/linux-64::pango-1.52.2-ha41ecd1_0\n",
      "  pcre2              conda-forge/linux-64::pcre2-10.43-hcad00b1_0\n",
      "  pixman             conda-forge/linux-64::pixman-0.43.2-h59595ed_0\n",
      "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h36c2ea0_1001\n",
      "  python-graphviz    conda-forge/noarch::python-graphviz-0.20.3-pyh717bed2_0\n",
      "  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-h7f98852_1002\n",
      "  xorg-libice        conda-forge/linux-64::xorg-libice-1.1.1-hd590300_0\n",
      "  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.4-h7391055_0\n",
      "  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.8.9-h8ee46fc_0\n",
      "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.11-hd590300_0\n",
      "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.3-h7f98852_0\n",
      "  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.4-h0b41bf4_2\n",
      "  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.11-hd590300_0\n",
      "  xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-h7f98852_1002\n",
      "  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h0b41bf4_1003\n",
      "  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h7f98852_1007\n",
      "  zlib               conda-forge/linux-64::zlib-1.2.13-hd590300_5\n",
      "  zstd               conda-forge/linux-64::zstd-1.5.6-ha6fb4c9_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                      2023.7.22-hbcca054_0 --> 2024.6.2-hbcca054_0\n",
      "  certifi                            2023.7.22-pyhd8ed1ab_0 --> 2024.2.2-pyhd8ed1ab_0\n",
      "  openssl                                  3.1.2-hd590300_0 --> 3.3.1-h4ab18f5_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "font-ttf-ubuntu-0.83 | 1.5 MB    | ##################################### | 100% \n",
      "libjpeg-turbo-3.0.0  | 604 KB    | ##################################### | 100% \n",
      "libexpat-2.6.2       | 72 KB     | ##################################### | 100% \n",
      "ca-certificates-2024 | 152 KB    | ##################################### | 100% \n",
      "certifi-2024.2.2     | 157 KB    | ##################################### | 100% \n",
      "giflib-5.2.2         | 75 KB     | ##################################### | 100% \n",
      "fribidi-1.0.10       | 112 KB    | ##################################### | 100% \n",
      "pango-1.52.2         | 436 KB    | ##################################### | 100% \n",
      "libdeflate-1.20      | 70 KB     | ##################################### | 100% \n",
      "zlib-1.2.13          | 91 KB     | ##################################### | 100% \n",
      "xorg-libxext-1.3.4   | 49 KB     | ##################################### | 100% \n",
      "atk-1.0-2.38.0       | 348 KB    | ##################################### | 100% \n",
      "libwebp-1.4.0        | 90 KB     | ##################################### | 100% \n",
      "font-ttf-dejavu-sans | 388 KB    | ##################################### | 100% \n",
      "xorg-libsm-1.2.4     | 27 KB     | ##################################### | 100% \n",
      "lerc-4.0.0           | 275 KB    | ##################################### | 100% \n",
      "xorg-libxau-1.0.11   | 14 KB     | ##################################### | 100% \n",
      "pcre2-10.43          | 929 KB    | ##################################### | 100% \n",
      "libgd-2.3.3          | 219 KB    | ##################################### | 100% \n",
      "pixman-0.43.2        | 378 KB    | ##################################### | 100% \n",
      "xorg-renderproto-0.1 | 9 KB      | ##################################### | 100% \n",
      "gdk-pixbuf-2.42.12   | 516 KB    | ##################################### | 100% \n",
      "fonts-conda-ecosyste | 4 KB      | ##################################### | 100% \n",
      "xorg-libxdmcp-1.1.3  | 19 KB     | ##################################### | 100% \n",
      "libwebp-base-1.4.0   | 429 KB    | ##################################### | 100% \n",
      "fonts-conda-forge-1  | 4 KB      | ##################################### | 100% \n",
      "xorg-xproto-7.0.31   | 73 KB     | ##################################### | 100% \n",
      "libxml2-2.12.7       | 688 KB    | ##################################### | 100% \n",
      "gts-0.7.6            | 311 KB    | ##################################### | 100% \n",
      "graphite2-1.3.13     | 95 KB     | ##################################### | 100% \n",
      "libxcb-1.15          | 375 KB    | ##################################### | 100% \n",
      "openssl-3.3.1        | 2.8 MB    | ##################################### | 100% \n",
      "xorg-libx11-1.8.9    | 809 KB    | ##################################### | 100% \n",
      "xorg-libice-1.1.1    | 57 KB     | ##################################### | 100% \n",
      "gtk2-2.24.33         | 6.2 MB    | ##################################### | 100% \n",
      "python-graphviz-0.20 | 37 KB     | ##################################### | 100% \n",
      "librsvg-2.58.0       | 5.7 MB    | ##################################### | 100% \n",
      "pthread-stubs-0.4    | 5 KB      | ##################################### | 100% \n",
      "cairo-1.18.0         | 959 KB    | ##################################### | 100% \n",
      "freetype-2.12.1      | 620 KB    | ##################################### | 100% \n",
      "expat-2.6.2          | 134 KB    | ##################################### | 100% \n",
      "libpng-1.6.43        | 281 KB    | ##################################### | 100% \n",
      "libtiff-4.6.0        | 276 KB    | ##################################### | 100% \n",
      "fontconfig-2.14.2    | 266 KB    | ##################################### | 100% \n",
      "harfbuzz-8.5.0       | 1.5 MB    | ##################################### | 100% \n",
      "libglib-2.80.2       | 3.7 MB    | ##################################### | 100% \n",
      "font-ttf-source-code | 684 KB    | ##################################### | 100% \n",
      "font-ttf-inconsolata | 94 KB     | ##################################### | 100% \n",
      "xorg-kbproto-1.0.7   | 27 KB     | ##################################### | 100% \n",
      "xorg-xextproto-7.3.0 | 30 KB     | ##################################### | 100% \n",
      "graphviz-11.0.0      | 2.2 MB    | ##################################### | 100% \n",
      "icu-73.2             | 11.5 MB   | ##################################### | 100% \n",
      "libiconv-1.17        | 689 KB    | ##################################### | 100% \n",
      "xorg-libxrender-0.9. | 37 KB     | ##################################### | 100% \n",
      "zstd-1.5.6           | 542 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: \\ \n",
      "- \n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!conda install python-graphviz -y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8b574e3-8a7d-4447-9517-15b21b7c1146",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tree_graph.png'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "from typing import Set\n",
    "\n",
    "def create_graph(tree):\n",
    "    dot = Digraph()\n",
    "    \n",
    "    for index, node in tree.all_nodes.items():\n",
    "        dot.node(str(index), label=str(index))\n",
    "        \n",
    "    for index, node in tree.all_nodes.items():\n",
    "        for child_index in node.children:\n",
    "            dot.edge(str(index), str(child_index))\n",
    "            \n",
    "    return dot\n",
    "\n",
    "graph = create_graph(tree)\n",
    "graph.attr(layout = 'dot')\n",
    "graph.render('tree_graph',format='png', cleanup = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0837236-38cb-457e-97c0-0684820acc80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
